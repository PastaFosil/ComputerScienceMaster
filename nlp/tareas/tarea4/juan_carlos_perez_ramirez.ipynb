{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juan Carlos Perez Ramirez\n",
    "## Procesamiento de Lenguaje Natural\n",
    "## Tarea 4: Modelo de Lenguaje de Política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento y tratamiento de datos\n",
    "Se limpiaron los saltos de linea y espacios multiples, asi como los \"|\". Se empleo el tokenizador de oraciones de nltk, que separa las oraciones de acuerdo a la aparicion de puntos (sin contar los de abreviaturas), signos de interrogacion y exclamacion, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def files_to_string(directory, pattern):\n",
    "    '''\n",
    "    Lee todos los archivos de un directorio que satisfacen el patron dado,\n",
    "    los almacena en una lista de strings y los devuelve.\n",
    "    '''\n",
    "    contents = {}\n",
    "\n",
    "    for file in glob.glob(directory + pattern):\n",
    "        with open(file, 'r', encoding=\"utf-8\") as f:\n",
    "            contents[file[-14:-4]] = f.read()\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../Corpus/estenograficas_limpias_por_fecha/\"\n",
    "data = files_to_string(file_path, \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "corpus = \"\"\n",
    "corpus = corpus.join(data.values()).lower()\n",
    "#corpus = unidecode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "corpus = re.sub(r\"\\d+\", \"\", corpus)\n",
    "corpus = re.sub(r\"\\s+\", \" \", corpus).strip()\n",
    "corpus = corpus.replace(\"|\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versión estenográfica. conferencia de prensa de la presidenta claudia sheinbaum pardo del de octubre'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/juancho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oraciones = sent_tokenize(corpus, language='spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['versión estenográfica.',\n",
       " 'conferencia de prensa de la presidenta claudia sheinbaum pardo del de octubre de  presidencia de la república  gobierno  gob.mx interruptor de navegación trámites gobierno english interruptor de navegación versiones estenográficas prensa protección de datos personales transparencia inicio presidencia de la república blog aa+ aa- publicaciones recientes nuevo -- :: - versión estenográfica.',\n",
       " 'entrega de tarjetas del programa de mejoramiento de vivienda para el bienestar nuevo -- :: - versión estenográfica.',\n",
       " 'entrega de tarjetas vivienda para el bienestar nuevo -- :: - versión estenográfica.',\n",
       " 'entrega de tarjetas vivienda para el bienestar nuevo -- :: - versión estenográfica.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "class NgramData:\n",
    "\n",
    "    def __init__(self, vocab_max, tokenizer):\n",
    "        self.vocab_max = vocab_max\n",
    "        self.tokenizer = tokenizer\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.final_vocabulary = set()\n",
    "\n",
    "    def fit(self, raw_texts):\n",
    "        freq_dist = FreqDist()\n",
    "        tokenized_corpus = []\n",
    "\n",
    "        for txt in raw_texts:\n",
    "            tokens = self.tokenizer.tokenize(txt)\n",
    "            tokenized_corpus.append(tokens)\n",
    "            for w in tokens:\n",
    "                freq_dist[w] += 1\n",
    "\n",
    "        self.final_vocabulary = set(tok for tok, _ in freq_dist.most_common(self.vocab_max))\n",
    "        self.final_vocabulary.update([self.UNK, self.SOS, self.EOS])\n",
    "\n",
    "        transformed_corpus = [self.transform(tokens) for tokens in tokenized_corpus]\n",
    "\n",
    "        return transformed_corpus\n",
    "    \n",
    "    def mask_oov(self, word):\n",
    "        return self.UNK if word not in self.final_vocabulary else word\n",
    "    \n",
    "    def add_sos_eos(self, tokens):\n",
    "        return [self.SOS, self.SOS, self.SOS] + tokens + [self.EOS]\n",
    "\n",
    "    def transform(self, tokens):\n",
    "        transformed = [self.mask_oov(w) for w in tokens]\n",
    "        transformed = self.add_sos_eos(transformed)\n",
    "        return transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Lenguaje y Evaluación\n",
    "## Tetragram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TetragramLanguageModel:\n",
    "\n",
    "    def __init__(self, lambda_tetra=0.4, lambda_tri=0.3, lambda_bi=0.2, lambda_uni=0.1):\n",
    "        \n",
    "        self.lambda_tetra = lambda_tetra\n",
    "        self.lambda_tri = lambda_tri\n",
    "        self.lambda_bi = lambda_bi\n",
    "        self.lambda_uni = lambda_uni\n",
    "\n",
    "        # Contadores\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "        self.tetragram_counts = {}\n",
    "\n",
    "        self.vocab = 0\n",
    "        self.total_tokens = 0\n",
    "        self.V = 0\n",
    "\n",
    "    def set_lambdas(self, lambda_tetra, lambda_tri, lambda_bi, lambda_uni):\n",
    "\n",
    "        self.lambda_tetra = lambda_tetra\n",
    "        self.lambda_tri = lambda_tri\n",
    "        self.lambda_bi = lambda_bi\n",
    "        self.lambda_uni = lambda_uni\n",
    "\n",
    "    def train(self, transformed_corpus, final_vocabulary):\n",
    "        self.vocab = set(final_vocabulary)\n",
    "        self.V = len(final_vocabulary)\n",
    "\n",
    "        for tokens in transformed_corpus:\n",
    "            for i, w in enumerate(tokens):\n",
    "\n",
    "                # Unigramas\n",
    "                self.unigram_counts[w] = self.unigram_counts.get(w, 0) + 1\n",
    "\n",
    "                # Bigramas\n",
    "                if i > 0:\n",
    "                    w_prev = tokens[i-1]\n",
    "                    self.bigram_counts[(w_prev, w)] = self.bigram_counts.get((w_prev, w), 0) + 1\n",
    "\n",
    "                # Trigramas\n",
    "                if i > 1:\n",
    "                    w_prev2 = tokens[i-2]\n",
    "                    self.trigram_counts[(w_prev2, w_prev, w)] = self.trigram_counts.get((w_prev2, w_prev, w), 0) + 1\n",
    "\n",
    "                # Tetragramas\n",
    "                if i > 2:\n",
    "                    w_prev3 = tokens[i-3]\n",
    "                    self.tetragram_counts[(w_prev3, w_prev2, w_prev, w)] = self.tetragram_counts.get((w_prev3, w_prev2, w_prev, w), 0) + 1\n",
    "                    \n",
    "        self.total_tokens = sum(self.unigram_counts.values())\n",
    "\n",
    "    def mask_oov(self, word):\n",
    "        return \"<unk>\" if word not in self.vocab else word\n",
    "\n",
    "    def unigram_probability(self, w):\n",
    "        return (self.unigram_counts.get(self.mask_oov(w), 0) + 1) / (self.total_tokens + self.V)\n",
    "\n",
    "    def bigram_probability(self, w_prev, w):\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "        numerator = self.bigram_counts.get((w_prev, w), 0) + 1\n",
    "        denominator = self.unigram_counts.get(w_prev, 0) + self.V\n",
    "        return numerator / denominator\n",
    "\n",
    "    def trigram_probability(self, w_prev2, w_prev, w):\n",
    "        w_prev2 = self.mask_oov(w_prev2)\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "        numerator = self.trigram_counts.get((w_prev2, w_prev, w), 0) + 1\n",
    "        denominator = self.bigram_counts.get((w_prev2, w_prev), 0) + self.V\n",
    "        return numerator / denominator\n",
    "\n",
    "    def tetragram_probability(self, w_prev3, w_prev2, w_prev, w):\n",
    "        w_prev3 = self.mask_oov(w_prev3)\n",
    "        w_prev2 = self.mask_oov(w_prev2)\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "        numerator = self.tetragram_counts.get((w_prev3, w_prev2, w_prev, w), 0) + 1\n",
    "        denominator = self.trigram_counts.get((w_prev3, w_prev2, w_prev), 0) + self.V\n",
    "        return numerator / denominator\n",
    "\n",
    "    def probability_of_word(self, w_prev3, w_prev2, w_prev, w):\n",
    "        return self.lambda_tetra * self.tetragram_probability(w_prev3, w_prev2, w_prev, w) + \\\n",
    "                self.lambda_tri * self.trigram_probability(w_prev2, w_prev, w) + \\\n",
    "                self.lambda_bi * self.bigram_probability(w_prev, w) + \\\n",
    "                self.lambda_uni * self.unigram_probability(w)\n",
    "\n",
    "    def sequence_probability(self, sequence):\n",
    "        log_prob = 0.0\n",
    "        l = len(sequence)\n",
    "        for i in range(3, l):\n",
    "            context = sequence[i-3:i]\n",
    "            w = sequence[i]\n",
    "            p = self.probability_of_word(context[-3], context[-2], context[-1], w)\n",
    "            log_prob += math.log(p + 1e-10)  # Para evitar log(0)\n",
    "\n",
    "        return log_prob  # Devuelve la log-probabilidad sin hacer exp()\n",
    "\n",
    "\n",
    "\n",
    "    def check_prob(self):\n",
    "        print(sum(self.unigram_probability(w) for w in self.vocab))\n",
    "        print(sum(self.bigram_probability(\"hola\", w) for w in self.vocab))\n",
    "        print(sum(self.trigram_probability(\"hola\", \"como\", w) for w in self.vocab))\n",
    "        print(sum(self.tetragram_probability(\"hola\", \"como\", \"te\", w) for w in self.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.arange(len(oraciones))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(len(indices) * 0.8)\n",
    "val_size = int(len(indices) * 0.1)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_corpus = [oraciones[i] for i in train_indices]\n",
    "val_corpus = [oraciones[i] for i in val_indices]\n",
    "test_corpus = [oraciones[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "Se separo el corpus total en una parte de entrenamiento (80% de los datos), una de validacion y una de prueba (10% cada una), entrenando a los cuatro modelos de lenguaje con el primer set, usando un vocabulario de 40 mil tokens (mas los tokens especiales de inicio y fin de linea y token desconocido).\n",
    "\n",
    "Los tokens no vistos se enmascaran con el simbolo \"\\<unk\\>\" para evitar probabilidades nulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-ZáéíóúüñÁÉÍÓÚÜÑ]+\")\n",
    "\n",
    "ngram_data = NgramData(40000, tokenizer)\n",
    "transformed_corpus = ngram_data.fit(train_corpus)\n",
    "final_vocab = ngram_data.final_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetra_lm = TetragramLanguageModel(lambda_tetra=1, lambda_tri=0, lambda_bi=0, lambda_uni=0)\n",
    "tri_lm = TetragramLanguageModel(lambda_tetra=0, lambda_tri=1, lambda_bi=0, lambda_uni=0)\n",
    "bi_lm = TetragramLanguageModel(lambda_tetra=0, lambda_tri=0, lambda_bi=1, lambda_uni=0)\n",
    "uni_lm = TetragramLanguageModel(lambda_tetra=0, lambda_tri=0, lambda_bi=0, lambda_uni=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tetra listo\n",
      "Tri listo\n",
      "Bi listo\n",
      "Uni listo\n"
     ]
    }
   ],
   "source": [
    "tetra_lm.train(transformed_corpus, final_vocab)\n",
    "print(\"Tetra listo\")\n",
    "tri_lm.train(transformed_corpus, final_vocab)\n",
    "print(\"Tri listo\")\n",
    "bi_lm.train(transformed_corpus, final_vocab)\n",
    "print(\"Bi listo\")\n",
    "uni_lm.train(transformed_corpus, final_vocab)\n",
    "print(\"Uni listo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion se muestran dos ejemplos: uno con una secuencia de palabras presentes en el vocabulario y otra con un token no visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las palabras estan en el corpus.\n",
      "\n",
      "P('oposición' | 'políticos', 'de', 'la') = 0.0001249625112466\n",
      "\n",
      "P('oposición' | 'de', 'la') = 0.0018416206261510\n",
      "\n",
      "P('oposición' | 'la') = 0.0017329854728229\n",
      "\n",
      "P('oposición') = 0.0000820228100446\n"
     ]
    }
   ],
   "source": [
    "w_prev3, w_prev2, w_prev, w = \"políticos\", \"de\", \"la\", \"oposición\"\n",
    "words = [w_prev3, w_prev2, w_prev, w]\n",
    "\n",
    "is_there = True\n",
    "for word in words:\n",
    "    is_there = is_there and word in final_vocab\n",
    "    if not is_there:\n",
    "        print(f\"La palabra '{word}' no esta en el corpus.\")\n",
    "        break\n",
    "if is_there:\n",
    "    print(\"Todas las palabras estan en el corpus.\")\n",
    "\n",
    "p_w = tetra_lm.tetragram_probability(*words)\n",
    "print(f\"\\nP('{w}' | '{w_prev3}', '{w_prev2}', '{w_prev}') = {p_w:.16f}\")\n",
    "\n",
    "p_w = tri_lm.trigram_probability(*words[1:])\n",
    "print(f\"\\nP('{w}' | '{w_prev2}', '{w_prev}') = {p_w:.16f}\")\n",
    "\n",
    "p_w = bi_lm.bigram_probability(*words[2:])\n",
    "print(f\"\\nP('{w}' | '{w_prev}') = {p_w:.16f}\")\n",
    "\n",
    "p_w = uni_lm.unigram_probability(w)\n",
    "print(f\"\\nP('{w}') = {p_w:.16f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra 'picafresa' no esta en el corpus.\n",
      "\n",
      "P('picafresa' | 'políticos', 'de', 'la') = 0.0001249625112466\n",
      "\n",
      "P('picafresa' | 'de', 'la') = 0.0018416206261510\n",
      "\n",
      "P('picafresa' | 'la') = 0.0017329854728229\n",
      "\n",
      "P('picafresa') = 0.0044435652126803\n"
     ]
    }
   ],
   "source": [
    "w = \"picafresa\"\n",
    "if not w in final_vocab:\n",
    "    print(f\"La palabra '{w}' no esta en el corpus.\")\n",
    "\n",
    "p_w = tetra_lm.tetragram_probability(*words)\n",
    "print(f\"\\nP('{w}' | '{w_prev3}', '{w_prev2}', '{w_prev}') = {p_w:.16f}\")\n",
    "\n",
    "p_w = tri_lm.trigram_probability(*words[1:])\n",
    "print(f\"\\nP('{w}' | '{w_prev2}', '{w_prev}') = {p_w:.16f}\")\n",
    "\n",
    "p_w = bi_lm.bigram_probability(*words[2:])\n",
    "print(f\"\\nP('{w}' | '{w_prev}') = {p_w:.16f}\")\n",
    "\n",
    "p_w = uni_lm.unigram_probability(w)\n",
    "print(f\"\\nP('{w}') = {p_w:.16f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estrategia empleada para palabras no nulas parece funcionar, si bien, debido a que las palabras que no quedaron en el vocabulario final se agrupan bajo el token desconocido, la probabilidad de aparicion de palabras no vistas es bastante alta, llevando a casos como este en el que la palabra \"picafresa\" tiene una probabilidad de aparicion mas alta que \"oposición\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo interpolado (lambdas sugeridas)\n",
    "\n",
    "Con las particiones detalladas arriba, se calculo la perplejidad para los modelos con los conjuntos de lambdas sugeridas, modificando sus valores en el modelo de tetragramas entrenado previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def perplexity(model, corpus):\n",
    "    l = len(corpus)\n",
    "    log_prob = 0.0\n",
    "    for i in range(3, l):\n",
    "        w_prev3 = corpus[i-3]\n",
    "        w_prev2 = corpus[i-2]\n",
    "        w_prev = corpus[i-1]\n",
    "        w = corpus[i]\n",
    "        p = model.probability_of_word(w_prev3, w_prev2, w_prev, w)\n",
    "        log_prob += math.log(p + 1e-10)\n",
    "    return math.exp(-log_prob / l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_txt = ''.join(val_corpus)\n",
    "test_txt = ''.join(test_corpus)\n",
    "transformed_val = ngram_data.fit([val_txt])\n",
    "transformed_test = ngram_data.fit([test_txt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor valor de la perplejidad alcanzada es bastante alto, atribuible al reducido tamano del corpus asi como a la presencia de basura en los documentos empleados para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad en validación: 828.1654233712904\n",
      "\n",
      "Perplejidad en validación: 480.6488924929459\n",
      "\n",
      "Perplejidad en validación: 598.923852110968\n",
      "\n",
      "Mejores lambdas seleccionadas: (0.1, 0.2, 0.3, 0.4)\n"
     ]
    }
   ],
   "source": [
    "# Definir tres conjuntos de lambdas\n",
    "lambda_sets = [\n",
    "    (0.4, 0.3, 0.2, 0.1),\n",
    "    (0.1, 0.2, 0.3, 0.4),\n",
    "    (0.1, 0.4, 0.4, 0.1)\n",
    "]\n",
    "\n",
    "best_lambda = None\n",
    "best_perplexity = float('inf')\n",
    "\n",
    "for lambdas in lambda_sets:\n",
    "    tetra_lm.set_lambdas(*lambdas)\n",
    "    \n",
    "    # Calcular perplejidad en validación\n",
    "    val_perplexity = perplexity(tetra_lm, transformed_val[0])\n",
    "    print(f\"Perplejidad en validación: {val_perplexity}\\n\")\n",
    "    \n",
    "    # Guardar el mejor modelo\n",
    "    if val_perplexity < best_perplexity:\n",
    "        best_perplexity = val_perplexity\n",
    "        best_lambda = lambdas\n",
    "\n",
    "print(f\"Mejores lambdas seleccionadas: {best_lambda}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La perplejidad medida en el corpus de prueba es bastante similar al observado en el conjunto de validacion, indicando un buen desempeno del modelo al tratar con secuencias no vistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad en test: 483.3050425132002\n"
     ]
    }
   ],
   "source": [
    "tetra_lm.set_lambdas(*best_lambda)\n",
    "test_perplexity = perplexity(tetra_lm, transformed_test[0])\n",
    "print(f\"Perplejidad en test: {test_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mi companera Kathia Rangel Pompa me ayudo con la siguiente funcion, puesto que mi intento por implementarla tardaba demasiado tiempo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expectation_maximization(model, val_sequence, max_iter=10):\n",
    "    N = 4\n",
    "    lambdas = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    perplexities = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Entrenar modelo con lambdas actuales\n",
    "        model.set_lambdas(*lambdas)\n",
    "        \n",
    "        q_m = np.zeros((len(val_sequence)-3, N))\n",
    "\n",
    "        # E-step\n",
    "        for m in range(3, len(val_sequence)):\n",
    "            w_prev3, w_prev2, w_prev, w = val_sequence[m-3], val_sequence[m-2], val_sequence[m-1], val_sequence[m]\n",
    "\n",
    "            p_4 = model.tetragram_probability(w_prev3, w_prev2, w_prev, w)\n",
    "            p_3 = model.trigram_probability(w_prev2, w_prev, w)\n",
    "            p_2 = model.bigram_probability(w_prev, w)\n",
    "            p_1 = model.unigram_probability(w)\n",
    "\n",
    "            probs = np.array([p_4, p_3, p_2, p_1])\n",
    "\n",
    "            q = probs * lambdas\n",
    "            q /= q.sum() # Normalizamos\n",
    "            q_m[m-3] = q\n",
    "\n",
    "        # M-step\n",
    "        lambdas = q_m.mean(axis=0)\n",
    "\n",
    "        # Calcular perplejidad\n",
    "        ppl = perplexity(model, val_sequence)\n",
    "        perplexities.append(ppl)\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            print(f\"Iteration: {i}, Perplejidad: {ppl}\")\n",
    "\n",
    "    return lambdas, perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al emplear EM se alcanzo una perplejidad muy similar a la mejor de las observadas en los conjuntos de lambdas anteriores, y dado que la convergencia mejora muy poco entre las ultimas iteraciones parece indicar que posiblemente sea el mejor resultado alcanzable por el modelo en estas circunstancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Perplejidad: 589.2285644387551\n",
      "Iteration: 2, Perplejidad: 402.8371860169364\n",
      "Iteration: 4, Perplejidad: 389.1047611298884\n",
      "Iteration: 6, Perplejidad: 386.4106180111605\n",
      "Iteration: 8, Perplejidad: 385.6031428432458\n",
      "[2.41138675e-06 1.59294668e-03 6.39186374e-01 3.59218268e-01]\n"
     ]
    }
   ],
   "source": [
    "lambdas, perplexities = expectation_maximization(tetra_lm, transformed_val[0])\n",
    "print(lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(perplexities)\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"PPL\")\n",
    "plt.title(\"Evolución de la perplejidad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Texto\n",
    "Se propone que, al calcular la probabilidad del tetragrama que incluye como token final \"\\</s\\>\", se multiplique su probabilidad por un factor alpha. Con cada iteracion, el valor de alpha se vuelve un poco mas grande (se multiplica por si mismo) para que, conforme avanza el algoritmo se vuelva mas probable elegir ese token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = list(final_vocab)\n",
    "tetra_lm.set_lambdas(*lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar texto\n",
    "Para la generacion de texto, se elige una palabra inicial al azar (asegurandose de que no es un final de linea). Hecho esto, se van agregando palabras a la oracion generada, calculando las probabilidades de los n-gramas segun su tamano, y tomando las secuencias mas probables tomando todas las palabras del vocabulario.\n",
    "\n",
    "Fue necesario agregar una condicion para no agregar los tokens de inicio de linea ni de palabra desconocida dentro de la oracion. El hecho de que la aparicion de inicios de linea **despues** de otro token podria deberse a que, dado que el modelo empleado toma en cuenta unigramas, la probabilidad de aparicion de este token sea mayor a la de otras palabras en el contexto dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(V, model, max_words=50, n=15, alpha=1.00001):\n",
    "    sentence = []\n",
    "\n",
    "    # Asegurar que la primera palabra no sea \"</s>\"\n",
    "    while not sentence or sentence[0] == \"</s>\":\n",
    "        sentence = [random.choice(V)]\n",
    "\n",
    "    for _ in range(max_words - 1):\n",
    "        candidates = []\n",
    "        for word in V:\n",
    "            if word in [\"<s>\", \"<unk>\"]:  # No se debe generar <s> ni <unk> dentro de la oración\n",
    "                continue\n",
    "\n",
    "            if len(sentence) == 1:\n",
    "                prob = model.bigram_probability(sentence[-1], word)\n",
    "            elif len(sentence) == 2:\n",
    "                prob = model.trigram_probability(sentence[-2], sentence[-1], word)\n",
    "            else:\n",
    "                prob = model.probability_of_word(sentence[-3], sentence[-2], sentence[-1], word)\n",
    "\n",
    "            # Aumentar la probabilidad de `</s>` a medida que la oración crece\n",
    "            if word == \"</s>\":\n",
    "                prob *= alpha\n",
    "                alpha *= alpha\n",
    "\n",
    "            candidates.append((prob, word))\n",
    "\n",
    "        # Ordenar por probabilidad descendente y tomar los mejores n candidatos\n",
    "        candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "        words = [word for _, word in candidates[:n]]\n",
    "        probs = [prob for prob, _ in candidates[:n]]\n",
    "\n",
    "        # Elegir palabra de manera ponderada por probabilidad\n",
    "        word = random.choices(words, weights=probs, k=1)[0]\n",
    "        sentence.append(word)\n",
    "\n",
    "        if word == \"</s>\":\n",
    "            break\n",
    "\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar las secuencias generadas es evidente la aparicion de secuencias repetidas, debido a que se trata de palabras que suelen ser muy utilizadas en el mismo contexto y en secuencias diferentes (por ejemplo, \"y el de la que se\"), volviendo su repeticion en la secuencia bastante probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ponerme acudió atención porque si hay un en un la gente porque las elecciones de la corrupción </s>\n",
      "exconvento atención importaran el los pueblos es que se que la conferencia de méxico y de que de la </s>\n",
      "compañeritos prosperó importaran a el caso de méxico </s>\n",
      "detuvo a juan y </s>\n",
      "azvi prosperó calumniaban por ciento el caso de la defensa </s>\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_text(V, tetra_lm, n=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar texto con semilla\n",
    "La modificacion hecha respecto a la funcion anterior consistio en eliminar la eleccion de una palabra inicial y los casos para oraciones de menos de 3 palabras, quedando solo la generacion a partir de tetragramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text_from_seed(seed, V, model, max_words=50, n=15, alpha=1.00001):\n",
    "    sentence = seed\n",
    "\n",
    "    for _ in range(max_words - 1):\n",
    "        candidates = []\n",
    "        for word in V:\n",
    "            if word in [\"<s>\", \"<unk>\"]:  # No se debe generar <s> ni <unk> dentro de la oración\n",
    "                continue\n",
    "\n",
    "            prob = model.probability_of_word(sentence[-3], sentence[-2], sentence[-1], word)\n",
    "\n",
    "            # Aumentar la probabilidad de `</s>` a medida que la oración crece\n",
    "            if word == \"</s>\":\n",
    "                prob *= alpha\n",
    "                alpha *= alpha\n",
    "\n",
    "            candidates.append((prob, word))\n",
    "\n",
    "        # Ordenar por probabilidad descendente y tomar los mejores n candidatos\n",
    "        candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "        words = [word for _, word in candidates[:n]]\n",
    "        probs = [prob for prob, _ in candidates[:n]]\n",
    "\n",
    "        # Elegir palabra de manera ponderada por probabilidad\n",
    "        word = random.choices(words, weights=probs, k=1)[0]\n",
    "        sentence.append(word)\n",
    "\n",
    "        if word == \"</s>\":\n",
    "            break\n",
    "\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan secuencias mas largas ligeramente mas coherentes que las observables en el caso anterior, dado que el contexto ingresado reduce bastante las secuencias que pueden generarse limitandolas a oraciones mas reconocibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> y no en de las </s>\n",
      "andrés manuel lópez obrador bueno el gobierno federal </s>\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_from_seed([\"<s>\", \"<s>\", \"<s>\"], V, tetra_lm))\n",
    "print(generate_text_from_seed([\"andrés\", \"manuel\", \"lópez\"], V, tetra_lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutacion de tokens\n",
    "Una vez generadas todas las permutaciones, se calculan las probabilidades de todas, se ordenan y se devuelven las 5 mayores y las 5 menores, observando correctamente como las primeras tienen mas sentido que las segundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def top_permutations(model, sentence, k = 5):\n",
    "    tokens = sentence.split()\n",
    "    p = list(permutations(tokens))\n",
    "    probs = []\n",
    "\n",
    "    for sequence in p:\n",
    "        p = math.exp(model.sequence_probability(sequence))\n",
    "        probs.append((' '.join(sequence), p))\n",
    "\n",
    "    probs = sorted(probs, key=lambda x: x[1])\n",
    "    top = probs[-k:]\n",
    "    bottom = probs[:k]\n",
    "\n",
    "    return top, bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración: sino gano me voy a la chingada\n",
      "\n",
      "Top:\n",
      "chingada sino gano me voy a la - 5.982246272716484e-08\n",
      "sino gano chingada me voy a la - 5.982452086903397e-08\n",
      "gano sino chingada me voy a la - 5.982452086903397e-08\n",
      "gano chingada sino me voy a la - 6.987079996692766e-08\n",
      "chingada gano sino me voy a la - 6.987079996692766e-08\n",
      "\n",
      "Bottom:\n",
      "me la a chingada sino voy gano - 1.082067184003965e-18\n",
      "la me a chingada sino voy gano - 1.082079333266404e-18\n",
      "me la a chingada voy gano sino - 1.0922775094238117e-18\n",
      "la me a chingada voy gano sino - 1.0922897733260035e-18\n",
      "me la a chingada voy sino gano - 1.1388677682246309e-18\n",
      "\n",
      "\n",
      "Oración: ya se va a acabar la corrupción\n",
      "\n",
      "Top:\n",
      "acabar ya se va a la corrupción - 3.2410238553290115e-06\n",
      "ya acabar corrupción se va a la - 3.5814492916629095e-06\n",
      "acabar ya corrupción se va a la - 3.5814492916629095e-06\n",
      "corrupción acabar ya se va a la - 2.3057249021089386e-05\n",
      "acabar corrupción ya se va a la - 2.3057284412373972e-05\n",
      "\n",
      "Bottom:\n",
      "se a la acabar va ya corrupción - 6.927360298202056e-15\n",
      "a se la acabar va ya corrupción - 6.9341980715986535e-15\n",
      "la a se acabar va ya corrupción - 7.30319118387357e-15\n",
      "a la se acabar va ya corrupción - 7.303199542252052e-15\n",
      "se a la acabar ya corrupción va - 7.756448264978614e-15\n",
      "\n",
      "\n",
      "Oración: el presidente de la republica\n",
      "\n",
      "Top:\n",
      "presidente el republica de la - 0.001527366665235028\n",
      "presidente republica el de la - 0.00246420443340866\n",
      "republica presidente el de la - 0.002464344919998928\n",
      "el republica presidente de la - 0.0034753977879176242\n",
      "republica el presidente de la - 0.0034791189619930107\n",
      "\n",
      "Bottom:\n",
      "la el de republica presidente - 2.1040248221069145e-09\n",
      "el la de republica presidente - 2.112535143034166e-09\n",
      "la de el republica presidente - 3.8454685001881855e-09\n",
      "de la el republica presidente - 3.847816379282011e-09\n",
      "la presidente de republica el - 8.207064956471403e-09\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oraciones = [\"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\", \"el presidente de la republica\"]\n",
    "for o in oraciones:\n",
    "    top, bottom = top_permutations(tetra_lm, o)\n",
    "    print(f\"Oración: {o}\")\n",
    "    print(\"\\nTop:\")\n",
    "    for seq, prob in top:\n",
    "        print(f\"{seq} - {prob}\")\n",
    "    print(\"\\nBottom:\")\n",
    "    for seq, prob in bottom:\n",
    "        print(f\"{seq} - {prob}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 palabras mas probables dadas las 3 primeras\n",
    "Ayude a mi companero Isaac Barron Jimenez con la siguiente funcion\n",
    "\n",
    "Se observa que las palabras devueltas por el algoritmo tienen, efectivamente, sentido dentro del contexto ingresado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5_next_word(model, word1, word2, word3):\n",
    "    probs = [(w, model.probability_of_word(word1, word2, word3, w)) for w in model.vocab]\n",
    "    probs = sorted(probs, key=lambda x: x[1], reverse=True)\n",
    "    return probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('obrador', 0.3370457999973343), ('<s>', 0.03978256940808791), ('de', 0.017766097468775645), ('</s>', 0.013621542215995961), ('que', 0.011950694070485714)]\n"
     ]
    }
   ],
   "source": [
    "print(top5_next_word(tetra_lm, \"andrés\", \"manuel\", \"lópez\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El Ahorcado\n",
    "Se implifico el codigo para que, en lugar de generar todas las posibles palabras que se forman por permutacion, insercion y eliminacion de letras, genere solo aquellas que se producen al reemplazar un \"\\_\" por una letra.\n",
    "\n",
    "Entre las pruebas se incluyen strings de 3 y 4 \"\\_\", con lo que el algoritmo devuelve las palabras mas comunes de 3 y 4 letras, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../../Corpus/big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def hangman(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(generate_possible_words(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "from itertools import product\n",
    "import string\n",
    "\n",
    "def generate_possible_words(pattern):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    blanks = pattern.count('_')  # Contar la cantidad de '_'\n",
    "    \n",
    "    # Generar todas las combinaciones posibles de letras para los espacios vacíos\n",
    "    replacements = product(letters, repeat=blanks)\n",
    "    \n",
    "    words = []\n",
    "    for replacement in replacements:\n",
    "        temp_pattern = list(pattern)  # Convertir el string en una lista mutable\n",
    "        rep_index = 0  # Índice para recorrer replacement\n",
    "        \n",
    "        # Reemplazar cada '_' por una letra del alfabeto\n",
    "        for i, char in enumerate(temp_pattern):\n",
    "            if char == '_':\n",
    "                temp_pattern[i] = replacement[rep_index]\n",
    "                rep_index += 1\n",
    "        \n",
    "        words.append(\"\".join(temp_pattern))  # Convertir lista a string y agregar a la lista de palabras\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe_p_e -> people\n",
      "phi__sop_y -> philosophy\n",
      "si_nif_c_nc_ -> significance\n",
      "c_mp_t_r -> computer\n",
      "a_tifi__a_ -> artificial\n",
      "_m__nit_ -> immunity\n",
      "a__le_e -> athlete\n",
      "f_r_e -> force\n",
      "___ -> the\n",
      "____ -> that\n"
     ]
    }
   ],
   "source": [
    "patterns = [\"pe_p_e\", \"phi__sop_y\", \"si_nif_c_nc_\", \"c_mp_t_r\", \"a_tifi__a_\", \"_m__nit_\", \"a__le_e\", \"f_r_e\", \"___\", \"____\"]\n",
    "for pattern in patterns:\n",
    "    print(f'{pattern} -> {hangman(pattern)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una manera de integrar el modelo de Norvig al modelo de lenguaje seria que, al analizar secuencias que posean una probabilidad de ocurrencia anormalmente baja (\"anormal\" respecto a algun umbral), encuentre la palabra que esta fuera de lugar al calcular su probabilidad de ocurrencia dentro del contexto y corregirla con el modelo de Norvig. Esto encontraria y corregiria errores como el caso de \"In the science off Maths ...\" asi como otros mas complejos, por ejemplo el caso de que mas de una palabra este escrita incorrectamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
