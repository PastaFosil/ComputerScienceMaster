{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83c9934",
   "metadata": {},
   "source": [
    "# Juan Carlos Perez Ramirez\n",
    "## Procesamiento de Lenguaje Natural\n",
    "## Examen 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7fd3a",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1126b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# Rutas de entrada y salida\n",
    "split = \"train\"\n",
    "dir = \"es_\" + split\n",
    "path = \"../../corpus/examen2/\"\n",
    "truth = path + dir + '/truth.txt'  # archivo con los identificadores\n",
    "xml_dir = path + dir  # carpeta donde están los archivos XML\n",
    "docs_file = path + split + '.csv'\n",
    "\n",
    "# abrimos los archivos de salida\n",
    "with open(docs_file, 'w', encoding='utf-8') as f_textos, \\\n",
    "     open(truth, 'r', encoding='utf-8') as f_in:\n",
    "\n",
    "    for linea in f_in:\n",
    "        partes = linea.strip().split(\":::\")\n",
    "        if len(partes) != 3:\n",
    "            continue  # Saltamos líneas mal formateadas\n",
    "\n",
    "        identificador = partes[0]\n",
    "        genero = partes[1]\n",
    "        nacionalidad = partes[2]\n",
    "        archivo_xml = os.path.join(xml_dir, f\"{identificador}.xml\")\n",
    "\n",
    "        if not os.path.exists(archivo_xml):\n",
    "            print(f\"Archivo {archivo_xml} no encontrado, saltando.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(archivo_xml)\n",
    "            root = tree.getroot()\n",
    "            documentos = root.find('documents')\n",
    "\n",
    "            # Guardamos todos los tweets en una lista\n",
    "            tweets = []\n",
    "            for doc in documentos.findall('document'):\n",
    "                texto = doc.text.strip()\n",
    "                texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)  # elimina URLs\n",
    "                texto = re.sub(r\"@\\w+\", \"<user>\", texto) # sustituye menciones por \"<user>\"\n",
    "                texto = texto.replace('\\n', ' ')  # elimina saltos de línea dentro del tweet\n",
    "                texto = texto.lower()  # pasar a minúsculas\n",
    "                tweets.append(texto)\n",
    "\n",
    "            if tweets:\n",
    "                linea_usuario = \" </s> \".join(tweets) + \" </s> \" + genero + \" </s> \" + nacionalidad + \"\\n\"\n",
    "                f_textos.write(linea_usuario)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {archivo_xml}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a337759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancho/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class AuthorProfilingDataset(Dataset):\n",
    "    def __init__(self, path_txt, max_len=1000, tokenizer=tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.load_data(path_txt)\n",
    "    \n",
    "\n",
    "    def load_data(self, path_txt):\n",
    "        texts = []\n",
    "        genders = []\n",
    "        nationalities = []\n",
    "\n",
    "        with open(path_txt, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if \"</s>\" in line:\n",
    "                    partes = line.strip().split(\"</s>\")\n",
    "                    if len(partes) < 3:\n",
    "                        continue\n",
    "                    *tweets, genero, nacionalidad = partes\n",
    "                    texto_completo = \" </s> \".join(tweets)  # concatena los tweets\n",
    "                    texts.append(texto_completo)\n",
    "                    genders.append(genero)\n",
    "                    nationalities.append(nacionalidad)\n",
    "\n",
    "\n",
    "        self.data = pd.DataFrame({'text': texts, 'gender': genders, 'nationality': nationalities})\n",
    "        self.data['gender'] = self.data['gender'].str.lower().str.strip()\n",
    "        self.data['nationality'] = self.data['nationality'].str.lower().str.strip()\n",
    "\n",
    "        self.gender_map = {'male': 0, 'female': 1}\n",
    "        self.nat_map = {l: i for i, l in enumerate(sorted(self.data['nationality'].unique()))}\n",
    "        self.data['gender'] = self.data['gender'].map(self.gender_map)\n",
    "        self.data['nationality'] = self.data['nationality'].map(self.nat_map)\n",
    "\n",
    "        print(f\"Nationalities: {self.nat_map}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        gender = self.data.iloc[idx]['gender']\n",
    "        nationality = self.data.iloc[idx]['nationality']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['gender'] = torch.tensor(gender, dtype=torch.long)\n",
    "        item['nationality'] = nationality\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3db8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BERTAuthorClassifier(nn.Module):\n",
    "    def __init__(self, n_generos, n_nacionalidades):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_genero = nn.Linear(hidden_size, n_generos)\n",
    "        self.fc_nacionalidad = nn.Linear(hidden_size, n_nacionalidades)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # [CLS] embedding\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        genero_logits = self.fc_genero(pooled_output)\n",
    "        nacionalidad_logits = self.fc_nacionalidad(pooled_output)\n",
    "        return genero_logits, nacionalidad_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823b69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        genero = batch[\"gender\"].to(device)\n",
    "        nacionalidad = batch[\"nationality\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out_gen, out_nat = model(input_ids, attention_mask)\n",
    "\n",
    "        loss_gen = F.cross_entropy(out_gen, genero)\n",
    "        loss_nat = F.cross_entropy(out_nat, nacionalidad)\n",
    "        loss = loss_gen + loss_nat\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df27abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationalities: {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancho/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÉpoca \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pérdida = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 22\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = AuthorProfilingDataset(\"../../corpus/examen2/train.csv\", max_len=512, tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTAuthorClassifier(n_generos=2, n_nacionalidades=len(train_dataset.nat_map)).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Época {epoch+1}: pérdida = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a471e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeaad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
