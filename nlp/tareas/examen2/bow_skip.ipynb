{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b69f8ca",
   "metadata": {},
   "source": [
    "# Juan Carlos Perez Ramirez\n",
    "## Procesamiento de Lenguaje Natural\n",
    "## Examen 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe979d3",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3475d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/juancho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98525b95",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de entrada y salida\n",
    "split = \"test\"\n",
    "dir = \"es_\" + split\n",
    "path = \"../../corpus/examen2/\"\n",
    "truth = path + dir + '/truth_order.txt'  # archivo con los identificadores\n",
    "xml_dir = path + dir  # carpeta donde están los archivos XML\n",
    "docs_file = path + split + '.csv'\n",
    "\n",
    "# abrimos los archivos de salida\n",
    "with open(docs_file, 'w', encoding='utf-8') as f_textos, \\\n",
    "     open(truth, 'r', encoding='utf-8') as f_in:\n",
    "\n",
    "    for linea in f_in:\n",
    "        partes = linea.strip().split(\":::\")\n",
    "        if len(partes) != 3:\n",
    "            continue  # Saltamos líneas mal formateadas\n",
    "\n",
    "        identificador = partes[0]\n",
    "        genero = partes[1]\n",
    "        nacionalidad = partes[2]\n",
    "        archivo_xml = os.path.join(xml_dir, f\"{identificador}.xml\")\n",
    "\n",
    "        if not os.path.exists(archivo_xml):\n",
    "            print(f\"Archivo {archivo_xml} no encontrado, saltando.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(archivo_xml)\n",
    "            root = tree.getroot()\n",
    "            documentos = root.find('documents')\n",
    "\n",
    "            # Guardamos todos los tweets en una lista\n",
    "            tweets = []\n",
    "            for doc in documentos.findall('document'):\n",
    "                texto = doc.text.strip()\n",
    "                texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)  # elimina URLs\n",
    "                texto = re.sub(r\"@\\w+\", \"<user>\", texto) # sustituye menciones por \"<user>\"\n",
    "                texto = texto.replace('\\n', ' ')  # elimina saltos de línea dentro del tweet\n",
    "                texto = texto.lower()  # pasar a minúsculas\n",
    "                tweets.append(texto)\n",
    "\n",
    "            if tweets:\n",
    "                linea_usuario = \" </s> \".join(tweets) + \" </s> \" + genero + \" </s> \" + nacionalidad + \"\\n\"\n",
    "                f_textos.write(linea_usuario)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {archivo_xml}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af43580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('spanish')))\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def k_skip_n_grams(tokens, n=2, k=1):\n",
    "    skip_ngrams = []\n",
    "    length = len(tokens)\n",
    "    for i in range(length):\n",
    "        window = tokens[i:i+k+n]\n",
    "        if len(window) >= n:\n",
    "            for indices in combinations(range(len(window)), n):\n",
    "                if max(indices) - min(indices) <= k + n - 1:\n",
    "                    ngram = [window[idx] for idx in indices]\n",
    "                    skip_ngrams.append(\"_\".join(ngram))\n",
    "    return skip_ngrams\n",
    "\n",
    "def multi_skipgrams(texto, n_vals=[2], k_vals=[1], tokenizer=tokenizer):\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    all_skips = []\n",
    "\n",
    "    for n in n_vals:\n",
    "        for k in k_vals:\n",
    "            skips = k_skip_n_grams(tokens, n=n, k=k)\n",
    "            all_skips.extend(skips)\n",
    "\n",
    "    return \" \".join(all_skips)\n",
    "\n",
    "def load_data(path_txt, n_skip, k):\n",
    "    texts = []\n",
    "    texts_skip = []\n",
    "    genders = []\n",
    "    nationalities = []\n",
    "\n",
    "    with open(path_txt, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if \"</s>\" in line:\n",
    "                partes = line.strip().split(\"</s>\")\n",
    "                if len(partes) < 3:\n",
    "                    continue\n",
    "                *tweets, genero, nacionalidad = partes\n",
    "                texto_completo = \" [SEP] \".join(tweets)\n",
    "                texts.append(texto_completo)\n",
    "                texts_skip.append(multi_skipgrams(texto_completo, n_vals=n_skip, k_vals=k))\n",
    "                genders.append(genero)\n",
    "                nationalities.append(nacionalidad)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'text_skip': texts_skip,\n",
    "        'gender': genders,\n",
    "        'nationality': nationalities\n",
    "    })\n",
    "\n",
    "    gender_map = {' male ': 0, ' female ': 1}\n",
    "    nat_map = {l: i for i, l in enumerate(sorted(df['nationality'].unique()))}\n",
    "    df['gender'] = df['gender'].map(gender_map)\n",
    "    df['nationality'] = df['nationality'].map(nat_map)\n",
    "\n",
    "    print(f\"Nationalities: {nat_map}\")\n",
    "    return df, gender_map, nat_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa213d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationalities: {' argentina': 0, ' chile': 1, ' colombia': 2, ' mexico': 3, ' peru': 4, ' spain': 5, ' venezuela': 6}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "path_train = \"../../corpus/examen2/train.csv\"\n",
    "path_val = \"../../corpus/examen2/val.csv\"\n",
    "path_test = \"../../corpus/examen2/test.csv\"\n",
    "\n",
    "df, gender_map, nat_map = load_data(path_train, n_skip=[3], k=[1])\n",
    "\n",
    "#df_test, _, _ = load_data(path_test)\n",
    "\n",
    "vectorizer_ngram = TfidfVectorizer(max_features=40000, ngram_range=(1, 3)) # para n-gramas\n",
    "vectorizer_skip = TfidfVectorizer(max_features=10000) # para skip-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34dd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gramas\n",
    "X_ngram = vectorizer_ngram.fit_transform(df['text'])\n",
    "\n",
    "# skip-gramas\n",
    "X_skip = vectorizer_skip.fit_transform(df['text_skip'])\n",
    "\n",
    "X = hstack([X_ngram, X_skip])\n",
    "\n",
    "y_gender = df['gender']\n",
    "y_nat = df['nationality']\n",
    "\n",
    "del X_ngram, X_skip, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cbe89d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nationalities: {' argentina': 0, ' chile': 1, ' colombia': 2, ' mexico': 3, ' peru': 4, ' spain': 5, ' venezuela': 6}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False)  # TF-IDF es sparse\n",
    "\n",
    "df_val, _, _ = load_data(path_val, n_skip=[3], k=[1])\n",
    "X_ngram = vectorizer_ngram.fit_transform(df_val['text']) # n-gramas\n",
    "X_skip = vectorizer_skip.fit_transform(df_val['text_skip']) # skip-gramas\n",
    "X_val = hstack([X_ngram, X_skip])\n",
    "\n",
    "y_gender_val = df_val['gender']\n",
    "y_nat_val = df_val['nationality']\n",
    "\n",
    "del X_ngram, X_skip, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6197b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97ad3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación de género:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.43      0.47       420\n",
      "           1       0.51      0.60      0.55       420\n",
      "\n",
      "    accuracy                           0.52       840\n",
      "   macro avg       0.52      0.52      0.51       840\n",
      "weighted avg       0.52      0.52      0.51       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200, 50), max_iter=300, random_state=42)\n",
    "clf.fit(X, y_gender)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Clasificación de género:\\n\", classification_report(y_gender_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50350480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificación de nacionalidad:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.02      0.03       120\n",
      "           1       0.40      0.02      0.03       120\n",
      "           2       0.40      0.02      0.03       120\n",
      "           3       0.15      0.82      0.25       120\n",
      "           4       0.26      0.05      0.08       120\n",
      "           5       0.17      0.03      0.06       120\n",
      "           6       0.26      0.22      0.24       120\n",
      "\n",
      "    accuracy                           0.17       840\n",
      "   macro avg       0.26      0.17      0.10       840\n",
      "weighted avg       0.26      0.17      0.10       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(200, 50), max_iter=300, random_state=42)\n",
    "\n",
    "clf.fit(X, y_nat)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Clasificación de nacionalidad:\\n\", classification_report(y_nat_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a0588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
