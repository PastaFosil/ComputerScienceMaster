{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juan Carlos Perez Ramirez\n",
    "## Procesamiento de Lenguaje Natural\n",
    "## Tarea 5: Modelos de Lenguaje Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace  # Guardar variables y par√°metros\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesamiento\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tqmd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los embeddings preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>-1.641680</td>\n",
       "      <td>1.447671</td>\n",
       "      <td>-2.283216</td>\n",
       "      <td>-1.965226</td>\n",
       "      <td>-0.222943</td>\n",
       "      <td>5.105217</td>\n",
       "      <td>-0.120701</td>\n",
       "      <td>-0.126822</td>\n",
       "      <td>-3.177338</td>\n",
       "      <td>-3.454396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399476</td>\n",
       "      <td>0.887398</td>\n",
       "      <td>-1.994891</td>\n",
       "      <td>2.912205</td>\n",
       "      <td>3.257981</td>\n",
       "      <td>-2.599953</td>\n",
       "      <td>-1.995134</td>\n",
       "      <td>-0.176465</td>\n",
       "      <td>3.946902</td>\n",
       "      <td>-2.792494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>que</th>\n",
       "      <td>2.167917</td>\n",
       "      <td>-2.581055</td>\n",
       "      <td>-3.290037</td>\n",
       "      <td>-2.788440</td>\n",
       "      <td>1.930902</td>\n",
       "      <td>-2.482932</td>\n",
       "      <td>-2.540504</td>\n",
       "      <td>0.918257</td>\n",
       "      <td>-0.375576</td>\n",
       "      <td>-4.981284</td>\n",
       "      <td>...</td>\n",
       "      <td>2.166590</td>\n",
       "      <td>0.142203</td>\n",
       "      <td>1.698710</td>\n",
       "      <td>-1.105366</td>\n",
       "      <td>6.916267</td>\n",
       "      <td>-3.977985</td>\n",
       "      <td>3.989532</td>\n",
       "      <td>-1.923247</td>\n",
       "      <td>2.035671</td>\n",
       "      <td>2.539047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>0.727598</td>\n",
       "      <td>-3.604198</td>\n",
       "      <td>0.793098</td>\n",
       "      <td>-4.076829</td>\n",
       "      <td>-0.050293</td>\n",
       "      <td>8.435444</td>\n",
       "      <td>-2.233952</td>\n",
       "      <td>2.568616</td>\n",
       "      <td>-2.221771</td>\n",
       "      <td>-1.738045</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.224793</td>\n",
       "      <td>6.436301</td>\n",
       "      <td>-5.083917</td>\n",
       "      <td>-3.572159</td>\n",
       "      <td>3.077797</td>\n",
       "      <td>1.510428</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>2.240481</td>\n",
       "      <td>0.825278</td>\n",
       "      <td>-5.749021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4.178944</td>\n",
       "      <td>1.255945</td>\n",
       "      <td>-0.714395</td>\n",
       "      <td>0.245530</td>\n",
       "      <td>-4.155548</td>\n",
       "      <td>1.866886</td>\n",
       "      <td>0.096566</td>\n",
       "      <td>1.327916</td>\n",
       "      <td>2.054787</td>\n",
       "      <td>0.943383</td>\n",
       "      <td>...</td>\n",
       "      <td>3.369879</td>\n",
       "      <td>3.655371</td>\n",
       "      <td>-1.750744</td>\n",
       "      <td>2.508173</td>\n",
       "      <td>6.827713</td>\n",
       "      <td>-5.676323</td>\n",
       "      <td>1.920513</td>\n",
       "      <td>-1.108223</td>\n",
       "      <td>3.379427</td>\n",
       "      <td>-1.197493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.799879</td>\n",
       "      <td>-0.130996</td>\n",
       "      <td>-1.711219</td>\n",
       "      <td>-0.395188</td>\n",
       "      <td>-0.195817</td>\n",
       "      <td>2.218104</td>\n",
       "      <td>-0.453679</td>\n",
       "      <td>2.695769</td>\n",
       "      <td>-4.302141</td>\n",
       "      <td>-3.187639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.321977</td>\n",
       "      <td>3.195324</td>\n",
       "      <td>-3.616187</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>4.989526</td>\n",
       "      <td>-4.719704</td>\n",
       "      <td>0.692478</td>\n",
       "      <td>-1.918830</td>\n",
       "      <td>2.808852</td>\n",
       "      <td>0.732657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7    \\\n",
       "0                                                                           \n",
       "de  -1.641680  1.447671 -2.283216 -1.965226 -0.222943  5.105217 -0.120701   \n",
       "que  2.167917 -2.581055 -3.290037 -2.788440  1.930902 -2.482932 -2.540504   \n",
       "la   0.727598 -3.604198  0.793098 -4.076829 -0.050293  8.435444 -2.233952   \n",
       "a    4.178944  1.255945 -0.714395  0.245530 -4.155548  1.866886  0.096566   \n",
       "y    0.799879 -0.130996 -1.711219 -0.395188 -0.195817  2.218104 -0.453679   \n",
       "\n",
       "          8         9         10   ...       91        92        93   \\\n",
       "0                                  ...                                 \n",
       "de  -0.126822 -3.177338 -3.454396  ...  0.399476  0.887398 -1.994891   \n",
       "que  0.918257 -0.375576 -4.981284  ...  2.166590  0.142203  1.698710   \n",
       "la   2.568616 -2.221771 -1.738045  ... -2.224793  6.436301 -5.083917   \n",
       "a    1.327916  2.054787  0.943383  ...  3.369879  3.655371 -1.750744   \n",
       "y    2.695769 -4.302141 -3.187639  ...  1.321977  3.195324 -3.616187   \n",
       "\n",
       "          94        95        96        97        98        99        100  \n",
       "0                                                                          \n",
       "de   2.912205  3.257981 -2.599953 -1.995134 -0.176465  3.946902 -2.792494  \n",
       "que -1.105366  6.916267 -3.977985  3.989532 -1.923247  2.035671  2.539047  \n",
       "la  -3.572159  3.077797  1.510428  0.011997  2.240481  0.825278 -5.749021  \n",
       "a    2.508173  6.827713 -5.676323  1.920513 -1.108223  3.379427 -1.197493  \n",
       "y    0.961742  4.989526 -4.719704  0.692478 -1.918830  2.808852  0.732657  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_df = pd.read_csv(\"word2vec_col.txt\", skiprows=1, header=None, sep=' ')\n",
    "embs_df.set_index(0,inplace=True)\n",
    "embs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierte el dataframe a un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_dict = embs_df.apply(lambda row: row.tolist(), axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)  # python seed\n",
    "np.random.seed(seed)  # numpy seed\n",
    "torch.manual_seed(seed)  # torch seed\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "  \"\"\"\n",
    "  Funci√≥n para leer los archivos de tuits. Cada l√≠nea (tuit) ser√° un elemento de la lista.\n",
    "  \"\"\"\n",
    "  tr_txt = []\n",
    "  tr_y = []\n",
    "  with open(path_corpus) as f_corpus, open(path_truth) as f_truth:\n",
    "    for tweet in f_corpus:\n",
    "      tr_txt.append(tweet)\n",
    "    for label in f_truth:\n",
    "      tr_y.append(label)\n",
    "  return tr_txt, tr_y\n",
    "\n",
    "\n",
    "PATH = \"../../corpus/\"\n",
    "\n",
    "X_train, y_train = get_texts_from_file(\n",
    "  PATH + \"/mex20_train.txt\",\n",
    "  PATH + \"/mex20_train_labels.txt\")\n",
    "\n",
    "X_val, y_val = get_texts_from_file(\n",
    "  PATH + \"/mex20_val.txt\",\n",
    "  PATH + \"/mex20_val_labels.txt\")\n",
    "\n",
    "y_train = list(map(int, y_train))\n",
    "y_val = list(map(int, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "  \"\"\"\n",
    "  Toma un corpus y a trav√©s de los m√©todos fit y transform, se crea una lista de \n",
    "  n-gramas pensada para el entrenamiento de la CBOW.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               N: int,\n",
    "               vocab_max: int = 5000,\n",
    "               tokenizer: callable = None,\n",
    "               embeddings: dict = None):\n",
    "    \"\"\"\n",
    "    Constructor de la clase.\n",
    "\n",
    "    Args:\n",
    "        N (int): Tama√±o de los n-gramas.\n",
    "        vocab_max (int, optional): Tama√±o m√°ximo del vocabulario a considerar. Defaults to 5000.\n",
    "        tokenizier (callable, optional): Tokenizador. Defaults to None.\n",
    "        embeddings (np.ndarray, optional): Matriz de embeddings pre-entrenada. Debe entrar en el orden en el que entran las palabras. Defaults to None.\n",
    "    \"\"\"\n",
    "    self.N = N\n",
    "    self.vocab_max = vocab_max\n",
    "    self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "    self.embeddings = embeddings\n",
    "\n",
    "    if self.embeddings:\n",
    "      self.embedding_size = len(list(embeddings.values())[0])\n",
    "\n",
    "    # tokens a ignorar\n",
    "    self.punct = ['.', ',', ';', ':', '-', '^', '\"',\n",
    "                  '\"', '!', '¬°', '¬ø', '?', '<url>', '#', '@usuario',\n",
    "                  '\\n', '\\t', '(', ')', \"'\", '\"', '$', '&', '*', '@', '_', '<', '>', '/', '‚Äú', '‚Äù']\n",
    "\n",
    "    # tokens especiales\n",
    "    self.UNK = \"<unk>\"\n",
    "    self.SOS = \"<s>\"\n",
    "    self.EOS = \"</s>\"\n",
    "\n",
    "  def get_vocab_size(self) -> int:\n",
    "    \"\"\"\n",
    "    Devuelve el tama√±o del vocabulario.\n",
    "\n",
    "    Returns:\n",
    "        int: Tama√±o del vocabulario.\n",
    "    \"\"\"\n",
    "    return len(self.vocab)\n",
    "\n",
    "  def default_tokenizer(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizador por defecto. Simplemente separa cada oraci√≥n por espacios.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento a tokenizar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    return doc.split(\" \")\n",
    "\n",
    "  def remove_word(self, word: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica si la palabra en cuesti√≥n debe eliminarse seg√∫n los siguientes criterios:\n",
    "    - Es un signo de puntuaci√≥n\n",
    "    - Es un d√≠gito\n",
    "\n",
    "    Args:\n",
    "        word (str): Palabra a evaluar.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si se elimina.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    is_punct = True if word in self.punct else False\n",
    "    is_digit = word.isnumeric()\n",
    "    return is_punct or is_digit\n",
    "\n",
    "  def sortFreqDist(self, freq_dist: nltk.FreqDist) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con el top de palabras por frecuencia. El tama√±o de la lista es self.vocab_max.\n",
    "\n",
    "    Args:\n",
    "        freq_dist (nltk.FreqDist): Objeto de frecuencias (nltk) del corpus considerado.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tama√±o self.vocab_max.\n",
    "    \"\"\"\n",
    "    freq_dist = dict(freq_dist)\n",
    "    # Aqu√≠ key es una funci√≥n que se aplica a cada par√°metro\n",
    "    # antes de compararlo. En este caso se pasa\n",
    "    # freq_dist.get para asegurarse de que el ordenamiento\n",
    "    # se haga por las frecuencias y no por orden alfab√©tico.\n",
    "    return sorted(freq_dist,\n",
    "                  key=freq_dist.get,\n",
    "                  reverse=True)\n",
    "\n",
    "  def get_vocab(self, corpus: list[str]) -> set:\n",
    "    \"\"\"\n",
    "    Devuelve el vocabulario a partir de un corpus dado.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Corpus del cual se quiere obtener el vocabulario. Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        set: Vocabulario.\n",
    "    \"\"\"\n",
    "    freq_dist = FreqDist(\n",
    "      [w.lower()\n",
    "       for sentence in corpus\n",
    "       for w in self.tokenizer(sentence)\n",
    "       if not self.remove_word(w)]\n",
    "    )\n",
    "    sorted_words = self.sortFreqDist(freq_dist)[:self.vocab_max-3]\n",
    "    return set(sorted_words)\n",
    "\n",
    "  def fit(self, corpus: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Carga el vocabulario y crea diccionarios de √≠ndices <-> palabras. Adem√°s, si se aporta una matriz de embeddings pre-entrenados, tambi√©n construye la submatriz con los elementos del vocabulario.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "    \"\"\"\n",
    "    # Cargamos el vocabulario\n",
    "    self.vocab = self.get_vocab(corpus)\n",
    "    self.vocab.add(self.UNK)\n",
    "    self.vocab.add(self.SOS)\n",
    "    self.vocab.add(self.EOS)\n",
    "\n",
    "    # Diccionarios palabras <-> ids\n",
    "    self.w2id = dict()\n",
    "    self.id2w = dict()\n",
    "\n",
    "    if self.embeddings:\n",
    "      self.embeddings_matrix = np.empty([self.vocab_max,\n",
    "                                         self.embedding_size])\n",
    "\n",
    "    id = 0\n",
    "    for doc in corpus:\n",
    "      for word in self.tokenizer(doc):\n",
    "        word_ = word.lower()\n",
    "        if (word_ in self.vocab) and (not word_ in self.w2id):\n",
    "          self.w2id[word_] = id\n",
    "          self.id2w[id] = word_\n",
    "\n",
    "          # Si se aporta una matriz de embeddings,\n",
    "          # aqu√≠ se crea la submatriz.\n",
    "          if self.embeddings:\n",
    "            if word_ in self.embeddings:\n",
    "              self.embeddings_matrix[id] = self.embeddings[word_]\n",
    "            else:\n",
    "              self.embeddings_matrix[id] = np.random.rand(\n",
    "                self.embedding_size)\n",
    "\n",
    "          id += 1\n",
    "\n",
    "    # A√±adirmos los tokens especiales a los diccionarios.\n",
    "    self.w2id.update(\n",
    "      {self.UNK: id,\n",
    "       self.SOS: id + 1,\n",
    "       self.EOS: id + 2}\n",
    "    )\n",
    "    self.id2w.update(\n",
    "      {id: self.UNK,\n",
    "       id + 1: self.SOS,\n",
    "       id + 2: self.EOS}\n",
    "    )\n",
    "\n",
    "  def get_ngram_doc(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con n-gramas de un documento dado.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento del que se quieren obtener los n-gramas.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de n-gramas.\n",
    "    \"\"\"\n",
    "    doc_tokens = self.tokenizer(doc)\n",
    "    doc_tokens = self.replace_unk(doc_tokens)\n",
    "    doc_tokens = [w.lower()\n",
    "                  for w in doc_tokens]\n",
    "    doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "\n",
    "    return list(nltk.ngrams(doc_tokens, self.N))\n",
    "\n",
    "  def replace_unk(self, doc_tokens: list[str]) -> list:\n",
    "    \"\"\"\n",
    "    Toma un lista de tokens e intercambia los tokens out-of-vocabulary por el token especial self.UNK.\n",
    "\n",
    "    Args:\n",
    "        doc_tokens (list[str]): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens procesada.\n",
    "    \"\"\"\n",
    "    for i, token in enumerate(doc_tokens):\n",
    "      if token.lower() not in self.vocab:\n",
    "        doc_tokens[i] = self.UNK\n",
    "    return doc_tokens\n",
    "\n",
    "  def transform(self, corpus: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Devuelve una tupla de arreglos de Numpy. El primero tendr√° los ids de las palabras en el contexto, mientras que la segunda el id de la palabra que se debe predecir.\n",
    "\n",
    "    Se piensa en un modelo de CBOW. Damos el contexto y queremos predecir la palabra que sigue.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arreglos de numpy con ids de los contextos y id de la palabra objetivo.\n",
    "    \"\"\"\n",
    "    X_ngrams = list()\n",
    "    y = []\n",
    "\n",
    "    for doc in corpus:\n",
    "      doc_ngram = self.get_ngram_doc(doc)\n",
    "      for words_window in doc_ngram:\n",
    "        words_window_ids = [self.w2id[w]\n",
    "                            for w in words_window]\n",
    "        X_ngrams.append(list(words_window_ids[:-1]))\n",
    "        y.append(words_window_ids[-1])\n",
    "\n",
    "    return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos nuestros datos usando la clase `NgramData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize, embs_dict)\n",
    "ngram_data.fit(X_train)\n",
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del vocabulario : 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tama√±o del vocabulario : {ngram_data.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean los objetos `TensorDataset` para guardar los datos de entrenamiento y validaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "args.batch_size = 64\n",
    "\n",
    "# Num of workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle=True)\n",
    "\n",
    "# Val\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : torch.Size([64, 3])\n",
      "y shape : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape : {batch[0].shape}')\n",
    "print(f'y shape : {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLanguageModel(nn.Module):\n",
    "  \"\"\"\n",
    "  Red neuronal de Bengio :)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, args, embeddings=None):\n",
    "    \"\"\"\n",
    "    Constructor  de la clase.\n",
    "\n",
    "    El modelo de red neuronal par lenguaje de Bengio tiene la siguiente estructura:\n",
    "    Para un modelo de n-gramas, se dan las primeras n-1 palabras como contexto y se intenta predecir la n-√©sima palabra.\n",
    "    (1) n-1 representaciones iniciales: suelen ser one-hot. Pero aqu√≠ se toman de NgramData.\n",
    "        x\n",
    "    (2) n-1 representaciones aprendidas de tama√±o m: se obtienen de manera individual (por palabra). \n",
    "        (x = Cx)\n",
    "        En esta implementaci√≥n C se inicia de manera aleatoria.\n",
    "    (3) Capa oculta de tama√±o h: se mezclan las n-1 representaciones del paso anterior y se aplica tanh. \n",
    "        (h = tanh(Hx + d))\n",
    "        Nosotros vamos a usar ReLu en vez de tanh.\n",
    "    (4) Capa de salida de tama√±o m: se aplica softmax a la salida de la capa anterior.\n",
    "        (y = softmax(Uh + b))\n",
    "        Nosotros no vamos a aplicar softmax aqu√≠, sino afuerita.\n",
    "\n",
    "    Args:\n",
    "        args (Any): Diccionario de variables.\n",
    "    \"\"\"\n",
    "    super(NeuralLanguageModel, self).__init__()\n",
    "\n",
    "    self.window_size = args.N - 1  # n-1 palabras de contexto\n",
    "    self.embedding_size = args.m  # tama√±o de las representaciones\n",
    "\n",
    "    if embeddings is None:\n",
    "      self.emb = nn.Embedding(args.vocab_size, args.m)\n",
    "    else:\n",
    "      embeddings = torch.from_numpy(embeddings).float()\n",
    "      self.emb = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "    # primera capa oculta\n",
    "    self.fc1 = nn.Linear(args.m * (args.N - 1), args.d_h)\n",
    "    self.drop1 = nn.Dropout(p=args.dropout)\n",
    "    # producto por la matriz U.\n",
    "    # softmax aplica fuera de la red\n",
    "    self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.emb(x)\n",
    "    # cambia el tama√±o para que se considere como una sola capa\n",
    "    x = x.view(-1, self.window_size * self.embedding_size)\n",
    "    # relu(Hx + d)\n",
    "    h = F.relu(self.fc1(x))  # relu(z) = max{0, z}\n",
    "    h = self.drop1(h)\n",
    "\n",
    "    # devuelve solamente (Uh + b)\n",
    "    return self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  Salida de la red neuronal (las neuronas de la √∫ltima capa oculta).\n",
    "  Uh + b\n",
    "  Se les aplica la softmax\n",
    "  softmax(Uh + b)\n",
    "  Y luego se devuelve el √≠ndice de la neurona de mayor valor.\n",
    "\n",
    "  Args:\n",
    "      raw_logits (torch.Tensor | float): La salida de la red (Uh + b)\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor | int: √çndice de la neurona con mayor valor despu√©s de softmax.\n",
    "  \"\"\"\n",
    "  probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "  y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data: torch.Tensor,\n",
    "               model,\n",
    "               gpu: bool = False) -> float | int:\n",
    "  \"\"\"\n",
    "  Eval√∫a el desempe√±o del modelo sobre un conjunto de validaci√≥n.\n",
    "\n",
    "  Args:\n",
    "      data (torch.Tensor): Conjunto de validaci√≥n sobre el que se va a evaluar el modelo.\n",
    "      model (_type_): Modelo que se va a evaluar.\n",
    "      gpu (bool, optional): ¬øUsamos gpu? S√≠/No. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "      float | int: Puntaje de accuracy sobre todo el conjunto de validaci√≥n.\n",
    "  \"\"\"\n",
    "  with torch.no_grad():\n",
    "    preds, tgts = list(), list()\n",
    "\n",
    "    for window_words, labels in data:\n",
    "      if gpu:\n",
    "        window_words = window_words.cuda()\n",
    "\n",
    "      outputs = model(window_words)\n",
    "\n",
    "      y_pred = get_preds(outputs)\n",
    "\n",
    "      tgt = labels.numpy()\n",
    "      tgts.append(tgt)\n",
    "      preds.append(y_pred)\n",
    "\n",
    "  # desempaquetado targets y predicciones\n",
    "  # e : element\n",
    "  # l : list\n",
    "  # lista de todos los target\n",
    "  tgts = [e for l in tgts for e in l]\n",
    "  # lista de todas las predicciones\n",
    "  preds = [e for l in preds for e in l]\n",
    "\n",
    "  return accuracy_score(tgts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state: dict,\n",
    "                    is_best: bool,\n",
    "                    checkpoint_path: str,\n",
    "                    filename: str = \"checkpoint.pt\"):\n",
    "  \"\"\"\n",
    "  Guarda el modelo si se ve una mejora evidente.\n",
    "\n",
    "  Args:\n",
    "      state (dict): Informaci√≥n que queremos guardar sobre el modelo.\n",
    "        DEBE incluir model.state_dict()\n",
    "      is_best (bool): ¬øEsta versi√≥n del modelo es mejor?\n",
    "      checkpoint_path (str): Directorio en el que se va a guardar el modelo.\n",
    "      filename (str, optional): _description_. Defaults to \"checkpoint.pt\".\n",
    "  \"\"\"\n",
    "  filename = os.path.join(checkpoint_path, filename)\n",
    "  torch.save(state, filename)\n",
    "\n",
    "  if is_best:\n",
    "    shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.m = 100  # dimensi√≥n de los embeddings de palabras\n",
    "args.d_h = 200  # dimensi√≥n de la capa oculta\n",
    "args.dropout = 0.1\n",
    "\n",
    "# entrenamiento\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "# scheduler\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# guardado de los modelos\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLanguageModel(args, ngram_data.embeddings_matrix)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "  model.cuda()\n",
    "\n",
    "# Perdida, optimizaci√≥n y scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  \"min\",\n",
    "  patience=args.lr_patience,\n",
    "  verbose=True,\n",
    "  factor=args.lr_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a410110122184c1bb971ce027a589499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.15171025740167918\n",
      "Epoch [1/100], Loss 5.6533 - Val accuracy 0.1989 - Epoch time : 4.217372179031372\n",
      "Train acc: 0.166273525689953\n",
      "Epoch [2/100], Loss 5.1789 - Val accuracy 0.1797 - Epoch time : 4.123731374740601\n",
      "Train acc: 0.17133330655204274\n",
      "Epoch [3/100], Loss 4.9737 - Val accuracy 0.2265 - Epoch time : 4.187483787536621\n",
      "Train acc: 0.17695612220302895\n",
      "Epoch [4/100], Loss 4.8127 - Val accuracy 0.2183 - Epoch time : 4.01511287689209\n",
      "Train acc: 0.18040086570521832\n",
      "Epoch [5/100], Loss 4.6875 - Val accuracy 0.2035 - Epoch time : 4.087488651275635\n",
      "Train acc: 0.18227921755111878\n",
      "Epoch [6/100], Loss 4.5652 - Val accuracy 0.1178 - Epoch time : 4.075319051742554\n",
      "Train acc: 0.18446701884063793\n",
      "Epoch [7/100], Loss 4.4631 - Val accuracy 0.1364 - Epoch time : 4.034951686859131\n",
      "Train acc: 0.18700695225565417\n",
      "Epoch [8/100], Loss 4.3649 - Val accuracy 0.1566 - Epoch time : 3.877821683883667\n",
      "Train acc: 0.18931213092033905\n",
      "Epoch [9/100], Loss 4.2828 - Val accuracy 0.2079 - Epoch time : 3.944413661956787\n",
      "Train acc: 0.19422660235809264\n",
      "Epoch [10/100], Loss 4.2071 - Val accuracy 0.1660 - Epoch time : 4.140096187591553\n",
      "Train acc: 0.19795160788173383\n",
      "Epoch [11/100], Loss 4.1257 - Val accuracy 0.1590 - Epoch time : 4.265397071838379\n",
      "Train acc: 0.2017277698549793\n",
      "Epoch [12/100], Loss 4.0701 - Val accuracy 0.1656 - Epoch time : 4.3706488609313965\n",
      "Train acc: 0.2053664684851163\n",
      "Epoch [13/100], Loss 4.0070 - Val accuracy 0.1806 - Epoch time : 4.245687961578369\n",
      "Train acc: 0.21061361376692245\n",
      "Epoch [14/100], Loss 3.9510 - Val accuracy 0.1570 - Epoch time : 4.367541313171387\n",
      "Train acc: 0.21324832784316874\n",
      "Epoch [15/100], Loss 3.9056 - Val accuracy 0.1852 - Epoch time : 4.303573131561279\n",
      "Train acc: 0.21842297533443136\n",
      "Epoch [16/100], Loss 3.8639 - Val accuracy 0.1728 - Epoch time : 4.3021934032440186\n",
      "Train acc: 0.22083580223355964\n",
      "Epoch [17/100], Loss 3.8229 - Val accuracy 0.1647 - Epoch time : 4.580136060714722\n",
      "Train acc: 0.26940492809223476\n",
      "Epoch [18/100], Loss 3.3688 - Val accuracy 0.1837 - Epoch time : 4.324361085891724\n",
      "Train acc: 0.2795517941790865\n",
      "Epoch [19/100], Loss 3.2866 - Val accuracy 0.1953 - Epoch time : 4.362340927124023\n",
      "Train acc: 0.28277339513116134\n",
      "Epoch [20/100], Loss 3.2564 - Val accuracy 0.1865 - Epoch time : 4.17231822013855\n",
      "Train acc: 0.2874524841320853\n",
      "Epoch [21/100], Loss 3.2252 - Val accuracy 0.1899 - Epoch time : 4.3085386753082275\n",
      "Train acc: 0.28729022717229746\n",
      "Epoch [22/100], Loss 3.2103 - Val accuracy 0.1762 - Epoch time : 4.425859451293945\n",
      "No improvement. Breaking out of loop.\n",
      "--- 96.95700001716614 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in tqdm(range(args.num_epochs),\n",
    "                  desc=\"Epochs\"):\n",
    "  epoch_start_time = time.time()\n",
    "  loss_epoch = []\n",
    "  training_metric = []\n",
    "  model.train()\n",
    "\n",
    "  for window_words, labels in train_loader:\n",
    "\n",
    "    # Si hay GPU\n",
    "    if args.use_gpu:\n",
    "      window_words = window_words.cuda()\n",
    "      labels = labels.cuda()\n",
    "\n",
    "    # Forward\n",
    "    outputs = model(window_words)\n",
    "    loss = criterion(outputs,\n",
    "                     labels)\n",
    "    loss_epoch.append(loss.item())\n",
    "\n",
    "    # Obtener m√©tricas de entrenamiento\n",
    "    y_pred = get_preds(outputs)\n",
    "    tgt = labels.cpu().numpy()\n",
    "    training_metric.append(accuracy_score(tgt,\n",
    "                                          y_pred))\n",
    "\n",
    "    # Backprop y optimizamos\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Guardamos la m√©trica por √©poca\n",
    "  mean_epoch_metric = np.mean(training_metric)\n",
    "  train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Validaci√≥n para esta √©poca\n",
    "  model.eval()\n",
    "  tuning_metric = model_eval(val_loader,\n",
    "                             model,\n",
    "                             gpu=args.use_gpu)\n",
    "  metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Scheduler\n",
    "  scheduler.step(tuning_metric)\n",
    "\n",
    "  # Revisa si la m√©trica mejor√≥\n",
    "  is_improvement = tuning_metric > best_metric\n",
    "  if is_improvement:\n",
    "    best_metric = tuning_metric\n",
    "    n_no_improve = 0\n",
    "  else:\n",
    "    n_no_improve += 1\n",
    "\n",
    "  # Si la m√©trica mejora, guarda el mejor modelo\n",
    "  save_checkpoint(\n",
    "    {\n",
    "      \"epoch\": epoch + 1,\n",
    "      \"state_dict\": model.state_dict(),\n",
    "      \"optimizer\": optimizer.state_dict(),\n",
    "      \"scheduler\": scheduler.state_dict(),\n",
    "      \"best_metric\": best_metric\n",
    "    },\n",
    "    is_improvement,\n",
    "    args.savedir\n",
    "  )\n",
    "\n",
    "  # Parada temprana por paciencie\n",
    "  if n_no_improve >= args.patience:\n",
    "    print(\"No improvement. Breaking out of loop.\")\n",
    "    break\n",
    "\n",
    "  print(f\"Train acc: {mean_epoch_metric}\")\n",
    "  print(f\"Epoch [{epoch + 1}/{args.num_epochs}], Loss {np.mean(loss_epoch):.4f} - Val accuracy {tuning_metric:.4f} - Epoch time : {time.time() - epoch_start_time}\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 palabras mas similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    '''Devuelve la lista de las n palabras mas cercanas a word'''\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]]) # obtener id de las palabras\n",
    "    word_embed = embeddings(word_id) # obtener el embedding de la palabra\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach() # calcular distancias a todas las palabras\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # ordenar por distancia\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Closest words to 'perro'\n",
      "------------------------------\n",
      "gato 9.527481\n",
      "perrito 12.485804\n",
      "enano 17.225166\n",
      "gordo 17.396631\n",
      "ni√±o 17.556175\n",
      "macho 17.792421\n",
      "loro 18.209782\n",
      "chamaco 18.251602\n",
      "burro 18.331388\n",
      "caballo 18.47935\n",
      "------------------------------\n",
      "Closest words to 'hijo'\n",
      "------------------------------\n",
      "padre 16.005705\n",
      "hija 16.08349\n",
      "marido 16.089521\n",
      "esposo 16.684036\n",
      "t√≠o 17.627182\n",
      "pap√° 18.268936\n",
      "hermano 18.30198\n",
      "abuelito 18.34545\n",
      "hermanito 19.020254\n",
      "esposa 19.2075\n",
      "------------------------------\n",
      "Closest words to 'casa'\n",
      "------------------------------\n",
      "oficina 22.108376\n",
      "depa 22.856125\n",
      "cuarto 23.119127\n",
      "cama 23.606852\n",
      "uni 24.154474\n",
      "chamba 25.643435\n",
      "mochila 25.767996\n",
      "heladera 26.069523\n",
      "moto 26.314129\n",
      "camioneta 26.537226\n"
     ]
    }
   ],
   "source": [
    "best_model = NeuralLanguageModel(args)\n",
    "state_dict = torch.load(\"model/model_best.pt\", weights_only=False)\n",
    "best_model.load_state_dict(state_dict[\"state_dict\"])\n",
    "best_model.train(False)\n",
    "\n",
    "words = [\"perro\", \"hijo\", \"casa\"]\n",
    "for w in words:\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Closest words to '{w}'\")\n",
    "    print(\"-\"*30)\n",
    "    print_closest_words(best_model.emb, ngram_data, w, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texto a partir de secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer, ngram_data):\n",
    "    '''Devuelve el texto tokenizado y los ids de las palabras'''\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer.tokenize(text)]\n",
    "    token_ids = [ngram_data.w2id[w.lower()] for w in all_tokens]\n",
    "    return all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits, temperature=1.0):\n",
    "    '''\n",
    "    Dados los logits y la temperatura \n",
    "    (un parametro de diversidad que indica cuan determinista sera el modelo), \n",
    "    devuelve la siguiente palabra\n",
    "    '''\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, token_ids_tensor):\n",
    "    y_raw_pred = model(token_ids_tensor).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, initial_text, tokenizer, ngram_data, max_length=100, parser=parse_text, joiner=\" \"):\n",
    "    all_tokens, window_word_ids = parser(initial_text, tokenizer, ngram_data)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i in range(max_length):\n",
    "        window_tensor = torch.tensor(window_word_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        y_pred = predict_next_token(model, window_tensor)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "\n",
    "    return joiner.join(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo genera oraciones con partes coherentes, pero esto ayudara mas para calcular la probabilidad de ocurrencia de oraciones dadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> si <unk> me vuelvo loca <unk> pero en estos <unk> <unk> <unk> y mi madre tambi√©n yo <unk> es <unk> <unk> <unk> qu√© derechos de ser <unk> <unk> porque me siento si me <unk> a huevo <unk> <unk> que esta <unk> <unk> <unk> <unk> üôÑ <unk> <unk> <unk> no hay otra tipa por mi que quer√≠an <unk> la puta madre <unk> padre <unk> <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk, ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> lo importante es ‚Äù ... pero mi vida en mujeres <unk> pero hasta hasta la verga o para que digan ma√±ana <unk> pero <unk> :( </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk, ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> estoy cagada <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> estoy'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk, ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> saludos a vergazos tiene m√°s cualidades del <unk> que otras cansado saben <unk> <unk> <unk> <unk> üé∂ üñïüèª la noche triste a un lado m√≠o va sentado un mejor del <unk> a menos las <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> saludos a'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk, ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, test, ngram_model):\n",
    "    # Generar n gram windows from input text and the respective label y\n",
    "    X, y = ngram_model.transform(test)\n",
    "    # discard first two n-gram windows since they may contain <s> tokens (not necessary)\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "    return np.sum(np.log(probs[i][w]+1e-10) for i, w in enumerate(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa como cambian los valores para oraciones menos probables de observar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood       | Frase\n",
      "------------------------------------------------------------\n",
      "-1091.6588           | Estamos en la clase de procesamiento de lenguaje\n",
      "-1271.3441           | la natural Estamos clase en de de lenguaje procesamiento\n",
      "-709.9353            | eres el mejor de los politicos\n",
      "-828.6460            | yo desde que lo recuerdo ha estado en el\n",
      "-927.9969            | siempre he creo en que estado lugar este\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5636/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Log Likelihood':<20} | Frase\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "frases = [\n",
    "    \"Estamos en la clase de procesamiento de lenguaje\",\n",
    "    \"la natural Estamos clase en de de lenguaje procesamiento\",\n",
    "    \"eres el mejor de los politicos\",\n",
    "    \"yo desde que lo recuerdo ha estado en el\",\n",
    "    \"siempre he creo en que estado lugar este\"\n",
    "]\n",
    "\n",
    "for frase in frases:\n",
    "    log_prob = log_likelihood(best_model, frase, ngram_data)\n",
    "    print(f\"{log_prob:<20.4f} | {frase}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutacion de estructuras sintacticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Mejores secuencias\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5636/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-535.8627253770828 si no eso dejas decir de\n",
      "-535.8627253770828 si no eso dejas de decir\n",
      "-535.8627253770828 si no eso decir dejas de\n",
      "-535.8627253770828 si no eso decir de dejas\n",
      "-535.8627253770828 si no eso de dejas decir\n",
      "------------------------------\n",
      "Peores secuencias\n",
      "------------------------------\n",
      "-544.7678281068802 de decir dejas si eso no\n",
      "-544.7678281068802 de decir dejas no si eso\n",
      "-544.7678281068802 de decir dejas no eso si\n",
      "-544.7678281068802 de decir dejas eso si no\n",
      "-544.7678281068802 de decir dejas eso no si\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"si no dejas de decir eso\".split(\" \")\n",
    "perms = [' '.join(p) for p in permutations(word_list)]\n",
    "#print(perms)\n",
    "\n",
    "print('-'*30)\n",
    "print(\"Mejores secuencias\")\n",
    "print('-'*30)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "\n",
    "print('-'*30)\n",
    "print(\"Peores secuencias\")\n",
    "print('-'*30)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancho/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90507a5282004b1a8493929ac4de78fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.17339839513116134\n",
      "Epoch [1/100], Loss 5.5124 - Val accuracy 0.2233 - Epoch time : 4.48490047454834\n",
      "Train acc: 0.1832508762503515\n",
      "Epoch [2/100], Loss 5.0737 - Val accuracy 0.1582 - Epoch time : 4.1563568115234375\n",
      "Train acc: 0.19087852257662796\n",
      "Epoch [3/100], Loss 4.8655 - Val accuracy 0.2261 - Epoch time : 4.294549465179443\n",
      "Train acc: 0.19665700447917087\n",
      "Epoch [4/100], Loss 4.6912 - Val accuracy 0.2313 - Epoch time : 4.221453905105591\n",
      "Train acc: 0.19743470805045596\n",
      "Epoch [5/100], Loss 4.5500 - Val accuracy 0.1127 - Epoch time : 3.9980862140655518\n",
      "Train acc: 0.2003164167637488\n",
      "Epoch [6/100], Loss 4.4173 - Val accuracy 0.2220 - Epoch time : 4.287186145782471\n",
      "Train acc: 0.20291409733660062\n",
      "Epoch [7/100], Loss 4.2956 - Val accuracy 0.2052 - Epoch time : 4.186666488647461\n",
      "Train acc: 0.20578419384967658\n",
      "Epoch [8/100], Loss 4.1773 - Val accuracy 0.2038 - Epoch time : 4.195430755615234\n",
      "Train acc: 0.20922046358414012\n",
      "Epoch [9/100], Loss 4.0664 - Val accuracy 0.1403 - Epoch time : 4.207903623580933\n",
      "Train acc: 0.21178832904832684\n",
      "Epoch [10/100], Loss 3.9666 - Val accuracy 0.1352 - Epoch time : 4.253751277923584\n",
      "Train acc: 0.2174597904029245\n",
      "Epoch [11/100], Loss 3.8705 - Val accuracy 0.1499 - Epoch time : 4.259315013885498\n",
      "Train acc: 0.22214201783633952\n",
      "Epoch [12/100], Loss 3.7829 - Val accuracy 0.2178 - Epoch time : 4.174250602722168\n",
      "Train acc: 0.23154036777407302\n",
      "Epoch [13/100], Loss 3.6929 - Val accuracy 0.1685 - Epoch time : 4.240253448486328\n",
      "Train acc: 0.2406521788253726\n",
      "Epoch [14/100], Loss 3.6070 - Val accuracy 0.2192 - Epoch time : 4.455595254898071\n",
      "Train acc: 0.24697988641385127\n",
      "Epoch [15/100], Loss 3.5403 - Val accuracy 0.1967 - Epoch time : 4.41868782043457\n",
      "Train acc: 0.25909141124010765\n",
      "Epoch [16/100], Loss 3.4724 - Val accuracy 0.1934 - Epoch time : 4.149477005004883\n",
      "Train acc: 0.30999678624512916\n",
      "Epoch [17/100], Loss 3.1216 - Val accuracy 0.2077 - Epoch time : 4.2818732261657715\n",
      "Train acc: 0.3173429151769574\n",
      "Epoch [18/100], Loss 3.0617 - Val accuracy 0.1730 - Epoch time : 4.419028282165527\n",
      "Train acc: 0.32053940866910374\n",
      "Epoch [19/100], Loss 3.0311 - Val accuracy 0.1441 - Epoch time : 4.4098875522613525\n",
      "Train acc: 0.322390456152332\n",
      "Epoch [20/100], Loss 3.0045 - Val accuracy 0.1909 - Epoch time : 4.4521324634552\n",
      "Train acc: 0.33014332593500184\n",
      "Epoch [21/100], Loss 2.9736 - Val accuracy 0.1790 - Epoch time : 4.363888502120972\n",
      "Train acc: 0.3313613515847828\n",
      "Epoch [22/100], Loss 2.9587 - Val accuracy 0.1902 - Epoch time : 4.2406134605407715\n",
      "Train acc: 0.33272217591290726\n",
      "Epoch [23/100], Loss 2.9343 - Val accuracy 0.1743 - Epoch time : 4.270567417144775\n",
      "No improvement. Breaking out of loop.\n",
      "--- 102.71249866485596 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralLanguageModel(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_pretrained_model = NeuralLanguageModel(args)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "  non_pretrained_model.cuda()\n",
    "\n",
    "# Perdida, optimizaci√≥n y scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(non_pretrained_model.parameters(),\n",
    "                            lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  \"min\",\n",
    "  patience=args.lr_patience,\n",
    "  verbose=True,\n",
    "  factor=args.lr_factor\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in tqdm(range(args.num_epochs),\n",
    "                  desc=\"Epochs\"):\n",
    "  epoch_start_time = time.time()\n",
    "  loss_epoch = []\n",
    "  training_metric = []\n",
    "  non_pretrained_model.train()\n",
    "\n",
    "  for window_words, labels in train_loader:\n",
    "\n",
    "    # Si hay GPU\n",
    "    if args.use_gpu:\n",
    "      window_words = window_words.cuda()\n",
    "      labels = labels.cuda()\n",
    "\n",
    "    # Forward\n",
    "    outputs = non_pretrained_model(window_words)\n",
    "    loss = criterion(outputs,\n",
    "                     labels)\n",
    "    loss_epoch.append(loss.item())\n",
    "\n",
    "    # Obtener m√©tricas de entrenamiento\n",
    "    y_pred = get_preds(outputs)\n",
    "    tgt = labels.cpu().numpy()\n",
    "    training_metric.append(accuracy_score(tgt,\n",
    "                                          y_pred))\n",
    "\n",
    "    # Backprop y optimizamos\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Guardamos la m√©trica por √©poca\n",
    "  mean_epoch_metric = np.mean(training_metric)\n",
    "  train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Validaci√≥n para esta √©poca\n",
    "  non_pretrained_model.eval()\n",
    "  tuning_metric = model_eval(val_loader,\n",
    "                             non_pretrained_model,\n",
    "                             gpu=args.use_gpu)\n",
    "  metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Scheduler\n",
    "  scheduler.step(tuning_metric)\n",
    "\n",
    "  # Revisa si la m√©trica mejor√≥\n",
    "  is_improvement = tuning_metric > best_metric\n",
    "  if is_improvement:\n",
    "    best_metric = tuning_metric\n",
    "    n_no_improve = 0\n",
    "  else:\n",
    "    n_no_improve += 1\n",
    "\n",
    "  # Si la m√©trica mejora, guarda el mejor modelo\n",
    "  save_checkpoint(\n",
    "    {\n",
    "      \"epoch\": epoch + 1,\n",
    "      \"state_dict\": non_pretrained_model.state_dict(),\n",
    "      \"optimizer\": optimizer.state_dict(),\n",
    "      \"scheduler\": scheduler.state_dict(),\n",
    "      \"best_metric\": best_metric\n",
    "    },\n",
    "    is_improvement,\n",
    "    args.savedir\n",
    "  )\n",
    "\n",
    "  # Parada temprana por paciencie\n",
    "  if n_no_improve >= args.patience:\n",
    "    print(\"No improvement. Breaking out of loop.\")\n",
    "    break\n",
    "\n",
    "  print(f\"Train acc: {mean_epoch_metric}\")\n",
    "  print(f\"Epoch [{epoch + 1}/{args.num_epochs}], Loss {np.mean(loss_epoch):.4f} - Val accuracy {tuning_metric:.4f} - Epoch time : {time.time() - epoch_start_time}\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds\")\n",
    "\n",
    "best_non_pretrained_model = NeuralLanguageModel(args)\n",
    "state_dict = torch.load(\"model/model_best.pt\", weights_only=False)\n",
    "best_non_pretrained_model.load_state_dict(state_dict[\"state_dict\"])\n",
    "best_non_pretrained_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, corpus, ngram_model):\n",
    "    total_log_likelihood = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for frase in corpus:\n",
    "        X, y = ngram_model.transform(frase)\n",
    "        \n",
    "        X, y = X[2:], y[2:]\n",
    "\n",
    "        if len(X) == 0:\n",
    "            continue\n",
    "\n",
    "        X_tensor = torch.LongTensor(X)\n",
    "        logits = model(X_tensor).detach()\n",
    "        probs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "        frase_log_likelihood = np.sum(np.log([probs[i][w] + 1e-10 for i, w in enumerate(y)]))\n",
    "        total_log_likelihood += frase_log_likelihood\n",
    "\n",
    "        total_tokens += len(y)\n",
    "\n",
    "    return np.exp(-total_log_likelihood / total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad (no preentrenado): 2208.4185075224887\n",
      "Perplejidad (preentrenado): 1755.5009952801508\n",
      "Perplejidad (probabilistico): 385.60\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplejidad (no preentrenado): {perplexity(best_non_pretrained_model, X_val, ngram_data)}\")\n",
    "print(f\"Perplejidad (preentrenado): {perplexity(best_model, X_val, ngram_data)}\")\n",
    "print(\"Perplejidad (probabilistico): 385.60\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pese a los resultados obtenidos por los NLMs entrenados aqui al encontrar palabras similares (muchas pertenecientes a algun grupo sintactico, p. ej., varias de las mas parecidas a \"casa\" son lugares) y al generar texto (encontrando sub-secuencias coherentes con bastante regularidad), la perplejidad calcujada es extremadamente alta, seguramente debido a un error en su calculo.\n",
    "\n",
    "Por su parte, la accuracy alcanzada por el NLM sin embeddings preentrenados es considerablemente mas alta que la alcanzada por el NLM con embeddings preentrenados, probablemente debido a que los embeddings aleatorios generados se encuentran mas cerca de un mejor optimo que en el primer caso, de manera que obtiene mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLM a nivel caracter\n",
    "Se define un tokenizer que separe los strings por caracter en lugar de por palabras. Dado que el vocabulario es mucho menor (las letras del abecedario y algunos simbolos), se adaptan los parametros para entrenar el modelo, pudiendo usarse las clases definidas anteriormente con solamente estas modificaciones (dado que el modelo a nivel caracter, FastText, es muy similar a los NLMs de Bengio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenizer(text):\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.N = 6\n",
    "vocab_max_size = 100\n",
    "\n",
    "char_ngram_data = NgramData(args.N, vocab_max_size, char_tokenizer, embs_dict)\n",
    "char_ngram_data.fit(X_train)\n",
    "\n",
    "X_ngram_train, y_ngram_train = char_ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = char_ngram_data.transform(X_val)\n",
    "\n",
    "# Batch size\n",
    "args.batch_size = 64\n",
    "\n",
    "# Num of workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle=True)\n",
    "\n",
    "# Val\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)\n",
    "\n",
    "args.vocab_size = char_ngram_data.get_vocab_size()\n",
    "\n",
    "# Embeddings\n",
    "args.m = 32\n",
    "args.d_h = 64\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Entrenamiento\n",
    "args.lr = 0.01\n",
    "args.num_epochs = 100\n",
    "args.patience = 10\n",
    "\n",
    "# Scheduler\n",
    "args.lr_patience = 5\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# guardado de los modelos\n",
    "args.savedir = 'char_model'\n",
    "os.makedirs(args.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancho/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c7e0f3594e41909d23c45f1459f0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.30969145610700893\n",
      "Epoch [1/100], Loss 2.4545 - Val accuracy 0.3640 - Epoch time : 32.57049298286438\n",
      "Train acc: 0.36758173141830497\n",
      "Epoch [2/100], Loss 2.1349 - Val accuracy 0.3922 - Epoch time : 34.05653119087219\n",
      "Train acc: 0.38612480657590015\n",
      "Epoch [3/100], Loss 2.0611 - Val accuracy 0.4062 - Epoch time : 32.86782670021057\n",
      "Train acc: 0.39696045061324525\n",
      "Epoch [4/100], Loss 2.0178 - Val accuracy 0.4136 - Epoch time : 31.996078491210938\n",
      "Train acc: 0.40467761784590456\n",
      "Epoch [5/100], Loss 1.9900 - Val accuracy 0.4213 - Epoch time : 33.48063540458679\n",
      "Train acc: 0.4100258266608509\n",
      "Epoch [6/100], Loss 1.9703 - Val accuracy 0.4253 - Epoch time : 32.648488998413086\n",
      "Train acc: 0.4144382328047273\n",
      "Epoch [7/100], Loss 1.9541 - Val accuracy 0.4285 - Epoch time : 33.18914604187012\n",
      "Train acc: 0.417731148290079\n",
      "Epoch [8/100], Loss 1.9404 - Val accuracy 0.4333 - Epoch time : 34.15164232254028\n",
      "Train acc: 0.4197335420755834\n",
      "Epoch [9/100], Loss 1.9335 - Val accuracy 0.4341 - Epoch time : 22.472358226776123\n",
      "Train acc: 0.4217086747463418\n",
      "Epoch [10/100], Loss 1.9282 - Val accuracy 0.4367 - Epoch time : 21.70553493499756\n",
      "Train acc: 0.42313095850079446\n",
      "Epoch [11/100], Loss 1.9231 - Val accuracy 0.4367 - Epoch time : 21.021594762802124\n",
      "Train acc: 0.42399633662543745\n",
      "Epoch [12/100], Loss 1.9189 - Val accuracy 0.4397 - Epoch time : 20.77315664291382\n",
      "Train acc: 0.4256354111495363\n",
      "Epoch [13/100], Loss 1.9142 - Val accuracy 0.4408 - Epoch time : 20.956969022750854\n",
      "Train acc: 0.42712130417172944\n",
      "Epoch [14/100], Loss 1.9084 - Val accuracy 0.4415 - Epoch time : 20.92181921005249\n",
      "Train acc: 0.42760080770788544\n",
      "Epoch [15/100], Loss 1.9062 - Val accuracy 0.4428 - Epoch time : 21.139044046401978\n",
      "Train acc: 0.42828996219792087\n",
      "Epoch [16/100], Loss 1.9050 - Val accuracy 0.4412 - Epoch time : 20.729050159454346\n",
      "Train acc: 0.42871640392144644\n",
      "Epoch [17/100], Loss 1.9024 - Val accuracy 0.4425 - Epoch time : 21.022608518600464\n",
      "Train acc: 0.4287363629518854\n",
      "Epoch [18/100], Loss 1.9022 - Val accuracy 0.4432 - Epoch time : 20.890361785888672\n",
      "Train acc: 0.429280611635563\n",
      "Epoch [19/100], Loss 1.8998 - Val accuracy 0.4425 - Epoch time : 20.623726844787598\n",
      "Train acc: 0.4301141851263358\n",
      "Epoch [20/100], Loss 1.8971 - Val accuracy 0.4431 - Epoch time : 20.651175260543823\n",
      "Train acc: 0.4304474847078128\n",
      "Epoch [21/100], Loss 1.8971 - Val accuracy 0.4435 - Epoch time : 21.06256628036499\n",
      "Train acc: 0.4307739690106033\n",
      "Epoch [22/100], Loss 1.8952 - Val accuracy 0.4433 - Epoch time : 20.726848602294922\n",
      "Train acc: 0.4310498255288656\n",
      "Epoch [23/100], Loss 1.8948 - Val accuracy 0.4433 - Epoch time : 21.301799774169922\n",
      "Train acc: 0.43101136788484906\n",
      "Epoch [24/100], Loss 1.8946 - Val accuracy 0.4445 - Epoch time : 20.919369220733643\n",
      "Train acc: 0.43103976487937606\n",
      "Epoch [25/100], Loss 1.8932 - Val accuracy 0.4437 - Epoch time : 20.691728591918945\n",
      "Train acc: 0.43175017654817166\n",
      "Epoch [26/100], Loss 1.8919 - Val accuracy 0.4449 - Epoch time : 20.84724497795105\n",
      "Train acc: 0.4313092929245724\n",
      "Epoch [27/100], Loss 1.8913 - Val accuracy 0.4446 - Epoch time : 20.669091939926147\n",
      "Train acc: 0.43156162050451236\n",
      "Epoch [28/100], Loss 1.8921 - Val accuracy 0.4445 - Epoch time : 20.884356260299683\n",
      "Train acc: 0.4317453084919671\n",
      "Epoch [29/100], Loss 1.8910 - Val accuracy 0.4444 - Epoch time : 21.355761528015137\n",
      "Train acc: 0.4322340613349119\n",
      "Epoch [30/100], Loss 1.8901 - Val accuracy 0.4445 - Epoch time : 21.151063442230225\n",
      "Train acc: 0.4322840400452794\n",
      "Epoch [31/100], Loss 1.8899 - Val accuracy 0.4448 - Epoch time : 20.850374460220337\n",
      "Train acc: 0.4321539006760757\n",
      "Epoch [32/100], Loss 1.8903 - Val accuracy 0.4444 - Epoch time : 20.975831747055054\n",
      "Train acc: 0.43238253704915314\n",
      "Epoch [33/100], Loss 1.8893 - Val accuracy 0.4443 - Epoch time : 20.893953323364258\n",
      "Train acc: 0.4317579654380991\n",
      "Epoch [34/100], Loss 1.8894 - Val accuracy 0.4444 - Epoch time : 22.235744953155518\n",
      "Train acc: 0.4320524828384792\n",
      "Epoch [35/100], Loss 1.8905 - Val accuracy 0.4445 - Epoch time : 21.20368456840515\n",
      "No improvement. Breaking out of loop.\n",
      "--- 855.6116361618042 seconds\n"
     ]
    }
   ],
   "source": [
    "char_model = NeuralLanguageModel(args)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "  char_model.cuda()\n",
    "\n",
    "# Perdida, optimizaci√≥n y scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(char_model.parameters(),\n",
    "                            lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  \"min\",\n",
    "  patience=args.lr_patience,\n",
    "  verbose=True,\n",
    "  factor=args.lr_factor\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in tqdm(range(args.num_epochs),\n",
    "                  desc=\"Epochs\"):\n",
    "  epoch_start_time = time.time()\n",
    "  loss_epoch = []\n",
    "  training_metric = []\n",
    "  char_model.train()\n",
    "\n",
    "  for window_words, labels in train_loader:\n",
    "\n",
    "    # Si hay GPU\n",
    "    if args.use_gpu:\n",
    "      window_words = window_words.cuda()\n",
    "      labels = labels.cuda()\n",
    "\n",
    "    # Forward\n",
    "    outputs = char_model(window_words)\n",
    "    loss = criterion(outputs,\n",
    "                     labels)\n",
    "    loss_epoch.append(loss.item())\n",
    "\n",
    "    # Obtener m√©tricas de entrenamiento\n",
    "    y_pred = get_preds(outputs)\n",
    "    tgt = labels.cpu().numpy()\n",
    "    training_metric.append(accuracy_score(tgt,\n",
    "                                          y_pred))\n",
    "\n",
    "    # Backprop y optimizamos\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Guardamos la m√©trica por √©poca\n",
    "  mean_epoch_metric = np.mean(training_metric)\n",
    "  train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Validaci√≥n para esta √©poca\n",
    "  char_model.eval()\n",
    "  tuning_metric = model_eval(val_loader,\n",
    "                             char_model,\n",
    "                             gpu=args.use_gpu)\n",
    "  metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Scheduler\n",
    "  scheduler.step(tuning_metric)\n",
    "\n",
    "  # Revisa si la m√©trica mejor√≥\n",
    "  is_improvement = tuning_metric > best_metric\n",
    "  if is_improvement:\n",
    "    best_metric = tuning_metric\n",
    "    n_no_improve = 0\n",
    "  else:\n",
    "    n_no_improve += 1\n",
    "\n",
    "  # Si la m√©trica mejora, guarda el mejor modelo\n",
    "  save_checkpoint(\n",
    "    {\n",
    "      \"epoch\": epoch + 1,\n",
    "      \"state_dict\": char_model.state_dict(),\n",
    "      \"optimizer\": optimizer.state_dict(),\n",
    "      \"scheduler\": scheduler.state_dict(),\n",
    "      \"best_metric\": best_metric\n",
    "    },\n",
    "    is_improvement,\n",
    "    args.savedir\n",
    "  )\n",
    "\n",
    "  # Parada temprana por paciencie\n",
    "  if n_no_improve >= args.patience:\n",
    "    print(\"No improvement. Breaking out of loop.\")\n",
    "    break\n",
    "\n",
    "  print(f\"Train acc: {mean_epoch_metric}\")\n",
    "  print(f\"Epoch [{epoch + 1}/{args.num_epochs}], Loss {np.mean(loss_epoch):.4f} - Val accuracy {tuning_metric:.4f} - Epoch time : {time.time() - epoch_start_time}\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLanguageModel(\n",
       "  (emb): Embedding(100, 32)\n",
       "  (fc1): Linear(in_features=160, out_features=64, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=64, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_char_model = NeuralLanguageModel(args)\n",
    "state_dict = torch.load(\"char_model/model_best.pt\", weights_only=False)\n",
    "best_char_model.load_state_dict(state_dict[\"state_dict\"])\n",
    "best_char_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_char(text, tokenizer, ngram):\n",
    "    '''Devuelve el texto tokenizado y los ids de caracteres'''\n",
    "    all_tokens = [w.lower() if w.lower() in ngram.w2id else '<unk>' for w in tokenizer(text)]\n",
    "    token_ids = [ngram.w2id[w] for w in all_tokens]\n",
    "    window_size = ngram.N - 1\n",
    "    token_ids = token_ids[-window_size:]\n",
    "    return all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola hey <unk>mona mandemos mi en ula sechetelos persos los mam√°n ywvada derente espero a mejo que mamisiate arigre<unk></s>\n",
      "ellas putas g√∫ de madran pinan mander madida feas y la verga  chas y lo quino ma el cocoragadar la vay pa√±a verga m√°s me que la vizez <unk><unk> asique esea de amos que bus as<unk> hoberten gata <unk>va fr de no le fanüòÇs<unk></s>\n",
      "vayansap pricieliriaca <unk>m√©nima ceapelada uecon<unk></s>\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(char_model, \"hola h\", char_tokenizer, char_ngram_data, max_length=300, parser=parse_text_char, joiner=\"\"))\n",
    "print(generate_text(char_model, \"ellas \", char_tokenizer, char_ngram_data, max_length=300, parser=parse_text_char, joiner=\"\"))\n",
    "print(generate_text(char_model, \"vayans\", char_tokenizer, char_ngram_data, max_length=300, parser=parse_text_char, joiner=\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood       | Frase\n",
      "------------------------------------------------------------\n",
      "-797.1431            | Estamos en la clase de procesamiento de lenguaje\n",
      "-937.4966            | la natural Estamos clase en de de lenguaje procesamiento\n",
      "-494.0180            | eres el mejor de los politicos\n",
      "-683.9063            | yo desde que lo recuerdo ha estado en el\n",
      "-680.1800            | siempre he creo en que estado lugar este\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7188/3418010109.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]+1e-10) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Log Likelihood':<20} | Frase\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "frases = [\n",
    "    \"Estamos en la clase de procesamiento de lenguaje\",\n",
    "    \"la natural Estamos clase en de de lenguaje procesamiento\",\n",
    "    \"eres el mejor de los politicos\",\n",
    "    \"yo desde que lo recuerdo ha estado en el\",\n",
    "    \"siempre he creo en que estado lugar este\"\n",
    "]\n",
    "\n",
    "for frase in frases:\n",
    "    log_prob = log_likelihood(best_char_model, frase, char_ngram_data)\n",
    "    print(f\"{log_prob:<20.4f} | {frase}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura morfologica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Mejores secuencias\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7188/3418010109.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]+1e-10) for i, w in enumerate(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-111.97369872578572 di sajes\n",
      "-111.97369872578572 di sajes\n",
      "-111.97369872578572 di asjes\n",
      "-111.97369872578572 di asjes\n",
      "-111.97369872578572 d isajes\n",
      "------------------------------\n",
      "Peores secuencias\n",
      "------------------------------\n",
      "-117.58164512966646 sjsd iea\n",
      "-117.58164512966646 sdsj iea\n",
      "-117.58164512966646 sdsj iea\n",
      "-117.58164512966646 sds jiea\n",
      "-117.58164512966646 sds jiea\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "char_list = char_tokenizer(\"si dejas\")\n",
    "perms = [''.join(p) for p in permutations(char_list)]\n",
    "#print(perms)\n",
    "\n",
    "print('-'*30)\n",
    "print(\"Mejores secuencias\")\n",
    "print('-'*30)\n",
    "for p, t in sorted([(log_likelihood(best_char_model, text, char_ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "\n",
    "print('-'*30)\n",
    "print(\"Peores secuencias\")\n",
    "print('-'*30)\n",
    "for p, t in sorted([(log_likelihood(best_char_model, text, char_ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad: 4374.900780770517\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplejidad: {perplexity(best_char_model, X_val, char_ngram_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en la NLM a nivel palabra, en este caso en la generacion de texto se observan secuencias de caracteres que suenan familiares en el espanol, si bien la mayoria no son palabras coherentes.\n",
    "\n",
    "En el caso del calculo del log likelihood, se observan valores bastante mas cercanos a cero que en el caso anterior, probablemente debido a que hay una menor cantidad de secuencias posibles (cada palabra puede combinarse con otros miles de palabras, pero cada caracter solo puede combinarse con algunos cientos de ellos), y por lo tanto, una mayor probabilidad asociada a su ocurrencia.\n",
    "\n",
    "Lo mismo sucede en las estructuras morfologicas, donde las permutaciones clasificadas como mejores poseen secuencias familiares, mientras que las peores son secuencias muy extranas o no vistas en el espanol (por ejemplo, varias consonantes o vocales consecutivas).\n",
    "\n",
    "Finalmente, la perplejidad del modelo es la mas alta que la del resto de los modelos entrenados, probablemente debido a la informalidad del lenguaje presente en el corpus (faltas de ortografia, palabras mal escritas, variabilidad en los emojis usados, etc.), lo cual puede introducir una mayor cantidad de ruido que el presente a nivel palabra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
