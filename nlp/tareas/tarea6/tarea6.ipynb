{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e236b4d",
   "metadata": {},
   "source": [
    "# Juan Carlos Perez Ramirez\n",
    "## Procesamiento de Lenguaje Natural\n",
    "## Tarea 6: Hierarchical Attention\n",
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9025f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Rutas de entrada y salida\n",
    "split = \"val\"\n",
    "dir = \"es_\" + split\n",
    "path = \"../../corpus/2025AuthorProfiling_Train_Val/\"\n",
    "truth = path + dir + '/truth.txt'  # Tu archivo con los identificadores\n",
    "xml_dir = path + dir  # Carpeta donde están los archivos XML\n",
    "docs_file = path + split + '.csv'\n",
    "\n",
    "# Abrimos los archivos de salida\n",
    "with open(docs_file, 'w', encoding='utf-8') as f_textos, \\\n",
    "     open(truth, 'r', encoding='utf-8') as f_in:\n",
    "\n",
    "    for linea in f_in:\n",
    "        partes = linea.strip().split(\":::\")\n",
    "        if len(partes) != 3:\n",
    "            continue  # Saltamos líneas mal formateadas\n",
    "\n",
    "        identificador = partes[0]\n",
    "        nacionalidad = partes[2]\n",
    "        archivo_xml = os.path.join(xml_dir, f\"{identificador}.xml\")\n",
    "\n",
    "        if not os.path.exists(archivo_xml):\n",
    "            print(f\"Archivo {archivo_xml} no encontrado, saltando.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(archivo_xml)\n",
    "            root = tree.getroot()\n",
    "            documentos = root.find('documents')\n",
    "\n",
    "            for doc in documentos.findall('document'):\n",
    "                texto = doc.text.strip()\n",
    "                f_textos.write(texto + \"</s>\" + identificador + \"</s>\" + nacionalidad + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {archivo_xml}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de859ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/juancho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d28afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class author_profiling_dataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.load_data(split)\n",
    "        self.vocab, self.emb_mat = self.load_vocab_embeddings()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Método principal para cargar una observación del dataset.\n",
    "           label: categoría a la que pertenece la observación.\n",
    "           word_ids: lista de índices de las palbras en el vocabulario.\n",
    "        '''\n",
    "        label = self.data.iloc[index]['target']\n",
    "        words, word_ids = self.preprocessed_text(index)\n",
    "        return word_ids, label, words\n",
    "        \n",
    "    def preprocessed_text(self, index):\n",
    "        '''Preprocess text and '''\n",
    "\n",
    "        text = self.data.iloc[index]['text']\n",
    "        words = nltk.word_tokenize(text)\n",
    "        word_ids = [self.vocab[word] if word in self.vocab.keys() else self.emb_mat.shape[0]-1\\\n",
    "                        for word in words]\n",
    "        return words, word_ids\n",
    "\n",
    "    def load_data(self, split):\n",
    "        # Lee el archivo como texto plano (sin separadores por columnas)\n",
    "        with open(f'{split}.csv', 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Separa cada línea usando el separador personalizado ':::'\n",
    "        textos = []\n",
    "        usuarios = []\n",
    "        etiquetas = []\n",
    "        for line in lines:\n",
    "            if \"</s>\" in line:\n",
    "                l = line.strip().split(\"</s>\")\n",
    "                if len(l) == 3:\n",
    "                    texto, usuario, etiqueta = line.strip().split(\"</s>\")\n",
    "                    textos.append(texto)\n",
    "                    usuarios.append(usuario)\n",
    "                    etiquetas.append(etiqueta)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        # Crea el DataFrame final\n",
    "        self.data = pd.DataFrame({'text': textos, 'user': usuarios, 'target': etiquetas})\n",
    "\n",
    "        # Si quieres convertir etiquetas en números:\n",
    "        label_map = {label: i for i, label in enumerate(sorted(set(etiquetas)))}\n",
    "        self.data['target'] = self.data['target'].map(label_map)\n",
    "\n",
    "        print(f\"Clases encontradas: {label_map}\")\n",
    "\n",
    "\n",
    "    def load_vocab_embeddings(self):\n",
    "        '''Embeddings preentrenados en twitter.\n",
    "           emb_mat: Matriz de embeddings. Un vector de tamaño 200 para cada palabra del vocabulario.\n",
    "           vocab: Diccionario, asigna a cada palabra su renglón correspondiente en la matriz de embeddings.\n",
    "        '''\n",
    "        embeddings_list = []\n",
    "        self.vocab_dict = {}\n",
    "        vocab = {}\n",
    "        with open('word2vec_col.txt', 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i!=0:\n",
    "                    values = line.split()\n",
    "                    self.vocab_dict[i+1] = values[0]\n",
    "                    vocab[values[0]] = i+1\n",
    "                    vector = np.asarray(values[1:], \"float32\")\n",
    "                    embeddings_list.append(vector)\n",
    "        embeddings_list.insert(0,np.mean(np.vstack(embeddings_list), axis=0))\n",
    "        embeddings_list.insert(0,np.zeros(100))\n",
    "        self.vocab_dict[0] = '[PAD]'\n",
    "        self.vocab_dict[1] = '[UNK]'\n",
    "        vocab['[PAD]'] = 0\n",
    "        vocab['[UNK]'] = 1\n",
    "        emb_mat = np.vstack(embeddings_list)\n",
    "\n",
    "        return vocab, emb_mat\n",
    "\n",
    "    def get_weights(self):\n",
    "        counts = self.data['target'].value_counts().sort_index()\n",
    "        maxi = counts.max()\n",
    "        weights = [maxi / count for count in counts]\n",
    "        return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        '''Función que ejecuta el dataloader para formar batches de datos.'''\n",
    "\n",
    "        zipped_batch = list(zip(*batch))\n",
    "        word_ids = [torch.tensor(t) for t in zipped_batch[0]]\n",
    "        word_ids = torch.cat(word_ids, dim=0)\n",
    "        lengths = torch.tensor([len(t) for t in zipped_batch[0]])\n",
    "        labels = torch.tensor(zipped_batch[1])\n",
    "        words = zipped_batch[2]\n",
    "        return word_ids, lengths, labels, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a636223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=100, hidden_size=128, num_layers=1,\n",
    "                 bidirectional=False, emb_mat=None, dense_hidden_size=256):\n",
    "        '''Constructor, aquí definimos las capas.\n",
    "        input:\n",
    "            input_size: Tamaño de los embeddings de las palabras.\n",
    "            hidden_size: Tamaño de la capa oculta de la GRU.\n",
    "            num_layers: Número de capas de la GRU.\n",
    "            bidirectional: True si se quiere una GRu bidireccional.\n",
    "            emb_mat: Matriz de embeddings del vocabulario.\n",
    "            dense_hidden_size: Tamaño de la capa ocula del clasificador.\n",
    "        '''\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # Matriz entrenable de embeddings, tamaño vocab_size x 100\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\\\n",
    "                            torch.FloatTensor(emb_mat), freeze=False)\n",
    "        # Gated Recurrent Unit\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, bidirectional=bidirectional)\n",
    "        # Número de direcciones de la GRU\n",
    "        directions = 2 if bidirectional else 1\n",
    "        # Clasificador MLP\n",
    "        self.classifier = nn.Sequential(\\\n",
    "                            nn.Linear(hidden_size*directions, dense_hidden_size),\n",
    "                            nn.BatchNorm1d(dense_hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(dense_hidden_size, num_classes))\n",
    "    \n",
    "    def forward(self, input_seq, lengths):\n",
    "        '''Función feed-forward de la red.\n",
    "        input:\n",
    "            input_seq: Lista de ids para cada palabra.\n",
    "            lengths: Número de palabras en cada una de las observaciones del batch.\n",
    "        output:\n",
    "            x: vectores para clasificar.\n",
    "            return None for consistency with the next model\n",
    "        '''\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        # Forma las secuencias de palabras que entraran a la GRU.\n",
    "        x = x.split(lengths.tolist())\n",
    "        # Añade pading y empaqueta las secuencias (mayor velocidad de cómputo).\n",
    "        x = pad_sequence(x)\n",
    "        x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "        output, hn = self.gru(x)\n",
    "        hn = torch.cat([h for h in hn], dim=-1)\n",
    "        x = self.classifier(hn)\n",
    "        return x, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0cd6ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion, device):\n",
    "    '''Función para evaluar el modelo.'''\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        preds = torch.empty(0).long()\n",
    "        targets = torch.empty(0).long()\n",
    "        scores_list = []\n",
    "        words_list = []\n",
    "        pred_list = []\n",
    "        for data in tqdm(dataloader):\n",
    "            torch.cuda.empty_cache()\n",
    "            seq, seq_len, labels, words = data\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            output, scores = model(seq, seq_len)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            loss = criterion(output, labels)\n",
    "            losses.append(loss.item())\n",
    "            predictions = F.log_softmax(output, dim=1).argmax(1)\n",
    "            preds = torch.cat([preds, predictions.cpu()], dim=0)\n",
    "            targets = torch.cat([targets, labels.cpu()], dim=0)\n",
    "            if scores is not None:\n",
    "                pred_list += predictions.tolist()\n",
    "                scores = scores.cpu().squeeze(2).tolist()\n",
    "                scores_list += scores\n",
    "                words_list += words\n",
    "\n",
    "        model.train()\n",
    "        preds = preds.numpy()\n",
    "        targets = targets.numpy()\n",
    "        f1 = f1_score(targets, preds, average='weighted')\n",
    "\n",
    "        return np.mean(losses), f1, scores_list, words_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0820a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases encontradas: {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}\n",
      "Clases encontradas: {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}\n",
      "Clases encontradas: {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = author_profiling_dataset('train')\n",
    "val_dataset = author_profiling_dataset('val')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn = train_dataset.collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn = val_dataset.collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 10\n",
    "weight_decay=0.0001\n",
    "beta1=0\n",
    "beta2=0.999\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "670faa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases encontradas: {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "ap = author_profiling_dataset(split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
