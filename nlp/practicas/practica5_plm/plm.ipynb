{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juan Carlos Perez Ramirez\n",
    "## Procesamiento de Lenguaje Natural\n",
    "## Practica 5: PLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 18:17:59.899536: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-08 18:18:00.275466: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-08 18:18:00.279097: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-08 18:18:01.434939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_txt += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label]\n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"../../Corpus/mex20_train.txt\", \"../../Corpus/mex20_train_labels.txt\")\n",
    "tr_y = list(map(int, tr_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento y tratamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "class TrigramData:\n",
    "\n",
    "    def __init__(self, vocab_max, tokenizer):\n",
    "        self.vocab_max = vocab_max\n",
    "        self.tokenizer = tokenizer\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.final_vocabulary = set()\n",
    "\n",
    "    def fit(self, raw_texts):\n",
    "\n",
    "        freq_dist = FreqDist()\n",
    "        tokenized_corpus = []\n",
    "\n",
    "        for txt in raw_texts:\n",
    "            tokens = self.tokenizer.tokenize(txt)\n",
    "            tokenized_corpus.append(tokens)\n",
    "            for w in tokens:\n",
    "                freq_dist[w] += 1\n",
    "\n",
    "        self.final_vocabulary = {tok for tok, _ in freq_dist.most_common(self.vocab_max)}\n",
    "        self.final_vocabulary.update([self.UNK, self.SOS, self.EOS])\n",
    "\n",
    "        transformed_corpus = []\n",
    "        for tokens in tokenized_corpus:\n",
    "            transformed_corpus.append(self.transform(tokens))\n",
    "        return transformed_corpus\n",
    "    \n",
    "    def mask_oov(self, w):\n",
    "        return self.UNK if w not in self.final_vocabulary else w\n",
    "    \n",
    "    def add_sos_eos(self, tokens):\n",
    "        return [self.SOS, self.SOS] + tokens + [self.EOS]\n",
    "\n",
    "    def transform(self, tokens):\n",
    "        transformed = []\n",
    "        for w in tokens:\n",
    "            transformed.append(self.mask_oov(w))\n",
    "        transformed = self.add_sos_eos(transformed)\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrigramLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "\n",
    "    def __init__(self, lambda1=0.4, lambda2=0.3, lambda3=0.3):\n",
    "        self.lambda1 = lambda1 # trigramas\n",
    "        self.lambda2 = lambda2 # bigramas\n",
    "        self.lambda3 = lambda3 # unigramas\n",
    "\n",
    "        # Contadores\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "\n",
    "        self.vocab = 0\n",
    "        self.total_tokens = 0\n",
    "        self.V = 0\n",
    "\n",
    "    def train(self, transformed_corpus, final_vocabulary):\n",
    "        self.vocab = final_vocabulary\n",
    "        self.V = len(final_vocabulary)\n",
    "\n",
    "        for tokens in transformed_corpus:\n",
    "            for i, w in enumerate(tokens):\n",
    "\n",
    "                # Unigramas\n",
    "                self.unigram_counts[w] = self.unigram_counts.get(w, 0) + 1\n",
    "\n",
    "                # Bigramas\n",
    "                if i > 0:\n",
    "                    w_prev = tokens[i-1]\n",
    "                    self.bigram_counts[(w_prev, w)] = self.bigram_counts.get((w_prev, w), 0) + 1\n",
    "                    \n",
    "                # Trigramas\n",
    "                if i > 1:\n",
    "                    w_prev2 = tokens[i-2]\n",
    "                    self.trigram_counts.get((w_prev2, w_prev, w), 0) + 1\n",
    "            self.total_tokens = sum(self.unigram_counts.values())\n",
    "\n",
    "    def mask_oov(self, w):\n",
    "        return \"<unk>\" if w not in self.vocab else w\n",
    "\n",
    "    def unigram_probability(self, w):\n",
    "        return (self.unigram_counts.get(self.mask_oov(w), 0) + 1) / (self.total_tokens + self.V)\n",
    "    \n",
    "    def bigram_probability(self, w_prev, w):\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "        \n",
    "        numerator = self.bigram_counts.get((w_prev, w), 0) + 1\n",
    "        denominator = self.unigram_counts.get(w_prev, 0) + self.V\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def trigram_probability(self, w_prev2, w_prev, w):\n",
    "        w_prev2 = self.mask_oov(w_prev2)\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "\n",
    "        numerator = self.trigram_counts.get((w_prev2, w_prev, w), 0) + 1\n",
    "        denominator = self.bigram_counts.get((w_prev2, w_prev), 0) + self.V\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def probability_of_word(self, w_prev2, w_prev, w):\n",
    "        return self.lambda1 * self.trigram_probability(w_prev2, w_prev, w) + \\\n",
    "                self.lambda2 * self.bigram_probability(w_prev, w) + \\\n",
    "                self.lambda3 * self.unigram_probability(w)\n",
    "    \n",
    "    def sequence_probability(self, sequence):\n",
    "        import math\n",
    "        log_prob = 0.0\n",
    "        for i in range(2, len(sequence)):\n",
    "            w_prev2 = sequence[i-2]\n",
    "            w_prev = sequence[i-1]\n",
    "            w = sequence[i]\n",
    "\n",
    "            p = self.probability_of_word(w_prev2, w_prev, w)\n",
    "            log_prob += math.log(p)\n",
    "        return math.exp(log_prob)\n",
    "    \n",
    "    def check_prob(self):\n",
    "        print(sum(self.unigram_probability(w) for w in self.vocab))\n",
    "\n",
    "        print(sum(self.bigram_probability(\"hola\", w) for w in self.vocab))\n",
    "\n",
    "        print(sum(self.trigram_probability(\"hola\", \"como\", w) for w in self.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "trigram_data = TrigramData(13580, tokenizer)\n",
    "transformed_corpus = trigram_data.fit(tr_txt)\n",
    "final_vocab = trigram_data.final_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_lm = TrigramLanguageModel(lambda1=6.0, lambda2=3.0, lambda3=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_lm.train(transformed_corpus, final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000000000098\n",
      "0.9999999999998532\n",
      "1.0000000000002154\n"
     ]
    }
   ],
   "source": [
    "trigram_lm.check_prob()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P('mundo' | '<s>', 'hola') = 0.0011064817715329\n"
     ]
    }
   ],
   "source": [
    "w_prev2, w_prev, w = \"<s>\", \"hola\", \"mundo\"\n",
    "p_w = trigram_lm.probability_of_word(w_prev2, w_prev, w)\n",
    "print(f\"\\nP('{w}' | '{w_prev2}', '{w_prev}') = {p_w:.16f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P('a' | '<s>', 'saludos') = 0.0167546573857528\n"
     ]
    }
   ],
   "source": [
    "w_prev2, w_prev, w = \"<s>\", \"saludos\", \"a\"\n",
    "p_w = trigram_lm.probability_of_word(w_prev2, w_prev, w)\n",
    "print(f\"\\nP('{w}' | '{w_prev2}', '{w_prev}') = {p_w:.16f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P('la' | 'vete', 'a') = 0.0717871315164460\n"
     ]
    }
   ],
   "source": [
    "w_prev2, w_prev, w = \"vete\", \"a\", \"la\"\n",
    "p_w = trigram_lm.probability_of_word(w_prev2, w_prev, w)\n",
    "print(f\"\\nP('{w}' | '{w_prev2}', '{w_prev}') = {p_w:.16f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P('tu' | 'hijo', 'de') = 0.0123598509137938\n"
     ]
    }
   ],
   "source": [
    "w_prev2, w_prev, w = \"hijo\", \"de\", \"tu\"\n",
    "p_w = trigram_lm.probability_of_word(w_prev2, w_prev, w)\n",
    "print(f\"\\nP('{w}' | '{w_prev2}', '{w_prev}') = {p_w:.16f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P(['hola', 'como', 'has', 'estado', '</s>']) = 0.0000000264188843\n"
     ]
    }
   ],
   "source": [
    "seq_example = [\"hola\", \"como\", \"has\", \"estado\", \"</s>\"]\n",
    "seq_prob = trigram_lm.sequence_probability(seq_example)\n",
    "print(f\"\\nP({seq_example}) = {seq_prob:.16f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
