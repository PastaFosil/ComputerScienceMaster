{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 6\n",
    "\n",
    "Modelo Neuronal de Bengio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace  # Guardar variables y parámetros\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesamiento\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tqmd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables generales para reproducibilidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)  # python seed\n",
    "np.random.seed(seed)  # numpy seed\n",
    "torch.manual_seed(seed)  # torch seed\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función : get_texts_from_file()\n",
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "  \"\"\"\n",
    "  Función para leer los archivos de tuits. Cada línea (tuit) será un elemento de la lista.\n",
    "  \"\"\"\n",
    "  tr_txt = []\n",
    "  tr_y = []\n",
    "  with open(path_corpus) as f_corpus, open(path_truth) as f_truth:\n",
    "    for tweet in f_corpus:\n",
    "      tr_txt.append(tweet)\n",
    "    for label in f_truth:\n",
    "      tr_y.append(label)\n",
    "  return tr_txt, tr_y\n",
    "\n",
    "\n",
    "PATH = \"../../corpus/\"\n",
    "\n",
    "X_train, y_train = get_texts_from_file(\n",
    "  PATH + \"/mex20_train.txt\",\n",
    "  PATH + \"/mex20_train_labels.txt\")\n",
    "\n",
    "X_val, y_val = get_texts_from_file(\n",
    "  PATH + \"/mex20_val.txt\",\n",
    "  PATH + \"/mex20_val_labels.txt\")\n",
    "\n",
    "y_train = list(map(int, y_train))\n",
    "y_val = list(map(int, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡! En la parte en la que se usa la matriz de embeddings (dentro de `NgramData.fit()`) se le asocia un atributo `.vector_size` que hast ahora no se ha definido. Checa eso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "  \"\"\"\n",
    "  Esta clase toma un corpus y a través de los métodos fit y transform, se crea una lista de \n",
    "  n-gramas pensada para el entrenamiento de la red neuronal de Bengio pensando en una CBOW.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               N: int,\n",
    "               vocab_max: int = 5000,\n",
    "               tokenizer: callable = None,\n",
    "               embeddings: np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Constructor de la clase.\n",
    "\n",
    "    Args:\n",
    "        N (int): Tamaño de los n-gramas.\n",
    "        vocab_max (int, optional): Tamaño máximo del vocabulario a considerar. Defaults to 5000.\n",
    "        tokenizier (callable, optional): Tokenizador. Defaults to None.\n",
    "        embeddings (np.ndarray, optional): Matriz de embeddings pre-entrenada. Debe entrar en el orden en el que entran las palabras. Defaults to None.\n",
    "    \"\"\"\n",
    "    self.N = N\n",
    "    self.vocab_max = vocab_max\n",
    "    self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "    self.embeddings = embeddings\n",
    "\n",
    "    # Tokens que no queremos en nuestro corpus.\n",
    "    self.punct = ['.', ',', ';', ':', '-', '^', '\"'\n",
    "                  '\"', '!', '¡', '¿', '?', '<url>', '#', '@usuario']\n",
    "\n",
    "    # Tokens especiales\n",
    "    self.UNK = \"<unk>\"\n",
    "    self.SOS = \"<s>\"\n",
    "    self.EOS = \"</s>\"\n",
    "\n",
    "  def get_vocab_size(self) -> int:\n",
    "    \"\"\"\n",
    "    Devuelve el tamaño del vocabulario.\n",
    "\n",
    "    Returns:\n",
    "        int: Tamaño del vocabulario.\n",
    "    \"\"\"\n",
    "    return len(self.vocab)\n",
    "\n",
    "  def default_tokenizer(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizador por defecto. Simplemente separa cada oración por espacios.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento a tokenizar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    return doc.split(\" \")\n",
    "\n",
    "  def remove_word(self, word: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica si la palabra en cuestión debe eliminarse según los siguientes criterios:\n",
    "    - Es un signo de puntuación\n",
    "    - Es un dígito\n",
    "\n",
    "    Args:\n",
    "        word (str): Palabra a evaluar.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si se elimina.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    is_punct = True if word in self.punct else False\n",
    "    is_digit = word.isnumeric()\n",
    "    return is_punct or is_digit\n",
    "\n",
    "  def sortFreqDist(self, freq_dist: nltk.FreqDist) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con el top de palabras por frecuencia. El tamaño de la lista es self.vocab_max.\n",
    "\n",
    "    Args:\n",
    "        freq_dist (nltk.FreqDist): Objeto de frecuencias (nltk) del corpus considerado.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tamaño self.vocab_max.\n",
    "    \"\"\"\n",
    "    freq_dist = dict(freq_dist)\n",
    "    # Aquí key es una función que se aplica a cada parámetro\n",
    "    # antes de compararlo. En este caso se pasa\n",
    "    # freq_dist.get para asegurarse de que el ordenamiento\n",
    "    # se haga por las frecuencias y no por orden alfabético.\n",
    "    return sorted(freq_dist,\n",
    "                  key=freq_dist.get,\n",
    "                  reverse=True)\n",
    "\n",
    "  def get_vocab(self, corpus: list[str]) -> set:\n",
    "    \"\"\"\n",
    "    Devuelve el vocabulario a partir de un corpus dado.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Corpus del cual se quiere obtener el vocabulario. Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        set: Vocabulario.\n",
    "    \"\"\"\n",
    "    freq_dist = FreqDist(\n",
    "      [w.lower()\n",
    "       for sentence in corpus\n",
    "       for w in self.tokenizer(sentence)\n",
    "       if not self.remove_word(w)]\n",
    "    )\n",
    "    sorted_words = self.sortFreqDist(freq_dist)[:self.vocab_max-3]\n",
    "    return set(sorted_words)\n",
    "\n",
    "  def fit(self, corpus: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Carga el vocabulario y crea diccionarios de índices <-> palabras. Además, si se aporta una matriz de embeddings pre-entrenados, también construye la submatriz con los elementos del vocabulario.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "    \"\"\"\n",
    "    # Cargamos el vocabulario\n",
    "    self.vocab = self.get_vocab(corpus)\n",
    "    self.vocab.add(self.UNK)\n",
    "    self.vocab.add(self.SOS)\n",
    "    self.vocab.add(self.EOS)\n",
    "\n",
    "    # Diccionarios palabras <-> ids\n",
    "    self.w2id = dict()\n",
    "    self.id2w = dict()\n",
    "\n",
    "    if self.embeddings:\n",
    "      self.embeddings_matrix = np.empty([self.vocab_max,\n",
    "                                         self.embeddings.vector_size])\n",
    "\n",
    "    id = 0\n",
    "    for doc in corpus:\n",
    "      for word in self.tokenizer(doc):\n",
    "        word_ = word.lower()\n",
    "        if (word_ in self.vocab) and (not word_ in self.w2id):\n",
    "          self.w2id[word_] = id\n",
    "          self.id2w[id] = word_\n",
    "\n",
    "          # Si se aporta una matriz de embeddings,\n",
    "          # aquí se crea la submatriz.\n",
    "          if self.embeddings:\n",
    "            if word in self.embeddings:\n",
    "              self.embeddings_matrix[id] = self.embeddings[word_]\n",
    "            else:\n",
    "              self.embeddings_matrix[id] = np.random.rand(\n",
    "                self.embeddings.vector_size)\n",
    "\n",
    "          id += 1\n",
    "\n",
    "    # Añadirmos los tokens especiales a los diccionarios.\n",
    "    self.w2id.update(\n",
    "      {self.UNK: id,\n",
    "       self.SOS: id + 1,\n",
    "       self.EOS: id + 2}\n",
    "    )\n",
    "    self.id2w.update(\n",
    "      {id: self.UNK,\n",
    "       id + 1: self.SOS,\n",
    "       id + 2: self.EOS}\n",
    "    )\n",
    "\n",
    "  def get_ngram_doc(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con n-gramas de un documento dado.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento del que se quieren obtener los n-gramas.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de n-gramas.\n",
    "    \"\"\"\n",
    "    doc_tokens = self.tokenizer(doc)\n",
    "    doc_tokens = self.replace_unk(doc_tokens)\n",
    "    doc_tokens = [w.lower()\n",
    "                  for w in doc_tokens]\n",
    "    doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "\n",
    "    return list(nltk.ngrams(doc_tokens, self.N))\n",
    "\n",
    "  def replace_unk(self, doc_tokens: list[str]) -> list:\n",
    "    \"\"\"\n",
    "    Toma un lista de tokens e intercambia los tokens out-of-vocabulary por el token especial self.UNK.\n",
    "\n",
    "    Args:\n",
    "        doc_tokens (list[str]): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens procesada.\n",
    "    \"\"\"\n",
    "    for i, token in enumerate(doc_tokens):\n",
    "      if token.lower() not in self.vocab:\n",
    "        doc_tokens[i] = self.UNK\n",
    "    return doc_tokens\n",
    "\n",
    "  def transform(self, corpus: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Devuelve una tupla de arreglos de Numpy. El primero tendrá los ids de las palabras en el contexto, mientras que la segunda el id de la palabra que se debe predecir.\n",
    "\n",
    "    Se piensa en un modelo de CBOW. Damos el contexto y queremos predecir la palabra que sigue.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arreglos de numpy con ids de los contextos y id de la palabra objetivo.\n",
    "    \"\"\"\n",
    "    X_ngrams = list()\n",
    "    y = []\n",
    "\n",
    "    for doc in corpus:\n",
    "      doc_ngram = self.get_ngram_doc(doc)\n",
    "      for words_window in doc_ngram:\n",
    "        words_window_ids = [self.w2id[w]\n",
    "                            for w in words_window]\n",
    "        X_ngrams.append(list(words_window_ids[:-1]))\n",
    "        y.append(words_window_ids[-1])\n",
    "\n",
    "    return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos nuestros datos usando la clase `NgramData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize)\n",
    "ngram_data.fit(X_train)\n",
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario : 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño del vocabulario : {ngram_data.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean los objetos `TensorDataset` para guardar los datos de entrenamiento y validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "args.batch_size = 64\n",
    "\n",
    "# Num of workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle=True)\n",
    "\n",
    "# Val\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : torch.Size([64, 3])\n",
      "y shape : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape : {batch[0].shape}')\n",
    "print(f'y shape : {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4998, 4997,  236],\n",
       "        [  32,   26, 4997],\n",
       "        [   6,  219,  162],\n",
       "        [ 109,    6, 1151],\n",
       "        [4998, 4998, 4998],\n",
       "        [  48,  343,   45],\n",
       "        [  43,    8, 1316],\n",
       "        [4997, 4997,    6],\n",
       "        [4997, 1792,  226],\n",
       "        [4998, 4997,   48],\n",
       "        [   6,    8,  570],\n",
       "        [4998,  706, 4997],\n",
       "        [4997, 4997, 4997],\n",
       "        [3538, 4997,  166],\n",
       "        [4998, 4998, 4998],\n",
       "        [  14, 4997, 4997],\n",
       "        [ 114,    1, 3259],\n",
       "        [ 459,   51,  460],\n",
       "        [  45, 4997,   83],\n",
       "        [  48,  378,   48],\n",
       "        [ 952,   43, 2497],\n",
       "        [  60,   45, 2299],\n",
       "        [ 254,   48,  256],\n",
       "        [  55,   48,   20],\n",
       "        [ 423, 2459,   33],\n",
       "        [4998, 4998, 4998],\n",
       "        [4997, 4997,   66],\n",
       "        [ 941, 4997, 4997],\n",
       "        [  48,  167,  129],\n",
       "        [1587,  338,   16],\n",
       "        [ 114,   48,   46],\n",
       "        [ 990, 4997,  702],\n",
       "        [4998, 4998, 4998],\n",
       "        [ 112,  273, 4997],\n",
       "        [1550,  193,  711],\n",
       "        [ 106, 4997,   93],\n",
       "        [ 267,   16,  929],\n",
       "        [4998, 1262,   45],\n",
       "        [4997,   60, 4142],\n",
       "        [1587,  319, 2389],\n",
       "        [4997,   39, 4274],\n",
       "        [1240,   83, 4997],\n",
       "        [ 161, 4997,   34],\n",
       "        [4998,   48,    8],\n",
       "        [ 254,   26, 4467],\n",
       "        [4998, 4998,  873],\n",
       "        [  32,  125,  773],\n",
       "        [4997,   55,   26],\n",
       "        [  39, 2315, 1874],\n",
       "        [ 641, 4997,   32],\n",
       "        [  65, 2200,    6],\n",
       "        [4998, 4998,   39],\n",
       "        [3174, 4997, 4997],\n",
       "        [   6,    8, 1273],\n",
       "        [4998, 4998, 4998],\n",
       "        [ 319,  505, 4997],\n",
       "        [2131,  129,  162],\n",
       "        [1571,   60, 1247],\n",
       "        [4998,  253, 4997],\n",
       "        [   6,  117,   50],\n",
       "        [ 129, 4997,   32],\n",
       "        [ 621,  621,   81],\n",
       "        [4997,  245,   60],\n",
       "        [4998,  424,  198]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de lenguaje neuronal de Bengio c:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLanguageModel(nn.Module):\n",
    "  \"\"\"\n",
    "  Red neuronal de Bengio :)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, args):\n",
    "    \"\"\"\n",
    "    Constructor  de la clase.\n",
    "\n",
    "    El modelo de red neuronal par lenguaje de Bengio tiene la siguiente estructura:\n",
    "    Para un modelo de n-gramas, se dan las primeras n-1 palabras como contexto y se intenta predecir la n-ésima palabra.\n",
    "    (1) n-1 representaciones iniciales: suelen ser one-hot. Pero aquí se toman de NgramData.\n",
    "        x\n",
    "    (2) n-1 representaciones aprendidas de tamaño m: se obtienen de manera individual (por palabra). \n",
    "        (x = Cx)\n",
    "        En esta implementación C se inicia de manera aleatoria.\n",
    "    (3) Capa oculta de tamaño h: se mezclan las n-1 representaciones del paso anterior y se aplica tanh. \n",
    "        (h = tanh(Hx + d))\n",
    "        Nosotros vamos a usar ReLu en vez de tanh.\n",
    "    (4) Capa de salida de tamaño m: se aplica softmax a la salida de la capa anterior.\n",
    "        (y = softmax(Uh + b))\n",
    "        Nosotros no vamos a aplicar softmax aquí, sino afuerita.\n",
    "\n",
    "    Args:\n",
    "        args (Any): Diccionario de variables.\n",
    "    \"\"\"\n",
    "    super(NeuralLanguageModel, self).__init__()\n",
    "\n",
    "    self.window_size = args.N - 1  # Las n-1 palabras que entran (el contexto).\n",
    "    self.embedding_size = args.m  # Tamaño de las representaciones.\n",
    "\n",
    "    # Matriz C para convertir las representaciones. Pero está chido porque sus entradas son \"entrenables\".\n",
    "    self.emb = nn.Embedding(args.vocab_size, args.m)\n",
    "    # Primera capa oculta de las representaciones aprendidas a la oculta.\n",
    "    self.fc1 = nn.Linear(args.m * (args.N - 1), args.d_h)\n",
    "    # Un dropout para alocarnos\n",
    "    self.drop1 = nn.Dropout(p=args.dropout)\n",
    "    # Aquí solamente se va a hacer el producto por la matriz U.\n",
    "    # La softmax se va a aplicar por fuera de la red para obtener la siguiente palabra según la red.\n",
    "    self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Aquí se cambia la representación inicial por la aprendida.\n",
    "    # Es un producto matricial. Aquí las representaciones siguen siendo matrices.\n",
    "    x = self.emb(x)\n",
    "    # Se cambia el tamaño para que se considere como una sola capa.\n",
    "    x = x.view(-1, self.window_size * self.embedding_size)\n",
    "    # Aquí se hace relu(Hx + d)\n",
    "    h = F.relu(self.fc1(x))  # relu(z) = max{0, z}\n",
    "    # El dropout para alocarnoooos wuuuuuuuu\n",
    "    h = self.drop1(h)\n",
    "\n",
    "    # Devolvemos solamente (Uh + b)\n",
    "    return self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  Aquí se toma la salida de la red neuronal (las neuronas de la última capa oculta).\n",
    "  Uh + b\n",
    "  Se les aplica la softmax\n",
    "  softmax(Uh + b)\n",
    "  Y luego se devuelve el índice de la neurona de mayor valor.\n",
    "\n",
    "  Args:\n",
    "      raw_logits (torch.Tensor | float): La salida de la red (Uh + b)\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor | int: Índice de la neurona con mayor valor después de softmax.\n",
    "  \"\"\"\n",
    "  # Se aplica softmax.\n",
    "  probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "  # Se obtiene el índice del valor máximo.\n",
    "  y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data: torch.Tensor,\n",
    "               model,\n",
    "               gpu: bool = False) -> float | int:\n",
    "  \"\"\"\n",
    "  Evalúa el desempeño del modelo sobre un conjunto de validación.\n",
    "\n",
    "  Args:\n",
    "      data (torch.Tensor): Conjunto de validación sobre el que se va a evaluar el modelo.\n",
    "      model (_type_): Modelo que se va a evaluar.\n",
    "      gpu (bool, optional): ¿Usamos gpu? Sí/No. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "      float | int: Puntaje de accuracy sobre todo el conjunto de validación.\n",
    "  \"\"\"\n",
    "  with torch.no_grad():\n",
    "    preds, tgts = list(), list()\n",
    "\n",
    "    for window_words, labels in data:\n",
    "      # windoe_words es una matriz de (64, 3)\n",
    "      # labels es un vector de (64)\n",
    "      if gpu:\n",
    "        window_words = window_words.cuda()\n",
    "\n",
    "      outputs = model(window_words)\n",
    "\n",
    "      # Obrenemos las predicciones\n",
    "      y_pred = get_preds(outputs)  # (64)\n",
    "\n",
    "      tgt = labels.numpy()  # Lo que esperábamos ver (64)\n",
    "      tgts.append(tgt)  # Se añade a tgts (Lista de respuestas)\n",
    "      preds.append(y_pred)  # Lo que vimos (Lista de predicciones)\n",
    "\n",
    "  # Aquí abajo \"desempaquetamos\" los targets y las predicciones\n",
    "  # e : element\n",
    "  # l : list\n",
    "  # lista de todos los target\n",
    "  tgts = [e for l in tgts for e in l]\n",
    "  # lista de todas las predicciones\n",
    "  preds = [e for l in preds for e in l]\n",
    "\n",
    "  return accuracy_score(tgts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state: dict,\n",
    "                    is_best: bool,\n",
    "                    checkpoint_path: str,\n",
    "                    filename: str = \"checkpoint.pt\"):\n",
    "  \"\"\"\n",
    "  Guarda el modelo si se ve una mejora cra. a iteraciones anteriores.\n",
    "\n",
    "  Args:\n",
    "      state (dict): Información que queremos guardar sobre el modelo.\n",
    "        DEBE incluir model.state_dict()\n",
    "      is_best (bool): ¿Esta versión del modelo es mejor?\n",
    "      checkpoint_path (str): Directorio en el que se va a guardar el modelo.\n",
    "      filename (str, optional): _description_. Defaults to \"checkpoint.pt\".\n",
    "  \"\"\"\n",
    "  # Armamos la ruta para el archivo que va a guardar el modelo.\n",
    "  filename = os.path.join(checkpoint_path, filename)\n",
    "  # Aquí se guarda.\n",
    "  torch.save(state, filename)\n",
    "\n",
    "  # Si además es el very besto modelo ever, le ponemos un nombre que le cheque :))\n",
    "  if is_best:\n",
    "    shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparámetros de la red c:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.m = 100  # Dimensión de los embeddings de palabras\n",
    "args.d_h = 200  # Dimensión de la capa oculta\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Entrenamiento\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Guardado de los modelos\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El scheduler sirve para\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancho/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo\n",
    "model = NeuralLanguageModel(args)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "  model.cuda()\n",
    "\n",
    "# Perdida, optimización y scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  \"min\",\n",
    "  patience=args.lr_patience,\n",
    "  verbose=True,\n",
    "  factor=args.lr_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9802c3e8704d47878ad046e7dee1b299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.17334786636805527\n",
      "Epoch [1/100], Loss 5.5255 - Val accuracy 0.2106 - Epoch time : 3.9712634086608887\n",
      "Train acc: 0.18161700829550473\n",
      "Epoch [2/100], Loss 5.0834 - Val accuracy 0.2051 - Epoch time : 3.347290277481079\n",
      "Train acc: 0.18968497670027717\n",
      "Epoch [3/100], Loss 4.8710 - Val accuracy 0.2247 - Epoch time : 3.378157377243042\n",
      "Train acc: 0.19355529164825452\n",
      "Epoch [4/100], Loss 4.7027 - Val accuracy 0.1987 - Epoch time : 3.3027946949005127\n",
      "Train acc: 0.19839004690073514\n",
      "Epoch [5/100], Loss 4.5524 - Val accuracy 0.1486 - Epoch time : 3.3078694343566895\n",
      "Train acc: 0.20009139115413974\n",
      "Epoch [6/100], Loss 4.4190 - Val accuracy 0.1334 - Epoch time : 3.4088571071624756\n",
      "Train acc: 0.20137689310247858\n",
      "Epoch [7/100], Loss 4.2941 - Val accuracy 0.2161 - Epoch time : 3.3657355308532715\n",
      "Train acc: 0.2048878575302294\n",
      "Epoch [8/100], Loss 4.1818 - Val accuracy 0.2211 - Epoch time : 3.3464605808258057\n",
      "Train acc: 0.2091693071345358\n",
      "Epoch [9/100], Loss 4.0697 - Val accuracy 0.1340 - Epoch time : 3.3119735717773438\n",
      "Train acc: 0.2104949810187603\n",
      "Epoch [10/100], Loss 3.9740 - Val accuracy 0.2160 - Epoch time : 3.363835334777832\n",
      "Train acc: 0.21539595719680232\n",
      "Epoch [11/100], Loss 3.8740 - Val accuracy 0.1487 - Epoch time : 3.419468879699707\n",
      "Train acc: 0.22095380729522354\n",
      "Epoch [12/100], Loss 3.7903 - Val accuracy 0.2251 - Epoch time : 3.45827054977417\n",
      "Train acc: 0.2292515089583417\n",
      "Epoch [13/100], Loss 3.6955 - Val accuracy 0.1953 - Epoch time : 3.3866078853607178\n",
      "Train acc: 0.23739354436990318\n",
      "Epoch [14/100], Loss 3.6198 - Val accuracy 0.1262 - Epoch time : 3.4248008728027344\n",
      "Train acc: 0.24653234594062587\n",
      "Epoch [15/100], Loss 3.5449 - Val accuracy 0.1079 - Epoch time : 3.327522039413452\n",
      "Train acc: 0.256464543245089\n",
      "Epoch [16/100], Loss 3.4748 - Val accuracy 0.2242 - Epoch time : 3.31701922416687\n",
      "Train acc: 0.26318926254368696\n",
      "Epoch [17/100], Loss 3.4234 - Val accuracy 0.1901 - Epoch time : 3.3023221492767334\n",
      "Train acc: 0.27227439691881256\n",
      "Epoch [18/100], Loss 3.3645 - Val accuracy 0.2064 - Epoch time : 3.264073133468628\n",
      "Train acc: 0.27788748342907643\n",
      "Epoch [19/100], Loss 3.3179 - Val accuracy 0.1408 - Epoch time : 3.2712292671203613\n",
      "Train acc: 0.2854853146466878\n",
      "Epoch [20/100], Loss 3.2681 - Val accuracy 0.1448 - Epoch time : 3.2411653995513916\n",
      "Train acc: 0.2910736075402724\n",
      "Epoch [21/100], Loss 3.2280 - Val accuracy 0.1524 - Epoch time : 3.2987987995147705\n",
      "Train acc: 0.2986513527899409\n",
      "Epoch [22/100], Loss 3.1860 - Val accuracy 0.1776 - Epoch time : 3.330838918685913\n",
      "Train acc: 0.30150261870807055\n",
      "Epoch [23/100], Loss 3.1516 - Val accuracy 0.1434 - Epoch time : 3.2651684284210205\n",
      "Train acc: 0.3079414268067328\n",
      "Epoch [24/100], Loss 3.1182 - Val accuracy 0.1564 - Epoch time : 3.350417137145996\n",
      "Train acc: 0.31284679679026234\n",
      "Epoch [25/100], Loss 3.0828 - Val accuracy 0.1957 - Epoch time : 3.3036201000213623\n",
      "Train acc: 0.31599778301128834\n",
      "Epoch [26/100], Loss 3.0537 - Val accuracy 0.1842 - Epoch time : 3.3246536254882812\n",
      "Train acc: 0.3664458884023621\n",
      "Epoch [27/100], Loss 2.7578 - Val accuracy 0.2071 - Epoch time : 3.3745269775390625\n",
      "Train acc: 0.3707857128510023\n",
      "Epoch [28/100], Loss 2.7169 - Val accuracy 0.2065 - Epoch time : 3.3068623542785645\n",
      "Train acc: 0.37449063240670066\n",
      "Epoch [29/100], Loss 2.6967 - Val accuracy 0.1776 - Epoch time : 3.3630824089050293\n",
      "Train acc: 0.37544534357048165\n",
      "Epoch [30/100], Loss 2.6826 - Val accuracy 0.1858 - Epoch time : 3.286593198776245\n",
      "Train acc: 0.37746900484071827\n",
      "Epoch [31/100], Loss 2.6717 - Val accuracy 0.1660 - Epoch time : 3.3044257164001465\n",
      "No improvement. Breaking out of loop.\n",
      "--- 107.49855828285217 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in tqdm(range(args.num_epochs),\n",
    "                  desc=\"Epochs\"):\n",
    "  epoch_start_time = time.time()\n",
    "  loss_epoch = []\n",
    "  training_metric = []\n",
    "  model.train()\n",
    "\n",
    "  for window_words, labels in train_loader:\n",
    "\n",
    "    # Si hay GPU\n",
    "    if args.use_gpu:\n",
    "      window_words = window_words.cuda()\n",
    "      labels = labels.cuda()\n",
    "\n",
    "    # Forward\n",
    "    outputs = model(window_words)\n",
    "    loss = criterion(outputs,\n",
    "                     labels)\n",
    "    loss_epoch.append(loss.item())\n",
    "\n",
    "    # Obtener métricas de entrenamiento\n",
    "    y_pred = get_preds(outputs)\n",
    "    tgt = labels.cpu().numpy()\n",
    "    training_metric.append(accuracy_score(tgt,\n",
    "                                          y_pred))\n",
    "\n",
    "    # Backprop y optimizamos\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Guardamos la métrica por época\n",
    "  mean_epoch_metric = np.mean(training_metric)\n",
    "  train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Validación para esta época\n",
    "  model.eval()\n",
    "  tuning_metric = model_eval(val_loader,\n",
    "                             model,\n",
    "                             gpu=args.use_gpu)\n",
    "  metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Scheduler\n",
    "  scheduler.step(tuning_metric)\n",
    "\n",
    "  # Revisa si la métrica mejoró\n",
    "  is_improvement = tuning_metric > best_metric\n",
    "  if is_improvement:\n",
    "    best_metric = tuning_metric\n",
    "    n_no_improve = 0\n",
    "  else:\n",
    "    n_no_improve += 1\n",
    "\n",
    "  # Si la métrica mejora, guarda el mejor modelo\n",
    "  save_checkpoint(\n",
    "    {\n",
    "      \"epoch\": epoch + 1,\n",
    "      \"state_dict\": model.state_dict(),\n",
    "      \"optimizer\": optimizer.state_dict(),\n",
    "      \"scheduler\": scheduler.state_dict(),\n",
    "      \"best_metric\": best_metric\n",
    "    },\n",
    "    is_improvement,\n",
    "    args.savedir\n",
    "  )\n",
    "\n",
    "  # Parada temprana por paciencie\n",
    "  if n_no_improve >= args.patience:\n",
    "    print(\"No improvement. Breaking out of loop.\")\n",
    "    break\n",
    "\n",
    "  print(f\"Train acc: {mean_epoch_metric}\")\n",
    "  print(f\"Epoch [{epoch + 1}/{args.num_epochs}], Loss {np.mean(loss_epoch):.4f} - Val accuracy {tuning_metric:.4f} - Epoch time : {time.time() - epoch_start_time}\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    '''Devuelve la lista de las n palabras mas cercanas a word'''\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]]) # obtener id de las palabras\n",
    "    word_embed = embeddings(word_id) # obtener el embedding de la palabra\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach() # calcular distancias a todas las palabras\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # ordenar por distancia\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<unk> 10.232197\n",
      "castiguen 10.771348\n",
      "dure 10.835211\n",
      "indios 10.851449\n",
      "pones 10.926342\n",
      "odian 10.955638\n",
      "mueres 11.088757\n",
      "quedado 11.160634\n",
      "estuve 11.17065\n",
      "holanda 11.220155\n"
     ]
    }
   ],
   "source": [
    "best_model = NeuralLanguageModel(args)\n",
    "state_dict = torch.load(\"model/model_best.pt\", weights_only=False)\n",
    "best_model.load_state_dict(state_dict[\"state_dict\"])\n",
    "best_model.train(False)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"perro\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    '''Devuelve el texto tokenizado y los ids de las palabras'''\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer.tokenize(text)]\n",
    "    token_ids = [ngram_data.w2id[w.lower()] for w in all_tokens]\n",
    "    return all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits, temperature=1.0):\n",
    "    '''\n",
    "    Dados los logits y la temperatura \n",
    "    (un parametro de diversidad que indica cuan determinista sera el modelo), \n",
    "    devuelve la siguiente palabra\n",
    "    '''\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, token_ids):\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "\n",
    "    # no es necesario aplicar softmax (de aplicarse daria el mismo resultado que la prediccion cruda)\n",
    "    #y_probs = F.softmax(y_raw_pred, dim=1)\n",
    "    #y_pred = torch.argmax(y_probs, dim=1).detach().numpy()\n",
    "\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "\n",
    "    for i in range(100):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "\n",
    "    return \" \".join(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo genera oraciones con partes coherentes, pero esto ayudara mas para calcular la probabilidad de ocurrencia de oraciones dadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> <unk> estoy <unk> <unk> de <unk> han <unk> hijos de portada <unk> quien chingados <unk> son drogadicto <unk> <unk> te llega pero narizon <unk> para <unk> en <unk> <unk> el ancho equivoca <unk> a <unk> <unk> <unk> cuál tipo <unk> <unk> <unk> antessss pero <unk> loca <unk> no tiene ni madres <unk> ni <unk> <unk> putos <unk> tantita madre <unk> <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> estoy hasta un sueño que cagado se años ejercicio el <unk> ya se <unk> más que los lea lentes les encanta <unk> <unk> qué me pones mucho <unk> porfa 😌 que soy feliz ni loca <unk> yo sigo </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> estoy'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> saludos a cruda ahi <unk> llegó han por eso todo el mundo para meter más dj te <unk> aún <unk> <unk> <unk> fundadora </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> saludos a'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "yo opino que despertar <unk> d yo dinero y el <unk> cancelar de <unk> <unk> madre <unk> una vez le enseña caso <unk> <unk> <unk> se <unk> 😍 <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = 'yo opino que'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, test, ngram_model):\n",
    "    # Generar n gram windows from input text and the respective label y\n",
    "    X, y = ngram_model.transform(test)\n",
    "    # discard first two n-gram windows since they may contain <s> tokens (not necessary)\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "    return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa como cambian los valores para oraciones menos probables de observar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -771.2288104891777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -859.9079782366753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos procesamiento clase en la de natural lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -916.2573439478874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \" la natural Estamos clase en de de lenguaje procesamiento\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructuras sintacticas correctas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-448.9200522303581 me voy sino la a gano chingada\n",
      "-448.9200522303581 me voy sino a la gano chingada\n",
      "-448.9200522303581 me voy sino a gano la chingada\n",
      "-448.9200522303581 me voy sino a gano chingada la\n",
      "-448.9200522303581 me voy la sino a gano chingada\n",
      "------------------------------\n",
      "-453.9536334872246 chingada gano me la a sino voy\n",
      "-453.9536334872246 chingada gano me a voy la sino\n",
      "-453.9536334872246 chingada gano me a sino la voy\n",
      "-453.9536334872246 chingada gano me a la voy sino\n",
      "-453.9536334872246 chingada gano me a la sino voy\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [' '.join(p) for p in permutations(word_list)]\n",
    "#print(perms)\n",
    "print('-'*30)\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "\n",
    "print('-'*30)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
