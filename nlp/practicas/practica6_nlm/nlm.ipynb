{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√°ctica 6\n",
    "\n",
    "Modelo Neuronal de Bengio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace  # Guardar variables y par√°metros\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesamiento\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tqmd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables generales para reproducibilidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed)  # python seed\n",
    "np.random.seed(seed)  # numpy seed\n",
    "torch.manual_seed(seed)  # torch seed\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n : get_texts_from_file()\n",
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "  \"\"\"\n",
    "  Funci√≥n para leer los archivos de tuits. Cada l√≠nea (tuit) ser√° un elemento de la lista.\n",
    "  \"\"\"\n",
    "  tr_txt = []\n",
    "  tr_y = []\n",
    "  with open(path_corpus) as f_corpus, open(path_truth) as f_truth:\n",
    "    for tweet in f_corpus:\n",
    "      tr_txt.append(tweet)\n",
    "    for label in f_truth:\n",
    "      tr_y.append(label)\n",
    "  return tr_txt, tr_y\n",
    "\n",
    "\n",
    "PATH = \"../../corpus/\"\n",
    "\n",
    "X_train, y_train = get_texts_from_file(\n",
    "  PATH + \"/mex20_train.txt\",\n",
    "  PATH + \"/mex20_train_labels.txt\")\n",
    "\n",
    "X_val, y_val = get_texts_from_file(\n",
    "  PATH + \"/mex20_val.txt\",\n",
    "  PATH + \"/mex20_val_labels.txt\")\n",
    "\n",
    "y_train = list(map(int, y_train))\n",
    "y_val = list(map(int, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°! En la parte en la que se usa la matriz de embeddings (dentro de `NgramData.fit()`) se le asocia un atributo `.vector_size` que hast ahora no se ha definido. Checa eso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "  \"\"\"\n",
    "  Esta clase toma un corpus y a trav√©s de los m√©todos fit y transform, se crea una lista de \n",
    "  n-gramas pensada para el entrenamiento de la red neuronal de Bengio pensando en una CBOW.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               N: int,\n",
    "               vocab_max: int = 5000,\n",
    "               tokenizer: callable = None,\n",
    "               embeddings: np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Constructor de la clase.\n",
    "\n",
    "    Args:\n",
    "        N (int): Tama√±o de los n-gramas.\n",
    "        vocab_max (int, optional): Tama√±o m√°ximo del vocabulario a considerar. Defaults to 5000.\n",
    "        tokenizier (callable, optional): Tokenizador. Defaults to None.\n",
    "        embeddings (np.ndarray, optional): Matriz de embeddings pre-entrenada. Debe entrar en el orden en el que entran las palabras. Defaults to None.\n",
    "    \"\"\"\n",
    "    self.N = N\n",
    "    self.vocab_max = vocab_max\n",
    "    self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "    self.embeddings = embeddings\n",
    "\n",
    "    # Tokens que no queremos en nuestro corpus.\n",
    "    self.punct = ['.', ',', ';', ':', '-', '^', '\"'\n",
    "                  '\"', '!', '¬°', '¬ø', '?', '<url>', '#', '@usuario']\n",
    "\n",
    "    # Tokens especiales\n",
    "    self.UNK = \"<unk>\"\n",
    "    self.SOS = \"<s>\"\n",
    "    self.EOS = \"</s>\"\n",
    "\n",
    "  def get_vocab_size(self) -> int:\n",
    "    \"\"\"\n",
    "    Devuelve el tama√±o del vocabulario.\n",
    "\n",
    "    Returns:\n",
    "        int: Tama√±o del vocabulario.\n",
    "    \"\"\"\n",
    "    return len(self.vocab)\n",
    "\n",
    "  def default_tokenizer(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizador por defecto. Simplemente separa cada oraci√≥n por espacios.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento a tokenizar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    return doc.split(\" \")\n",
    "\n",
    "  def remove_word(self, word: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica si la palabra en cuesti√≥n debe eliminarse seg√∫n los siguientes criterios:\n",
    "    - Es un signo de puntuaci√≥n\n",
    "    - Es un d√≠gito\n",
    "\n",
    "    Args:\n",
    "        word (str): Palabra a evaluar.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si se elimina.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    is_punct = True if word in self.punct else False\n",
    "    is_digit = word.isnumeric()\n",
    "    return is_punct or is_digit\n",
    "\n",
    "  def sortFreqDist(self, freq_dist: nltk.FreqDist) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con el top de palabras por frecuencia. El tama√±o de la lista es self.vocab_max.\n",
    "\n",
    "    Args:\n",
    "        freq_dist (nltk.FreqDist): Objeto de frecuencias (nltk) del corpus considerado.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tama√±o self.vocab_max.\n",
    "    \"\"\"\n",
    "    freq_dist = dict(freq_dist)\n",
    "    # Aqu√≠ key es una funci√≥n que se aplica a cada par√°metro\n",
    "    # antes de compararlo. En este caso se pasa\n",
    "    # freq_dist.get para asegurarse de que el ordenamiento\n",
    "    # se haga por las frecuencias y no por orden alfab√©tico.\n",
    "    return sorted(freq_dist,\n",
    "                  key=freq_dist.get,\n",
    "                  reverse=True)\n",
    "\n",
    "  def get_vocab(self, corpus: list[str]) -> set:\n",
    "    \"\"\"\n",
    "    Devuelve el vocabulario a partir de un corpus dado.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Corpus del cual se quiere obtener el vocabulario. Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        set: Vocabulario.\n",
    "    \"\"\"\n",
    "    freq_dist = FreqDist(\n",
    "      [w.lower()\n",
    "       for sentence in corpus\n",
    "       for w in self.tokenizer(sentence)\n",
    "       if not self.remove_word(w)]\n",
    "    )\n",
    "    sorted_words = self.sortFreqDist(freq_dist)[:self.vocab_max-3]\n",
    "    return set(sorted_words)\n",
    "\n",
    "  def fit(self, corpus: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Carga el vocabulario y crea diccionarios de √≠ndices <-> palabras. Adem√°s, si se aporta una matriz de embeddings pre-entrenados, tambi√©n construye la submatriz con los elementos del vocabulario.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "    \"\"\"\n",
    "    # Cargamos el vocabulario\n",
    "    self.vocab = self.get_vocab(corpus)\n",
    "    self.vocab.add(self.UNK)\n",
    "    self.vocab.add(self.SOS)\n",
    "    self.vocab.add(self.EOS)\n",
    "\n",
    "    # Diccionarios palabras <-> ids\n",
    "    self.w2id = dict()\n",
    "    self.id2w = dict()\n",
    "\n",
    "    if self.embeddings:\n",
    "      self.embeddings_matrix = np.empty([self.vocab_max,\n",
    "                                         self.embeddings.vector_size])\n",
    "\n",
    "    id = 0\n",
    "    for doc in corpus:\n",
    "      for word in self.tokenizer(doc):\n",
    "        word_ = word.lower()\n",
    "        if (word_ in self.vocab) and (not word_ in self.w2id):\n",
    "          self.w2id[word_] = id\n",
    "          self.id2w[id] = word_\n",
    "\n",
    "          # Si se aporta una matriz de embeddings,\n",
    "          # aqu√≠ se crea la submatriz.\n",
    "          if self.embeddings:\n",
    "            if word in self.embeddings:\n",
    "              self.embeddings_matrix[id] = self.embeddings[word_]\n",
    "            else:\n",
    "              self.embeddings_matrix[id] = np.random.rand(\n",
    "                self.embeddings.vector_size)\n",
    "\n",
    "          id += 1\n",
    "\n",
    "    # A√±adirmos los tokens especiales a los diccionarios.\n",
    "    self.w2id.update(\n",
    "      {self.UNK: id,\n",
    "       self.SOS: id + 1,\n",
    "       self.EOS: id + 2}\n",
    "    )\n",
    "    self.id2w.update(\n",
    "      {id: self.UNK,\n",
    "       id + 1: self.SOS,\n",
    "       id + 2: self.EOS}\n",
    "    )\n",
    "\n",
    "  def get_ngram_doc(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con n-gramas de un documento dado.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento del que se quieren obtener los n-gramas.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de n-gramas.\n",
    "    \"\"\"\n",
    "    doc_tokens = self.tokenizer(doc)\n",
    "    doc_tokens = self.replace_unk(doc_tokens)\n",
    "    doc_tokens = [w.lower()\n",
    "                  for w in doc_tokens]\n",
    "    doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "\n",
    "    return list(nltk.ngrams(doc_tokens, self.N))\n",
    "\n",
    "  def replace_unk(self, doc_tokens: list[str]) -> list:\n",
    "    \"\"\"\n",
    "    Toma un lista de tokens e intercambia los tokens out-of-vocabulary por el token especial self.UNK.\n",
    "\n",
    "    Args:\n",
    "        doc_tokens (list[str]): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens procesada.\n",
    "    \"\"\"\n",
    "    for i, token in enumerate(doc_tokens):\n",
    "      if token.lower() not in self.vocab:\n",
    "        doc_tokens[i] = self.UNK\n",
    "    return doc_tokens\n",
    "\n",
    "  def transform(self, corpus: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Devuelve una tupla de arreglos de Numpy. El primero tendr√° los ids de las palabras en el contexto, mientras que la segunda el id de la palabra que se debe predecir.\n",
    "\n",
    "    Se piensa en un modelo de CBOW. Damos el contexto y queremos predecir la palabra que sigue.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arreglos de numpy con ids de los contextos y id de la palabra objetivo.\n",
    "    \"\"\"\n",
    "    X_ngrams = list()\n",
    "    y = []\n",
    "\n",
    "    for doc in corpus:\n",
    "      doc_ngram = self.get_ngram_doc(doc)\n",
    "      for words_window in doc_ngram:\n",
    "        words_window_ids = [self.w2id[w]\n",
    "                            for w in words_window]\n",
    "        X_ngrams.append(list(words_window_ids[:-1]))\n",
    "        y.append(words_window_ids[-1])\n",
    "\n",
    "    return np.array(X_ngrams), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos nuestros datos usando la clase `NgramData`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize)\n",
    "ngram_data.fit(X_train)\n",
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del vocabulario : 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tama√±o del vocabulario : {ngram_data.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean los objetos `TensorDataset` para guardar los datos de entrenamiento y validaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "args.batch_size = 64\n",
    "\n",
    "# Num of workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          shuffle=True)\n",
    "\n",
    "# Val\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : torch.Size([64, 3])\n",
      "y shape : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape : {batch[0].shape}')\n",
    "print(f'y shape : {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4998, 4997,  236],\n",
       "        [  32,   26, 4997],\n",
       "        [   6,  219,  162],\n",
       "        [ 109,    6, 1151],\n",
       "        [4998, 4998, 4998],\n",
       "        [  48,  343,   45],\n",
       "        [  43,    8, 1316],\n",
       "        [4997, 4997,    6],\n",
       "        [4997, 1792,  226],\n",
       "        [4998, 4997,   48],\n",
       "        [   6,    8,  570],\n",
       "        [4998,  706, 4997],\n",
       "        [4997, 4997, 4997],\n",
       "        [3538, 4997,  166],\n",
       "        [4998, 4998, 4998],\n",
       "        [  14, 4997, 4997],\n",
       "        [ 114,    1, 3259],\n",
       "        [ 459,   51,  460],\n",
       "        [  45, 4997,   83],\n",
       "        [  48,  378,   48],\n",
       "        [ 952,   43, 2497],\n",
       "        [  60,   45, 2299],\n",
       "        [ 254,   48,  256],\n",
       "        [  55,   48,   20],\n",
       "        [ 423, 2459,   33],\n",
       "        [4998, 4998, 4998],\n",
       "        [4997, 4997,   66],\n",
       "        [ 941, 4997, 4997],\n",
       "        [  48,  167,  129],\n",
       "        [1587,  338,   16],\n",
       "        [ 114,   48,   46],\n",
       "        [ 990, 4997,  702],\n",
       "        [4998, 4998, 4998],\n",
       "        [ 112,  273, 4997],\n",
       "        [1550,  193,  711],\n",
       "        [ 106, 4997,   93],\n",
       "        [ 267,   16,  929],\n",
       "        [4998, 1262,   45],\n",
       "        [4997,   60, 4142],\n",
       "        [1587,  319, 2389],\n",
       "        [4997,   39, 4274],\n",
       "        [1240,   83, 4997],\n",
       "        [ 161, 4997,   34],\n",
       "        [4998,   48,    8],\n",
       "        [ 254,   26, 4467],\n",
       "        [4998, 4998,  873],\n",
       "        [  32,  125,  773],\n",
       "        [4997,   55,   26],\n",
       "        [  39, 2315, 1874],\n",
       "        [ 641, 4997,   32],\n",
       "        [  65, 2200,    6],\n",
       "        [4998, 4998,   39],\n",
       "        [3174, 4997, 4997],\n",
       "        [   6,    8, 1273],\n",
       "        [4998, 4998, 4998],\n",
       "        [ 319,  505, 4997],\n",
       "        [2131,  129,  162],\n",
       "        [1571,   60, 1247],\n",
       "        [4998,  253, 4997],\n",
       "        [   6,  117,   50],\n",
       "        [ 129, 4997,   32],\n",
       "        [ 621,  621,   81],\n",
       "        [4997,  245,   60],\n",
       "        [4998,  424,  198]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de lenguaje neuronal de Bengio c:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLanguageModel(nn.Module):\n",
    "  \"\"\"\n",
    "  Red neuronal de Bengio :)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, args):\n",
    "    \"\"\"\n",
    "    Constructor  de la clase.\n",
    "\n",
    "    El modelo de red neuronal par lenguaje de Bengio tiene la siguiente estructura:\n",
    "    Para un modelo de n-gramas, se dan las primeras n-1 palabras como contexto y se intenta predecir la n-√©sima palabra.\n",
    "    (1) n-1 representaciones iniciales: suelen ser one-hot. Pero aqu√≠ se toman de NgramData.\n",
    "        x\n",
    "    (2) n-1 representaciones aprendidas de tama√±o m: se obtienen de manera individual (por palabra). \n",
    "        (x = Cx)\n",
    "        En esta implementaci√≥n C se inicia de manera aleatoria.\n",
    "    (3) Capa oculta de tama√±o h: se mezclan las n-1 representaciones del paso anterior y se aplica tanh. \n",
    "        (h = tanh(Hx + d))\n",
    "        Nosotros vamos a usar ReLu en vez de tanh.\n",
    "    (4) Capa de salida de tama√±o m: se aplica softmax a la salida de la capa anterior.\n",
    "        (y = softmax(Uh + b))\n",
    "        Nosotros no vamos a aplicar softmax aqu√≠, sino afuerita.\n",
    "\n",
    "    Args:\n",
    "        args (Any): Diccionario de variables.\n",
    "    \"\"\"\n",
    "    super(NeuralLanguageModel, self).__init__()\n",
    "\n",
    "    self.window_size = args.N - 1  # Las n-1 palabras que entran (el contexto).\n",
    "    self.embedding_size = args.m  # Tama√±o de las representaciones.\n",
    "\n",
    "    # Matriz C para convertir las representaciones. Pero est√° chido porque sus entradas son \"entrenables\".\n",
    "    self.emb = nn.Embedding(args.vocab_size, args.m)\n",
    "    # Primera capa oculta de las representaciones aprendidas a la oculta.\n",
    "    self.fc1 = nn.Linear(args.m * (args.N - 1), args.d_h)\n",
    "    # Un dropout para alocarnos\n",
    "    self.drop1 = nn.Dropout(p=args.dropout)\n",
    "    # Aqu√≠ solamente se va a hacer el producto por la matriz U.\n",
    "    # La softmax se va a aplicar por fuera de la red para obtener la siguiente palabra seg√∫n la red.\n",
    "    self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Aqu√≠ se cambia la representaci√≥n inicial por la aprendida.\n",
    "    # Es un producto matricial. Aqu√≠ las representaciones siguen siendo matrices.\n",
    "    x = self.emb(x)\n",
    "    # Se cambia el tama√±o para que se considere como una sola capa.\n",
    "    x = x.view(-1, self.window_size * self.embedding_size)\n",
    "    # Aqu√≠ se hace relu(Hx + d)\n",
    "    h = F.relu(self.fc1(x))  # relu(z) = max{0, z}\n",
    "    # El dropout para alocarnoooos wuuuuuuuu\n",
    "    h = self.drop1(h)\n",
    "\n",
    "    # Devolvemos solamente (Uh + b)\n",
    "    return self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  Aqu√≠ se toma la salida de la red neuronal (las neuronas de la √∫ltima capa oculta).\n",
    "  Uh + b\n",
    "  Se les aplica la softmax\n",
    "  softmax(Uh + b)\n",
    "  Y luego se devuelve el √≠ndice de la neurona de mayor valor.\n",
    "\n",
    "  Args:\n",
    "      raw_logits (torch.Tensor | float): La salida de la red (Uh + b)\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor | int: √çndice de la neurona con mayor valor despu√©s de softmax.\n",
    "  \"\"\"\n",
    "  # Se aplica softmax.\n",
    "  probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "  # Se obtiene el √≠ndice del valor m√°ximo.\n",
    "  y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data: torch.Tensor,\n",
    "               model,\n",
    "               gpu: bool = False) -> float | int:\n",
    "  \"\"\"\n",
    "  Eval√∫a el desempe√±o del modelo sobre un conjunto de validaci√≥n.\n",
    "\n",
    "  Args:\n",
    "      data (torch.Tensor): Conjunto de validaci√≥n sobre el que se va a evaluar el modelo.\n",
    "      model (_type_): Modelo que se va a evaluar.\n",
    "      gpu (bool, optional): ¬øUsamos gpu? S√≠/No. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "      float | int: Puntaje de accuracy sobre todo el conjunto de validaci√≥n.\n",
    "  \"\"\"\n",
    "  with torch.no_grad():\n",
    "    preds, tgts = list(), list()\n",
    "\n",
    "    for window_words, labels in data:\n",
    "      # windoe_words es una matriz de (64, 3)\n",
    "      # labels es un vector de (64)\n",
    "      if gpu:\n",
    "        window_words = window_words.cuda()\n",
    "\n",
    "      outputs = model(window_words)\n",
    "\n",
    "      # Obrenemos las predicciones\n",
    "      y_pred = get_preds(outputs)  # (64)\n",
    "\n",
    "      tgt = labels.numpy()  # Lo que esper√°bamos ver (64)\n",
    "      tgts.append(tgt)  # Se a√±ade a tgts (Lista de respuestas)\n",
    "      preds.append(y_pred)  # Lo que vimos (Lista de predicciones)\n",
    "\n",
    "  # Aqu√≠ abajo \"desempaquetamos\" los targets y las predicciones\n",
    "  # e : element\n",
    "  # l : list\n",
    "  # lista de todos los target\n",
    "  tgts = [e for l in tgts for e in l]\n",
    "  # lista de todas las predicciones\n",
    "  preds = [e for l in preds for e in l]\n",
    "\n",
    "  return accuracy_score(tgts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state: dict,\n",
    "                    is_best: bool,\n",
    "                    checkpoint_path: str,\n",
    "                    filename: str = \"checkpoint.pt\"):\n",
    "  \"\"\"\n",
    "  Guarda el modelo si se ve una mejora cra. a iteraciones anteriores.\n",
    "\n",
    "  Args:\n",
    "      state (dict): Informaci√≥n que queremos guardar sobre el modelo.\n",
    "        DEBE incluir model.state_dict()\n",
    "      is_best (bool): ¬øEsta versi√≥n del modelo es mejor?\n",
    "      checkpoint_path (str): Directorio en el que se va a guardar el modelo.\n",
    "      filename (str, optional): _description_. Defaults to \"checkpoint.pt\".\n",
    "  \"\"\"\n",
    "  # Armamos la ruta para el archivo que va a guardar el modelo.\n",
    "  filename = os.path.join(checkpoint_path, filename)\n",
    "  # Aqu√≠ se guarda.\n",
    "  torch.save(state, filename)\n",
    "\n",
    "  # Si adem√°s es el very besto modelo ever, le ponemos un nombre que le cheque :))\n",
    "  if is_best:\n",
    "    shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperpar√°metros de la red c:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.m = 100  # Dimensi√≥n de los embeddings de palabras\n",
    "args.d_h = 200  # Dimensi√≥n de la capa oculta\n",
    "args.dropout = 0.1\n",
    "\n",
    "# Entrenamiento\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Guardado de los modelos\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El scheduler sirve para\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancho/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo\n",
    "model = NeuralLanguageModel(args)\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "  model.cuda()\n",
    "\n",
    "# Perdida, optimizaci√≥n y scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "  optimizer,\n",
    "  \"min\",\n",
    "  patience=args.lr_patience,\n",
    "  verbose=True,\n",
    "  factor=args.lr_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9802c3e8704d47878ad046e7dee1b299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.17334786636805527\n",
      "Epoch [1/100], Loss 5.5255 - Val accuracy 0.2106 - Epoch time : 3.9712634086608887\n",
      "Train acc: 0.18161700829550473\n",
      "Epoch [2/100], Loss 5.0834 - Val accuracy 0.2051 - Epoch time : 3.347290277481079\n",
      "Train acc: 0.18968497670027717\n",
      "Epoch [3/100], Loss 4.8710 - Val accuracy 0.2247 - Epoch time : 3.378157377243042\n",
      "Train acc: 0.19355529164825452\n",
      "Epoch [4/100], Loss 4.7027 - Val accuracy 0.1987 - Epoch time : 3.3027946949005127\n",
      "Train acc: 0.19839004690073514\n",
      "Epoch [5/100], Loss 4.5524 - Val accuracy 0.1486 - Epoch time : 3.3078694343566895\n",
      "Train acc: 0.20009139115413974\n",
      "Epoch [6/100], Loss 4.4190 - Val accuracy 0.1334 - Epoch time : 3.4088571071624756\n",
      "Train acc: 0.20137689310247858\n",
      "Epoch [7/100], Loss 4.2941 - Val accuracy 0.2161 - Epoch time : 3.3657355308532715\n",
      "Train acc: 0.2048878575302294\n",
      "Epoch [8/100], Loss 4.1818 - Val accuracy 0.2211 - Epoch time : 3.3464605808258057\n",
      "Train acc: 0.2091693071345358\n",
      "Epoch [9/100], Loss 4.0697 - Val accuracy 0.1340 - Epoch time : 3.3119735717773438\n",
      "Train acc: 0.2104949810187603\n",
      "Epoch [10/100], Loss 3.9740 - Val accuracy 0.2160 - Epoch time : 3.363835334777832\n",
      "Train acc: 0.21539595719680232\n",
      "Epoch [11/100], Loss 3.8740 - Val accuracy 0.1487 - Epoch time : 3.419468879699707\n",
      "Train acc: 0.22095380729522354\n",
      "Epoch [12/100], Loss 3.7903 - Val accuracy 0.2251 - Epoch time : 3.45827054977417\n",
      "Train acc: 0.2292515089583417\n",
      "Epoch [13/100], Loss 3.6955 - Val accuracy 0.1953 - Epoch time : 3.3866078853607178\n",
      "Train acc: 0.23739354436990318\n",
      "Epoch [14/100], Loss 3.6198 - Val accuracy 0.1262 - Epoch time : 3.4248008728027344\n",
      "Train acc: 0.24653234594062587\n",
      "Epoch [15/100], Loss 3.5449 - Val accuracy 0.1079 - Epoch time : 3.327522039413452\n",
      "Train acc: 0.256464543245089\n",
      "Epoch [16/100], Loss 3.4748 - Val accuracy 0.2242 - Epoch time : 3.31701922416687\n",
      "Train acc: 0.26318926254368696\n",
      "Epoch [17/100], Loss 3.4234 - Val accuracy 0.1901 - Epoch time : 3.3023221492767334\n",
      "Train acc: 0.27227439691881256\n",
      "Epoch [18/100], Loss 3.3645 - Val accuracy 0.2064 - Epoch time : 3.264073133468628\n",
      "Train acc: 0.27788748342907643\n",
      "Epoch [19/100], Loss 3.3179 - Val accuracy 0.1408 - Epoch time : 3.2712292671203613\n",
      "Train acc: 0.2854853146466878\n",
      "Epoch [20/100], Loss 3.2681 - Val accuracy 0.1448 - Epoch time : 3.2411653995513916\n",
      "Train acc: 0.2910736075402724\n",
      "Epoch [21/100], Loss 3.2280 - Val accuracy 0.1524 - Epoch time : 3.2987987995147705\n",
      "Train acc: 0.2986513527899409\n",
      "Epoch [22/100], Loss 3.1860 - Val accuracy 0.1776 - Epoch time : 3.330838918685913\n",
      "Train acc: 0.30150261870807055\n",
      "Epoch [23/100], Loss 3.1516 - Val accuracy 0.1434 - Epoch time : 3.2651684284210205\n",
      "Train acc: 0.3079414268067328\n",
      "Epoch [24/100], Loss 3.1182 - Val accuracy 0.1564 - Epoch time : 3.350417137145996\n",
      "Train acc: 0.31284679679026234\n",
      "Epoch [25/100], Loss 3.0828 - Val accuracy 0.1957 - Epoch time : 3.3036201000213623\n",
      "Train acc: 0.31599778301128834\n",
      "Epoch [26/100], Loss 3.0537 - Val accuracy 0.1842 - Epoch time : 3.3246536254882812\n",
      "Train acc: 0.3664458884023621\n",
      "Epoch [27/100], Loss 2.7578 - Val accuracy 0.2071 - Epoch time : 3.3745269775390625\n",
      "Train acc: 0.3707857128510023\n",
      "Epoch [28/100], Loss 2.7169 - Val accuracy 0.2065 - Epoch time : 3.3068623542785645\n",
      "Train acc: 0.37449063240670066\n",
      "Epoch [29/100], Loss 2.6967 - Val accuracy 0.1776 - Epoch time : 3.3630824089050293\n",
      "Train acc: 0.37544534357048165\n",
      "Epoch [30/100], Loss 2.6826 - Val accuracy 0.1858 - Epoch time : 3.286593198776245\n",
      "Train acc: 0.37746900484071827\n",
      "Epoch [31/100], Loss 2.6717 - Val accuracy 0.1660 - Epoch time : 3.3044257164001465\n",
      "No improvement. Breaking out of loop.\n",
      "--- 107.49855828285217 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in tqdm(range(args.num_epochs),\n",
    "                  desc=\"Epochs\"):\n",
    "  epoch_start_time = time.time()\n",
    "  loss_epoch = []\n",
    "  training_metric = []\n",
    "  model.train()\n",
    "\n",
    "  for window_words, labels in train_loader:\n",
    "\n",
    "    # Si hay GPU\n",
    "    if args.use_gpu:\n",
    "      window_words = window_words.cuda()\n",
    "      labels = labels.cuda()\n",
    "\n",
    "    # Forward\n",
    "    outputs = model(window_words)\n",
    "    loss = criterion(outputs,\n",
    "                     labels)\n",
    "    loss_epoch.append(loss.item())\n",
    "\n",
    "    # Obtener m√©tricas de entrenamiento\n",
    "    y_pred = get_preds(outputs)\n",
    "    tgt = labels.cpu().numpy()\n",
    "    training_metric.append(accuracy_score(tgt,\n",
    "                                          y_pred))\n",
    "\n",
    "    # Backprop y optimizamos\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Guardamos la m√©trica por √©poca\n",
    "  mean_epoch_metric = np.mean(training_metric)\n",
    "  train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Validaci√≥n para esta √©poca\n",
    "  model.eval()\n",
    "  tuning_metric = model_eval(val_loader,\n",
    "                             model,\n",
    "                             gpu=args.use_gpu)\n",
    "  metric_history.append(mean_epoch_metric)\n",
    "\n",
    "  # Scheduler\n",
    "  scheduler.step(tuning_metric)\n",
    "\n",
    "  # Revisa si la m√©trica mejor√≥\n",
    "  is_improvement = tuning_metric > best_metric\n",
    "  if is_improvement:\n",
    "    best_metric = tuning_metric\n",
    "    n_no_improve = 0\n",
    "  else:\n",
    "    n_no_improve += 1\n",
    "\n",
    "  # Si la m√©trica mejora, guarda el mejor modelo\n",
    "  save_checkpoint(\n",
    "    {\n",
    "      \"epoch\": epoch + 1,\n",
    "      \"state_dict\": model.state_dict(),\n",
    "      \"optimizer\": optimizer.state_dict(),\n",
    "      \"scheduler\": scheduler.state_dict(),\n",
    "      \"best_metric\": best_metric\n",
    "    },\n",
    "    is_improvement,\n",
    "    args.savedir\n",
    "  )\n",
    "\n",
    "  # Parada temprana por paciencie\n",
    "  if n_no_improve >= args.patience:\n",
    "    print(\"No improvement. Breaking out of loop.\")\n",
    "    break\n",
    "\n",
    "  print(f\"Train acc: {mean_epoch_metric}\")\n",
    "  print(f\"Epoch [{epoch + 1}/{args.num_epochs}], Loss {np.mean(loss_epoch):.4f} - Val accuracy {tuning_metric:.4f} - Epoch time : {time.time() - epoch_start_time}\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    '''Devuelve la lista de las n palabras mas cercanas a word'''\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]]) # obtener id de las palabras\n",
    "    word_embed = embeddings(word_id) # obtener el embedding de la palabra\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim=1).detach() # calcular distancias a todas las palabras\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # ordenar por distancia\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx], difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<unk> 10.232197\n",
      "castiguen 10.771348\n",
      "dure 10.835211\n",
      "indios 10.851449\n",
      "pones 10.926342\n",
      "odian 10.955638\n",
      "mueres 11.088757\n",
      "quedado 11.160634\n",
      "estuve 11.17065\n",
      "holanda 11.220155\n"
     ]
    }
   ],
   "source": [
    "best_model = NeuralLanguageModel(args)\n",
    "state_dict = torch.load(\"model/model_best.pt\", weights_only=False)\n",
    "best_model.load_state_dict(state_dict[\"state_dict\"])\n",
    "best_model.train(False)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"perro\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    '''Devuelve el texto tokenizado y los ids de las palabras'''\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer.tokenize(text)]\n",
    "    token_ids = [ngram_data.w2id[w.lower()] for w in all_tokens]\n",
    "    return all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits, temperature=1.0):\n",
    "    '''\n",
    "    Dados los logits y la temperatura \n",
    "    (un parametro de diversidad que indica cuan determinista sera el modelo), \n",
    "    devuelve la siguiente palabra\n",
    "    '''\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, token_ids):\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "\n",
    "    # no es necesario aplicar softmax (de aplicarse daria el mismo resultado que la prediccion cruda)\n",
    "    #y_probs = F.softmax(y_raw_pred, dim=1)\n",
    "    #y_pred = torch.argmax(y_probs, dim=1).detach().numpy()\n",
    "\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "\n",
    "    for i in range(100):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "\n",
    "    return \" \".join(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo genera oraciones con partes coherentes, pero esto ayudara mas para calcular la probabilidad de ocurrencia de oraciones dadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> <unk> estoy <unk> <unk> de <unk> han <unk> hijos de portada <unk> quien chingados <unk> son drogadicto <unk> <unk> te llega pero narizon <unk> para <unk> en <unk> <unk> el ancho equivoca <unk> a <unk> <unk> <unk> cu√°l tipo <unk> <unk> <unk> antessss pero <unk> loca <unk> no tiene ni madres <unk> ni <unk> <unk> putos <unk> tantita madre <unk> <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> estoy hasta un sue√±o que cagado se a√±os ejercicio el <unk> ya se <unk> m√°s que los lea lentes les encanta <unk> <unk> qu√© me pones mucho <unk> porfa üòå que soy feliz ni loca <unk> yo sigo </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> estoy'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> saludos a cruda ahi <unk> lleg√≥ han por eso todo el mundo para meter m√°s dj te <unk> a√∫n <unk> <unk> <unk> fundadora </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> saludos a'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "yo opino que despertar <unk> d yo dinero y el <unk> cancelar de <unk> <unk> madre <unk> una vez le ense√±a caso <unk> <unk> <unk> se <unk> üòç <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = 'yo opino que'\n",
    "print('-'*30)\n",
    "print(\"Learned embeddings\")\n",
    "print('-'*30)\n",
    "print(generate_text(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, test, ngram_model):\n",
    "    # Generar n gram windows from input text and the respective label y\n",
    "    X, y = ngram_model.transform(test)\n",
    "    # discard first two n-gram windows since they may contain <s> tokens (not necessary)\n",
    "    X, y = X[2:], y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "\n",
    "    return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa como cambian los valores para oraciones menos probables de observar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -771.2288104891777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -859.9079782366753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos procesamiento clase en la de natural lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -916.2573439478874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \" la natural Estamos clase en de de lenguaje procesamiento\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructuras sintacticas correctas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10181/971388848.py:11: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.log(probs[i][w]) for i, w in enumerate(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-448.9200522303581 me voy sino la a gano chingada\n",
      "-448.9200522303581 me voy sino a la gano chingada\n",
      "-448.9200522303581 me voy sino a gano la chingada\n",
      "-448.9200522303581 me voy sino a gano chingada la\n",
      "-448.9200522303581 me voy la sino a gano chingada\n",
      "------------------------------\n",
      "-453.9536334872246 chingada gano me la a sino voy\n",
      "-453.9536334872246 chingada gano me a voy la sino\n",
      "-453.9536334872246 chingada gano me a sino la voy\n",
      "-453.9536334872246 chingada gano me a la voy sino\n",
      "-453.9536334872246 chingada gano me a la sino voy\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [' '.join(p) for p in permutations(word_list)]\n",
    "#print(perms)\n",
    "print('-'*30)\n",
    "\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[:5]:\n",
    "    print(p, t)\n",
    "\n",
    "print('-'*30)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
