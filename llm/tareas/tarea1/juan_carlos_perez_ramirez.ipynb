{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d6a487",
   "metadata": {},
   "source": [
    "# Nombre: Juan Carlos Perez Ramirez\n",
    "# T1: Attention Recurrent Language Model\n",
    "## Language Modelling\n",
    "### Vectorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200b14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # activa trazas s√≠ncronas\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from math import exp, sqrt, log\n",
    "\n",
    "stop = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d21961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para leer datos\n",
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for tweet in f_corpus:\n",
    "            tr_txt += [tweet]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label]\n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokeniza y limpia el texto\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "def tweet_tokenizer(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\d+([.,]d+)*\", \"<num>\", text)\n",
    "    text = re.sub(r'[\"();:‚Ä¶]', \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22afec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab, max_len=50):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    '''Sustituye cada token por su id en el vocabulario. Si el token no est√° en el vocabulario,\n",
    "    se sustituye por la id de <unk>. Adem√°s, se limita la longitud m√°xima de la secuencia a max_len.'''\n",
    "    def encode(self, tweet):\n",
    "        tokens = tweet_tokenizer(tweet)[:self.max_len]\n",
    "        tokens = [\"<s>\"] + tokens[:self.max_len-2] + [\"</s>\"]\n",
    "        ids = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        return ids\n",
    "    '''Tamano del dataset'''\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    '''Devuelve los ids y la etiqueta de un tweet dado su √≠ndice'''\n",
    "    def __getitem__(self, i):\n",
    "        ids = self.encode(self.tweets[i])\n",
    "        return ids\n",
    "\n",
    "'''Realiza el padding para construir el batch para el DataLoader.'''\n",
    "def collate(batch, pad_id=0):\n",
    "    lens = torch.tensor([len(x) for x in batch], dtype=torch.long)\n",
    "    T = lens.max().item()\n",
    "    B = len(batch)\n",
    "    padded = torch.full((B, T), pad_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(batch):\n",
    "        padded[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "    return padded, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecanismo de atenci√≥n a la manera de transformer\n",
    "class selfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.h = n_heads\n",
    "        self.dh = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n",
    "        self.o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, X, pad_mask=None):\n",
    "        B, T, D = X.shape\n",
    "        X = self.ln(X)\n",
    "        q, k, v = self.qkv(X).chunk(3, dim=-1)\n",
    "        def split(x): return x.view(B, T, self.h, self.dh).transpose(1, 2)\n",
    "        Q, K, V = map(split, (q, k, v))\n",
    "\n",
    "        attention = (Q @ K.transpose(-2, -1)) / sqrt(self.dh)\n",
    "        if pad_mask is not None:\n",
    "            attention = attention.masked_fill(pad_mask[:, None, None, :], float('-inf'))\n",
    "        causal = torch.triu(torch.ones(T, T, dtype=torch.bool, device=X.device), diagonal=1)\n",
    "        attention = attention.masked_fill(causal, float('-inf'))\n",
    "\n",
    "        A = torch.softmax(attention, dim=-1)\n",
    "        A = self.drop(A)\n",
    "        Y = (A @ V).transpose(1, 2).contiguous().view(B, T, D)\n",
    "\n",
    "        return self.o(Y)\n",
    "\n",
    "\n",
    "# arquitectura de rnn con self-attention\n",
    "class LSTMSelfAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=128, num_layers=1, num_heads=2, pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        # definicion de la arquitectura\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        d_model = hidden_dim\n",
    "        self.attention = selfAttention(d_model, n_heads=num_heads, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.head.weight = self.embedding.weight\n",
    "\n",
    "    # definicion del metodo forward\n",
    "    def forward(self, x, lens):\n",
    "        pad_mask = (x == self.pad_id)\n",
    "        X = self.embedding(x)\n",
    "        if lens is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                X, lens.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            packed_out, _ = self.lstm(packed)\n",
    "            H, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "                packed_out, batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            H, _ = self.lstm(X)\n",
    "\n",
    "        H = self.attention(H, pad_mask=pad_mask)\n",
    "        H = self.dropout(H)\n",
    "\n",
    "        logits = self.head(H)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42a8e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funci√≥n de p√©rdida para entrenamiento\n",
    "def loss(logits, y, ignore_pad_id=0):\n",
    "    logits = logits[:, :-1, :].contiguous()\n",
    "    y = y[:, 1:].contiguous()\n",
    "    B, T, V = logits.shape\n",
    "    loss_sum = F.cross_entropy(\n",
    "        logits.view(B*T, V),\n",
    "        y.view(B*T),\n",
    "        ignore_index=ignore_pad_id,\n",
    "        reduction='sum'\n",
    "    )\n",
    "    valid = (y != ignore_pad_id).sum().clamp(min=1)  # tensor\n",
    "    return loss_sum, valid\n",
    "\n",
    "# funci√≥n para calcular la perplexidad del modelo\n",
    "@torch.no_grad()\n",
    "def perplexity_corpus(model, loader, pad_id=0, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_loss_sum = 0.0\n",
    "    total_valid = 0\n",
    "    for x, lens in loader:\n",
    "        x, lens = x.to(device), lens.to(device)\n",
    "        logits = model(x, lens)\n",
    "        loss_sum, valid = loss(logits, x, ignore_pad_id=pad_id)\n",
    "        total_loss_sum += float(loss_sum)\n",
    "        total_valid    += int(valid.item())\n",
    "    return exp(total_loss_sum / max(total_valid, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c018fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c48b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"../../corpus/mex20_train.txt\", \"../../corpus/mex20_train_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b2e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 18\n"
     ]
    }
   ],
   "source": [
    "# obtiene la longitud m√°xima de las secuencias\n",
    "lengths = [len(tweet_tokenizer(s)) for s in tr_txt]\n",
    "import numpy as np\n",
    "max_len = int(np.percentile(lengths, 95))  # p.ej. 95¬∫ percentil\n",
    "print(f\"max_len: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342008a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construye el vocabulario\n",
    "fdist = FreqDist(\n",
    "    tok\n",
    "    for doc in tr_txt\n",
    "    for tok in tweet_tokenizer(doc)\n",
    ")\n",
    "V = {\"<pad>\": 0, \"<unk>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
    "for i, (val, freq) in enumerate(fdist.most_common(10000-4)):\n",
    "    if val not in V:\n",
    "        V[val] = i + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22700106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " '<s>': 2,\n",
       " '</s>': 3,\n",
       " ',': 4,\n",
       " '.': 5,\n",
       " '!': 6,\n",
       " '@usuario': 7,\n",
       " 'verga': 8,\n",
       " 'madre': 9,\n",
       " '?': 10,\n",
       " '<num>': 11,\n",
       " 'putos': 12,\n",
       " 'si': 13,\n",
       " 'putas': 14,\n",
       " '...': 15,\n",
       " 'üòÇ': 16,\n",
       " 'loca': 17,\n",
       " 'gorda': 18,\n",
       " 'bien': 19,\n",
       " '¬ø': 20,\n",
       " 'feas': 21,\n",
       " 'pinche': 22,\n",
       " 'puta': 23,\n",
       " 'ser': 24,\n",
       " 'puto': 25,\n",
       " 'hdp': 26,\n",
       " 'tan': 27,\n",
       " 'as√≠': 28,\n",
       " 'mamar': 29,\n",
       " 'q': 30,\n",
       " 'quiero': 31,\n",
       " 'solo': 32,\n",
       " '¬°': 33,\n",
       " '<url>': 34,\n",
       " 'joto': 35,\n",
       " 'cosas': 36,\n",
       " 'marica': 37,\n",
       " 'siempre': 38,\n",
       " 'ahora': 39,\n",
       " 'hace': 40,\n",
       " 'ver': 41,\n",
       " 'vale': 42,\n",
       " 'mejor': 43,\n",
       " 'hoy': 44,\n",
       " 'gente': 45,\n",
       " 'vida': 46,\n",
       " 'Ô∏è': 47,\n",
       " 'va': 48,\n",
       " 'voy': 49,\n",
       " 'pinches': 50,\n",
       " 'd√≠a': 51,\n",
       " 'jajaja': 52,\n",
       " 'vez': 53,\n",
       " 'mierda': 54,\n",
       " '-': 55,\n",
       " 'luchona': 56,\n",
       " 'üò≠': 57,\n",
       " 'pues': 58,\n",
       " 'hijo': 59,\n",
       " 'üòç': 60,\n",
       " 'jajajaja': 61,\n",
       " 'alguien': 62,\n",
       " 'tontas': 63,\n",
       " 'hacer': 64,\n",
       " 'üò°': 65,\n",
       " '‚Äú': 66,\n",
       " 'toda': 67,\n",
       " 'cagado': 68,\n",
       " 'mas': 69,\n",
       " 'gusta': 70,\n",
       " '‚Äù': 71,\n",
       " 'pendejo': 72,\n",
       " 'tonta': 73,\n",
       " 'mamando': 74,\n",
       " 'puedo': 75,\n",
       " 'hijos': 76,\n",
       " 'üôÑ': 77,\n",
       " 'mal': 78,\n",
       " 'nunca': 79,\n",
       " '..': 80,\n",
       " 'madres': 81,\n",
       " 'chingada': 82,\n",
       " 'maricon': 83,\n",
       " 'mujer': 84,\n",
       " 'cada': 85,\n",
       " 'cabrona': 86,\n",
       " 'tener': 87,\n",
       " 'van': 88,\n",
       " 'dos': 89,\n",
       " 'verdad': 90,\n",
       " 'da': 91,\n",
       " 'puede': 92,\n",
       " 'neta': 93,\n",
       " 'mujeres': 94,\n",
       " 'todas': 95,\n",
       " 'wey': 96,\n",
       " 'vas': 97,\n",
       " 'decir': 98,\n",
       " 's√©': 99,\n",
       " \"'\": 100,\n",
       " 'mam√°': 101,\n",
       " 'ma√±ana': 102,\n",
       " 'v': 103,\n",
       " 'd': 104,\n",
       " 'ir': 105,\n",
       " 'alv': 106,\n",
       " 'veces': 107,\n",
       " 'm√©xico': 108,\n",
       " 'menos': 109,\n",
       " 'a√±os': 110,\n",
       " '‚ù§': 111,\n",
       " '/': 112,\n",
       " 'quiere': 113,\n",
       " 'd√≠as': 114,\n",
       " 'bueno': 115,\n",
       " 'amor': 116,\n",
       " 'igual': 117,\n",
       " 'nadie': 118,\n",
       " 'mundo': 119,\n",
       " 'pendeja': 120,\n",
       " 'siento': 121,\n",
       " 'cuenta': 122,\n",
       " 'digo': 123,\n",
       " 'dice': 124,\n",
       " 'ü§î': 125,\n",
       " 'ganas': 126,\n",
       " 'aqu√≠': 127,\n",
       " 'ah√≠': 128,\n",
       " 'dicen': 129,\n",
       " 'creo': 130,\n",
       " 'personas': 131,\n",
       " 'tiempo': 132,\n",
       " 'gracias': 133,\n",
       " 'üòí': 134,\n",
       " 'amigos': 135,\n",
       " 's√≥lo': 136,\n",
       " 'veo': 137,\n",
       " 'camote': 138,\n",
       " 'jaja': 139,\n",
       " 'pedo': 140,\n",
       " 'cabron': 141,\n",
       " 'fotos': 142,\n",
       " 'pasa': 143,\n",
       " 'mundial': 144,\n",
       " 'pendejos': 145,\n",
       " 'sabe': 146,\n",
       " 'hacen': 147,\n",
       " 'ojal√°': 148,\n",
       " 'ardida': 149,\n",
       " 'cabr√≥n': 150,\n",
       " 'buena': 151,\n",
       " 'luego': 152,\n",
       " 'c√≥mo': 153,\n",
       " 'mil': 154,\n",
       " 'unas': 155,\n",
       " 'amo': 156,\n",
       " 'mismo': 157,\n",
       " 'hombres': 158,\n",
       " 'culo': 159,\n",
       " 'sabes': 160,\n",
       " 'chingar': 161,\n",
       " 've': 162,\n",
       " 'foto': 163,\n",
       " 'ustedes': 164,\n",
       " 'jajajajaja': 165,\n",
       " '&': 166,\n",
       " 'trabajo': 167,\n",
       " 'casa': 168,\n",
       " 'qui√©n': 169,\n",
       " 'valer': 170,\n",
       " 'deja': 171,\n",
       " 'fin': 172,\n",
       " '*': 173,\n",
       " 'quieren': 174,\n",
       " 'mientras': 175,\n",
       " 'üòà': 176,\n",
       " 'mames': 177,\n",
       " 'chinga': 178,\n",
       " 's√∫per': 179,\n",
       " 'chingas': 180,\n",
       " 'noche': 181,\n",
       " 'cara': 182,\n",
       " 'buen': 183,\n",
       " 'amigo': 184,\n",
       " 'pueden': 185,\n",
       " 'ü§£': 186,\n",
       " 'rico': 187,\n",
       " 'perra': 188,\n",
       " 'gata': 189,\n",
       " 'semana': 190,\n",
       " 'encanta': 191,\n",
       " 'dan': 192,\n",
       " 'vamos': 193,\n",
       " 'prieta': 194,\n",
       " 'despu√©s': 195,\n",
       " 'üò©': 196,\n",
       " 'üé∂': 197,\n",
       " 'ven': 198,\n",
       " 'gay': 199,\n",
       " 'chingo': 200,\n",
       " 'a√∫n': 201,\n",
       " 'xd': 202,\n",
       " 'digan': 203,\n",
       " 'hablar': 204,\n",
       " 'caga': 205,\n",
       " 'dar': 206,\n",
       " 'dinero': 207,\n",
       " 'putita': 208,\n",
       " 'odio': 209,\n",
       " 'amiga': 210,\n",
       " 'ando': 211,\n",
       " 'persona': 212,\n",
       " 'creen': 213,\n",
       " 'dijo': 214,\n",
       " 'tal': 215,\n",
       " 'horas': 216,\n",
       " 'asi': 217,\n",
       " 'peor': 218,\n",
       " 'dejen': 219,\n",
       " 'mamen': 220,\n",
       " 'andar': 221,\n",
       " 'triste': 222,\n",
       " 'saben': 223,\n",
       " 'entiendo': 224,\n",
       " 'novio': 225,\n",
       " 'rt': 226,\n",
       " 'rica': 227,\n",
       " 'boca': 228,\n",
       " 'hora': 229,\n",
       " 'falta': 230,\n",
       " 'hija': 231,\n",
       " 'dejar': 232,\n",
       " 'maric√≥n': 233,\n",
       " 'üò†': 234,\n",
       " 'ah': 235,\n",
       " 'asco': 236,\n",
       " 're': 237,\n",
       " 'iba': 238,\n",
       " 'haciendo': 239,\n",
       " 'novia': 240,\n",
       " 'pa√≠s': 241,\n",
       " 'puedes': 242,\n",
       " 'pobre': 243,\n",
       " 'pelan': 244,\n",
       " 'vali√≥': 245,\n",
       " 'sigue': 246,\n",
       " 'twitter': 247,\n",
       " 'quieres': 248,\n",
       " 'dormir': 249,\n",
       " 'dios': 250,\n",
       " 'x': 251,\n",
       " 'puro': 252,\n",
       " 'viendo': 253,\n",
       " 'gusto': 254,\n",
       " 'seguro': 255,\n",
       " 'üá≤üáΩ': 256,\n",
       " 'hombre': 257,\n",
       " 'feliz': 258,\n",
       " 'palabra': 259,\n",
       " 'cosa': 260,\n",
       " 'se√±ora': 261,\n",
       " 'entonces': 262,\n",
       " 'mamadas': 263,\n",
       " '#masterchefmx': 264,\n",
       " 'tarea': 265,\n",
       " 'poca': 266,\n",
       " 'mira': 267,\n",
       " 'nalgas': 268,\n",
       " 'nuevo': 269,\n",
       " 'saber': 270,\n",
       " 'golfa': 271,\n",
       " 'üòò': 272,\n",
       " 'aunque': 273,\n",
       " 'üò§': 274,\n",
       " 'perro': 275,\n",
       " 'mandar': 276,\n",
       " 'hago': 277,\n",
       " 'valiendo': 278,\n",
       " 'chinguen': 279,\n",
       " 'pone': 280,\n",
       " 'mama': 281,\n",
       " 'vieja': 282,\n",
       " 'importa': 283,\n",
       " 'vergas': 284,\n",
       " 'ay': 285,\n",
       " 'üòè': 286,\n",
       " 'feo': 287,\n",
       " 'chile': 288,\n",
       " 'llorar': 289,\n",
       " 'bonito': 290,\n",
       " 'favor': 291,\n",
       " 'porqu√©': 292,\n",
       " 'ponen': 293,\n",
       " 'a√±o': 294,\n",
       " 'misma': 295,\n",
       " 'buenos': 296,\n",
       " 'acabo': 297,\n",
       " 'anda': 298,\n",
       " 'risa': 299,\n",
       " 'minutos': 300,\n",
       " 'ex': 301,\n",
       " 'debe': 302,\n",
       " 'fea': 303,\n",
       " 'lameculos': 304,\n",
       " 'hizo': 305,\n",
       " 'miedo': 306,\n",
       " 'parece': 307,\n",
       " 'queda': 308,\n",
       " 'üòå': 309,\n",
       " 'culpa': 310,\n",
       " 'üíî': 311,\n",
       " 'primera': 312,\n",
       " 'siguen': 313,\n",
       " 'üçÜ': 314,\n",
       " 'lugar': 315,\n",
       " 'pu√±al': 316,\n",
       " 'partido': 317,\n",
       " 'sigo': 318,\n",
       " 'poner': 319,\n",
       " 'pensar': 320,\n",
       " 'llega': 321,\n",
       " 'canci√≥n': 322,\n",
       " 'jajajajajaja': 323,\n",
       " 'huevos': 324,\n",
       " 'salir': 325,\n",
       " 'parte': 326,\n",
       " 'ayer': 327,\n",
       " 'diga': 328,\n",
       " 'tarde': 329,\n",
       " 'familia': 330,\n",
       " 'pa': 331,\n",
       " 'volver': 332,\n",
       " 'video': 333,\n",
       " 'servicio': 334,\n",
       " 'sola': 335,\n",
       " 'd√≥nde': 336,\n",
       " 'mando': 337,\n",
       " 'dije': 338,\n",
       " '$': 339,\n",
       " 'üî•': 340,\n",
       " 'llevo': 341,\n",
       " 'momento': 342,\n",
       " 'hermano': 343,\n",
       " 'pena': 344,\n",
       " 'ves': 345,\n",
       " 'bonita': 346,\n",
       " 'espero': 347,\n",
       " 'lado': 348,\n",
       " 'primero': 349,\n",
       " 'üòî': 350,\n",
       " 'dices': 351,\n",
       " 'vaya': 352,\n",
       " 'sido': 353,\n",
       " 'necesito': 354,\n",
       " 'poder': 355,\n",
       " 'viejas': 356,\n",
       " 'üòû': 357,\n",
       " 'conmigo': 358,\n",
       " 'visto': 359,\n",
       " 'ni√±a': 360,\n",
       " 'sale': 361,\n",
       " 'perros': 362,\n",
       " 'üôÉ': 363,\n",
       " 'contigo': 364,\n",
       " 'tanta': 365,\n",
       " 'valen': 366,\n",
       " 'cagan': 367,\n",
       " 'agua': 368,\n",
       " 'clase': 369,\n",
       " 'vi': 370,\n",
       " 'haces': 371,\n",
       " '|': 372,\n",
       " 'amigas': 373,\n",
       " 'üòé': 374,\n",
       " 'casi': 375,\n",
       " 'claro': 376,\n",
       " 'grande': 377,\n",
       " 'pongo': 378,\n",
       " 'quiera': 379,\n",
       " 'tres': 380,\n",
       " 'luis': 381,\n",
       " 'dicho': 382,\n",
       " 'huevo': 383,\n",
       " 'gordas': 384,\n",
       " 'problema': 385,\n",
       " 'pasan': 386,\n",
       " 'forma': 387,\n",
       " 'doy': 388,\n",
       " 'lleva': 389,\n",
       " 'üí¶': 390,\n",
       " 'escuchar': 391,\n",
       " 'cualquier': 392,\n",
       " 'valgo': 393,\n",
       " 'ü§¶üèª\\u200d‚ôÄ': 394,\n",
       " 'escuela': 395,\n",
       " 'pap√°': 396,\n",
       " 'buenas': 397,\n",
       " 'pasado': 398,\n",
       " 'hecho': 399,\n",
       " 'ni√±o': 400,\n",
       " 'trabajar': 401,\n",
       " 'maldito': 402,\n",
       " 'chiflar': 403,\n",
       " 'creer': 404,\n",
       " 'haga': 405,\n",
       " 'meter': 406,\n",
       " 'seguir': 407,\n",
       " 'hambre': 408,\n",
       " 'padre': 409,\n",
       " 'hondure√±os': 410,\n",
       " 'cu√°l': 411,\n",
       " 'viejo': 412,\n",
       " 'vista': 413,\n",
       " 'ni√±as': 414,\n",
       " 'jam√°s': 415,\n",
       " 'rato': 416,\n",
       " 'm√∫sica': 417,\n",
       " 'haber': 418,\n",
       " 'andan': 419,\n",
       " '#': 420,\n",
       " 'parecen': 421,\n",
       " 'quer√≠a': 422,\n",
       " 'diciendo': 423,\n",
       " 'üò±': 424,\n",
       " 'dem√°s': 425,\n",
       " 'ja': 426,\n",
       " 'das': 427,\n",
       " 'apenas': 428,\n",
       " 'medio': 429,\n",
       " 'vete': 430,\n",
       " 'pienso': 431,\n",
       " 'estan': 432,\n",
       " 'siendo': 433,\n",
       " 'chica': 434,\n",
       " 'equipo': 435,\n",
       " 'üò£': 436,\n",
       " 'llegar': 437,\n",
       " 'esperando': 438,\n",
       " 'caso': 439,\n",
       " 'aparte': 440,\n",
       " 'calle': 441,\n",
       " 'xq': 442,\n",
       " 'vivo': 443,\n",
       " 'tipo': 444,\n",
       " 'comer': 445,\n",
       " 'adem√°s': 446,\n",
       " 'üôä': 447,\n",
       " 'deber√≠an': 448,\n",
       " '‚òπ': 449,\n",
       " 'clases': 450,\n",
       " 'hice': 451,\n",
       " 'maldita': 452,\n",
       " 'final': 453,\n",
       " 'celular': 454,\n",
       " 'hablando': 455,\n",
       " '√∫nico': 456,\n",
       " 'cargo': 457,\n",
       " 'pagar': 458,\n",
       " 'llama': 459,\n",
       " 'tambien': 460,\n",
       " 'mano': 461,\n",
       " 'deje': 462,\n",
       " 'nombre': 463,\n",
       " 'lluvia': 464,\n",
       " 'am': 465,\n",
       " 'punto': 466,\n",
       " 'ardidas': 467,\n",
       " 'üò¨': 468,\n",
       " '#gay': 469,\n",
       " 'gt': 470,\n",
       " 'den': 471,\n",
       " 'noches': 472,\n",
       " 'raz√≥n': 473,\n",
       " 'üòÅ': 474,\n",
       " 'üòä': 475,\n",
       " 'pueblo': 476,\n",
       " 'pasar': 477,\n",
       " 'coraz√≥n': 478,\n",
       " 'respeto': 479,\n",
       " 'caracteres': 480,\n",
       " 'muchas': 481,\n",
       " '@': 482,\n",
       " 'paso': 483,\n",
       " 'palabras': 484,\n",
       " 'mariquita': 485,\n",
       " 'üò¢': 486,\n",
       " 'ojos': 487,\n",
       " 'perd√≥n': 488,\n",
       " 'cae': 489,\n",
       " 'grupo': 490,\n",
       " 'viene': 491,\n",
       " 'periodistas': 492,\n",
       " 'juego': 493,\n",
       " 'vos': 494,\n",
       " 'ahorita': 495,\n",
       " 'mala': 496,\n",
       " 'sue√±o': 497,\n",
       " 'poniendo': 498,\n",
       " 'morra': 499,\n",
       " 'todav√≠a': 500,\n",
       " 'f√∫tbol': 501,\n",
       " 's': 502,\n",
       " 'mexico': 503,\n",
       " 'mamo': 504,\n",
       " 'teresa': 505,\n",
       " 'sabemos': 506,\n",
       " 'gran': 507,\n",
       " 'gobierno': 508,\n",
       " 'tacos': 509,\n",
       " 'pase': 510,\n",
       " 't': 511,\n",
       " 'pura': 512,\n",
       " 'ü§§': 513,\n",
       " 'vuelve': 514,\n",
       " 'messi': 515,\n",
       " 'darle': 516,\n",
       " 'word': 517,\n",
       " 'partir': 518,\n",
       " 'culero': 519,\n",
       " 'videos': 520,\n",
       " 'üòú': 521,\n",
       " 'costumbre': 522,\n",
       " 'lleno': 523,\n",
       " 'amp': 524,\n",
       " 'lados': 525,\n",
       " 'üò™': 526,\n",
       " 'tantita': 527,\n",
       " 'hermoso': 528,\n",
       " 'ratas': 529,\n",
       " 'v√°yanse': 530,\n",
       " 'pens√©': 531,\n",
       " 'bola': 532,\n",
       " 'üòï': 533,\n",
       " 'ayuda': 534,\n",
       " 'partidos': 535,\n",
       " 'üòã': 536,\n",
       " 'jugar': 537,\n",
       " 'chingue': 538,\n",
       " 'historia': 539,\n",
       " 'gustan': 540,\n",
       " 'viva': 541,\n",
       " 'salen': 542,\n",
       " 'putito': 543,\n",
       " 'dejan': 544,\n",
       " 'hermosa': 545,\n",
       " 'basura': 546,\n",
       " 'pesos': 547,\n",
       " 'arruga': 548,\n",
       " 'tantas': 549,\n",
       " 'primer': 550,\n",
       " 'acabar': 551,\n",
       " 'vayan': 552,\n",
       " 'escribir': 553,\n",
       " 'dio': 554,\n",
       " 'pol√≠ticos': 555,\n",
       " 'par': 556,\n",
       " 'demasiado': 557,\n",
       " 'verg√ºenza': 558,\n",
       " 'hagan': 559,\n",
       " 'usar': 560,\n",
       " 'nivel': 561,\n",
       " 'meses': 562,\n",
       " 'fan': 563,\n",
       " 'mes': 564,\n",
       " 'serio': 565,\n",
       " 'volviendo': 566,\n",
       " 'pendejas': 567,\n",
       " 'dado': 568,\n",
       " 'dando': 569,\n",
       " 'nueva': 570,\n",
       " 'tuits': 571,\n",
       " 'azul': 572,\n",
       " 'golfas': 573,\n",
       " 'llorando': 574,\n",
       " 'paz': 575,\n",
       " 'matar': 576,\n",
       " 'manos': 577,\n",
       " 'cabeza': 578,\n",
       " 'leche': 579,\n",
       " 'hablas': 580,\n",
       " 'trump': 581,\n",
       " 'mamaste': 582,\n",
       " 'coger': 583,\n",
       " 'examen': 584,\n",
       " 'ba√±o': 585,\n",
       " 'argentina': 586,\n",
       " 'suerte': 587,\n",
       " 'internet': 588,\n",
       " 'mamada': 589,\n",
       " 'darme': 590,\n",
       " 'berga': 591,\n",
       " 'extra√±o': 592,\n",
       " 'üíï': 593,\n",
       " 'cierto': 594,\n",
       " 'selecci√≥n': 595,\n",
       " 'cagada': 596,\n",
       " 'tantos': 597,\n",
       " 'p': 598,\n",
       " 'frente': 599,\n",
       " 'üòÖ': 600,\n",
       " 'sali√≥': 601,\n",
       " 'puebla': 602,\n",
       " '#putita': 603,\n",
       " 'mayor': 604,\n",
       " 'puras': 605,\n",
       " 'bonitas': 606,\n",
       " 'vato': 607,\n",
       " 'mente': 608,\n",
       " 'moral': 609,\n",
       " 'frase': 610,\n",
       " 'hablan': 611,\n",
       " 'alg√∫n': 612,\n",
       " 'rateros': 613,\n",
       " 'c': 614,\n",
       " 'habla': 615,\n",
       " 'saca': 616,\n",
       " 'mucha': 617,\n",
       " 'diario': 618,\n",
       " 'üòª': 619,\n",
       " 'pones': 620,\n",
       " 'fr√≠o': 621,\n",
       " 'üôà': 622,\n",
       " 'pela': 623,\n",
       " 'esperar': 624,\n",
       " 'clima': 625,\n",
       " '#noerapenal': 626,\n",
       " 'temprano': 627,\n",
       " 'peda': 628,\n",
       " 'derechos': 629,\n",
       " 'facebook': 630,\n",
       " 'tierra': 631,\n",
       " 'we': 632,\n",
       " 'perder': 633,\n",
       " 'sienten': 634,\n",
       " 'excelente': 635,\n",
       " 'besos': 636,\n",
       " 'dijeron': 637,\n",
       " 'gym': 638,\n",
       " 'venir': 639,\n",
       " 'ropa': 640,\n",
       " 'siquiera': 641,\n",
       " 'tristes': 642,\n",
       " 'ni√±os': 643,\n",
       " 'hermana': 644,\n",
       " 'comiendo': 645,\n",
       " 'sab√≠a': 646,\n",
       " 'sigan': 647,\n",
       " 'obvio': 648,\n",
       " 'dieron': 649,\n",
       " 'üòâ': 650,\n",
       " 'hola': 651,\n",
       " 'gol': 652,\n",
       " 'hacerlo': 653,\n",
       " 'ü§ó': 654,\n",
       " 'don': 655,\n",
       " 'mam√≥': 656,\n",
       " 'oye': 657,\n",
       " 'porno': 658,\n",
       " 'querer': 659,\n",
       " 'ning√∫n': 660,\n",
       " 'vales': 661,\n",
       " 'pan': 662,\n",
       " 'mandan': 663,\n",
       " 'cu√°ndo': 664,\n",
       " 'mentada': 665,\n",
       " 'empieza': 666,\n",
       " 'juntos': 667,\n",
       " 'alguna': 668,\n",
       " 'super': 669,\n",
       " 'üëè': 670,\n",
       " 'vivir': 671,\n",
       " 'tel√©fono': 672,\n",
       " 'sentir': 673,\n",
       " 'ü§∑üèª\\u200d‚ôÄ': 674,\n",
       " 'san': 675,\n",
       " 'üòë': 676,\n",
       " 'salgo': 677,\n",
       " 'aun': 678,\n",
       " 'prietas': 679,\n",
       " 'cuerpo': 680,\n",
       " 'atenci√≥n': 681,\n",
       " 'bajo': 682,\n",
       " 'traigo': 683,\n",
       " 'diferente': 684,\n",
       " '‚ò∫': 685,\n",
       " 'pasen': 686,\n",
       " 'real': 687,\n",
       " 'pensando': 688,\n",
       " 'fiesta': 689,\n",
       " 'empiezan': 690,\n",
       " 'lt': 691,\n",
       " 'empezar': 692,\n",
       " 'deben': 693,\n",
       " 'pas√≥': 694,\n",
       " 'arriba': 695,\n",
       " 'naturaleza': 696,\n",
       " 'üëå': 697,\n",
       " 'comida': 698,\n",
       " 'dime': 699,\n",
       " 'sigues': 700,\n",
       " 'ponerme': 701,\n",
       " 'ok': 702,\n",
       " 'gatos': 703,\n",
       " 'duele': 704,\n",
       " 'cago': 705,\n",
       " 'imagen': 706,\n",
       " 'vergazos': 707,\n",
       " 'm': 708,\n",
       " 'usan': 709,\n",
       " 'cree': 710,\n",
       " 'critican': 711,\n",
       " 'metro': 712,\n",
       " 'conozco': 713,\n",
       " 'ejercicio': 714,\n",
       " 'acaba': 715,\n",
       " 'normal': 716,\n",
       " 'calcuta': 717,\n",
       " 'the': 718,\n",
       " 'puse': 719,\n",
       " 'alcohol': 720,\n",
       " 'vemos': 721,\n",
       " 'c√°llate': 722,\n",
       " 'horrible': 723,\n",
       " 'papi': 724,\n",
       " 'trae': 725,\n",
       " 'gringos': 726,\n",
       " 'üòê': 727,\n",
       " 'asquerosa': 728,\n",
       " 'se√±or': 729,\n",
       " 'carro': 730,\n",
       " 'valga': 731,\n",
       " 'mediocre': 732,\n",
       " 'edad': 733,\n",
       " 'karma': 734,\n",
       " 'pel√≠cula': 735,\n",
       " 'fuerte': 736,\n",
       " 'seria': 737,\n",
       " 'subir': 738,\n",
       " 'decirle': 739,\n",
       " 'tweets': 740,\n",
       " 'justo': 741,\n",
       " 'i': 742,\n",
       " 'viernes': 743,\n",
       " 'pm': 744,\n",
       " 'üòñ': 745,\n",
       " 'realidad': 746,\n",
       " 'gato': 747,\n",
       " 'etc': 748,\n",
       " 'idiota': 749,\n",
       " 'saludos': 750,\n",
       " 'dentro': 751,\n",
       " '√∫nica': 752,\n",
       " 'cagas': 753,\n",
       " 'crees': 754,\n",
       " 'mame': 755,\n",
       " 'entrar': 756,\n",
       " 'pesar': 757,\n",
       " 'vuelven': 758,\n",
       " 'üíñ': 759,\n",
       " 'recuerdo': 760,\n",
       " 'piel': 761,\n",
       " 'banda': 762,\n",
       " 'canciones': 763,\n",
       " 'comprar': 764,\n",
       " 'üéµ': 765,\n",
       " 'üòì': 766,\n",
       " 'piensan': 767,\n",
       " 'sexo': 768,\n",
       " 'presidente': 769,\n",
       " 'argentinos': 770,\n",
       " 'hacerme': 771,\n",
       " 'llegue': 772,\n",
       " 'perfecto': 773,\n",
       " 'gordo': 774,\n",
       " 'tomar': 775,\n",
       " 'sino': 776,\n",
       " 'mensajes': 777,\n",
       " 'tobog√°n': 778,\n",
       " 'estilo': 779,\n",
       " 'millones': 780,\n",
       " 'dejo': 781,\n",
       " 'üôÇ': 782,\n",
       " 'puso': 783,\n",
       " 'queriendo': 784,\n",
       " 'instagram': 785,\n",
       " 'ü§¶üèª\\u200d‚ôÇ': 786,\n",
       " 'usa': 787,\n",
       " 'necesario': 788,\n",
       " 'tr√°fico': 789,\n",
       " 'di': 790,\n",
       " 'pendejadas': 791,\n",
       " 'vengo': 792,\n",
       " 'puesto': 793,\n",
       " 'manera': 794,\n",
       " 'loco': 795,\n",
       " '‚Äî': 796,\n",
       " 'libro': 797,\n",
       " 'cargar': 798,\n",
       " 'comentarios': 799,\n",
       " 'venga': 800,\n",
       " 'caliente': 801,\n",
       " 'cine': 802,\n",
       " 'muertos': 803,\n",
       " 'manda': 804,\n",
       " 'caras': 805,\n",
       " 'regreso': 806,\n",
       " 'come': 807,\n",
       " 'malas': 808,\n",
       " 'lunes': 809,\n",
       " 'idea': 810,\n",
       " 'doble': 811,\n",
       " 'üê∑': 812,\n",
       " 'pasando': 813,\n",
       " 'domingo': 814,\n",
       " 'placer': 815,\n",
       " 'mexicano': 816,\n",
       " 'padres': 817,\n",
       " 'necesita': 818,\n",
       " 'usted': 819,\n",
       " 'urge': 820,\n",
       " 'oigan': 821,\n",
       " 'vecinos': 822,\n",
       " '‚úäüèº': 823,\n",
       " 'zona': 824,\n",
       " 'lengua': 825,\n",
       " 'cruz': 826,\n",
       " 'piernas': 827,\n",
       " 'am√©rica': 828,\n",
       " 'pe√±a': 829,\n",
       " 'nieto': 830,\n",
       " 'cualquiera': 831,\n",
       " 'nomas': 832,\n",
       " 'u': 833,\n",
       " 'jeje': 834,\n",
       " 'unico': 835,\n",
       " '√°rbitro': 836,\n",
       " 'busca': 837,\n",
       " 'pide': 838,\n",
       " 'ta': 839,\n",
       " 'profe': 840,\n",
       " 'buscando': 841,\n",
       " 'zorras': 842,\n",
       " 'corruptos': 843,\n",
       " 'chilenos': 844,\n",
       " 'cabrones': 845,\n",
       " 'üëä': 846,\n",
       " 'maquillaje': 847,\n",
       " 'esposa': 848,\n",
       " 'septiembre': 849,\n",
       " 'pareja': 850,\n",
       " 'quedo': 851,\n",
       " 'parada': 852,\n",
       " 'mam√≥n': 853,\n",
       " 'onda': 854,\n",
       " 'veas': 855,\n",
       " 'üíô': 856,\n",
       " 'violencia': 857,\n",
       " 'so√±√©': 858,\n",
       " 'tweet': 859,\n",
       " 'pasas': 860,\n",
       " 'quede': 861,\n",
       " 'iphone': 862,\n",
       " 'deber√≠a': 863,\n",
       " 'üí™': 864,\n",
       " 'chava': 865,\n",
       " 'hicieron': 866,\n",
       " 'sacan': 867,\n",
       " 'rancho': 868,\n",
       " 'quita': 869,\n",
       " 'traen': 870,\n",
       " 'llegu√©': 871,\n",
       " 'dise√±o': 872,\n",
       " 'coco': 873,\n",
       " 'joder': 874,\n",
       " 'gustar': 875,\n",
       " 'üò∞': 876,\n",
       " 'carajo': 877,\n",
       " 'drogas': 878,\n",
       " 'sacar': 879,\n",
       " 'cabello': 880,\n",
       " 'toca': 881,\n",
       " 'macho': 882,\n",
       " 'locas': 883,\n",
       " 'ac√°': 884,\n",
       " 'kilos': 885,\n",
       " 'carg√≥': 886,\n",
       " 'apoyo': 887,\n",
       " 'seg√∫n': 888,\n",
       " 'semanas': 889,\n",
       " 'mierdas': 890,\n",
       " 'f√°cil': 891,\n",
       " 'encuentro': 892,\n",
       " 'blanca': 893,\n",
       " 'pronto': 894,\n",
       " 'opini√≥n': 895,\n",
       " 'nervios': 896,\n",
       " 'j√≥venes': 897,\n",
       " 'holanda': 898,\n",
       " 'malditos': 899,\n",
       " 'hocico': 900,\n",
       " 'goool': 901,\n",
       " 'sonrisa': 902,\n",
       " 'mexicanos': 903,\n",
       " 'mand√≥': 904,\n",
       " 'oso': 905,\n",
       " '%': 906,\n",
       " 'vender': 907,\n",
       " 'leyendo': 908,\n",
       " 'hueva': 909,\n",
       " 'üëèüèª': 910,\n",
       " 'gritando': 911,\n",
       " 'club': 912,\n",
       " 'maestra': 913,\n",
       " 'tigres': 914,\n",
       " 'andes': 915,\n",
       " 'dejes': 916,\n",
       " 'mueren': 917,\n",
       " 'coraje': 918,\n",
       " 'ahi': 919,\n",
       " 'machorra': 920,\n",
       " 'buscar': 921,\n",
       " 'cambio': 922,\n",
       " 'acaban': 923,\n",
       " 'like': 924,\n",
       " 'diosito': 925,\n",
       " 'cami√≥n': 926,\n",
       " 'vieron': 927,\n",
       " 'romper': 928,\n",
       " 'mandarte': 929,\n",
       " 'vs': 930,\n",
       " 'lujo': 931,\n",
       " 'aire': 932,\n",
       " 'nena': 933,\n",
       " 'madrid': 934,\n",
       " 'traes': 935,\n",
       " 'bro': 936,\n",
       " 'vive': 937,\n",
       " 'embarazada': 938,\n",
       " 'mega': 939,\n",
       " 'acuerdo': 940,\n",
       " 'sirve': 941,\n",
       " 'naca': 942,\n",
       " 'dientes': 943,\n",
       " 'ponte': 944,\n",
       " 'pregunta': 945,\n",
       " 'pasaron': 946,\n",
       " 'voz': 947,\n",
       " 'alto': 948,\n",
       " 'verla': 949,\n",
       " 'himno': 950,\n",
       " 'robar': 951,\n",
       " 'darte': 952,\n",
       " 'duro': 953,\n",
       " 'lindo': 954,\n",
       " 'muerto': 955,\n",
       " 'leer': 956,\n",
       " 'merece': 957,\n",
       " 'memoria': 958,\n",
       " 'corriente': 959,\n",
       " 'pueda': 960,\n",
       " 'alma': 961,\n",
       " 'honor': 962,\n",
       " 'cuanto': 963,\n",
       " 'est√∫pida': 964,\n",
       " 'tuit': 965,\n",
       " 'quisiera': 966,\n",
       " 'guapas': 967,\n",
       " 'cc': 968,\n",
       " 'jueves': 969,\n",
       " '#mexicandesmotherpalmundial': 970,\n",
       " 'hacerse': 971,\n",
       " '#travesti': 972,\n",
       " 'compartir': 973,\n",
       " 'sentirse': 974,\n",
       " 'despertar': 975,\n",
       " 'pedir': 976,\n",
       " 'culeros': 977,\n",
       " 'üòÑ': 978,\n",
       " 'üòÉ': 979,\n",
       " 'hagas': 980,\n",
       " 'hijas': 981,\n",
       " 'üò•': 982,\n",
       " 'dejas': 983,\n",
       " 'cdmx': 984,\n",
       " 'malo': 985,\n",
       " 'pri': 986,\n",
       " 'morras': 987,\n",
       " 'segunda': 988,\n",
       " 'oportunidad': 989,\n",
       " 'ciudad': 990,\n",
       " 'tatuaje': 991,\n",
       " 'meta': 992,\n",
       " 'ptm': 993,\n",
       " 'fans': 994,\n",
       " 'fb': 995,\n",
       " 'perfil': 996,\n",
       " 'qued√≥': 997,\n",
       " 'puerco': 998,\n",
       " 'oficina': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializa el modelo, el optimizador y el DataLoader\n",
    "model = LSTMSelfAttention(\n",
    "    vocab_size=len(V),\n",
    "    emb_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    pad_id=0,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "tr_data = TweetsDataset(tr_txt, tr_y, V)\n",
    "tr_loader = DataLoader(\n",
    "    tr_data,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate(batch, pad_id=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a5041",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236968b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Loss: 0.0115 | PPL: 1.0116\n",
      "Epoch 2/200 | Loss: 0.0109 | PPL: 1.0110\n",
      "Epoch 3/200 | Loss: 0.0109 | PPL: 1.0109\n",
      "Epoch 4/200 | Loss: 0.0108 | PPL: 1.0108\n",
      "Epoch 5/200 | Loss: 0.0107 | PPL: 1.0107\n",
      "Epoch 6/200 | Loss: 0.0106 | PPL: 1.0107\n",
      "Epoch 7/200 | Loss: 0.0106 | PPL: 1.0106\n",
      "Epoch 8/200 | Loss: 0.0105 | PPL: 1.0106\n",
      "Epoch 9/200 | Loss: 0.0105 | PPL: 1.0105\n",
      "Epoch 10/200 | Loss: 0.0104 | PPL: 1.0105\n",
      "Epoch 11/200 | Loss: 0.0104 | PPL: 1.0105\n",
      "Epoch 12/200 | Loss: 0.0104 | PPL: 1.0104\n",
      "Epoch 13/200 | Loss: 0.0103 | PPL: 1.0104\n",
      "Epoch 14/200 | Loss: 0.0102 | PPL: 1.0103\n",
      "Epoch 15/200 | Loss: 0.0102 | PPL: 1.0102\n",
      "Epoch 16/200 | Loss: 0.0101 | PPL: 1.0102\n",
      "Epoch 17/200 | Loss: 0.0101 | PPL: 1.0102\n",
      "Epoch 18/200 | Loss: 0.0101 | PPL: 1.0101\n",
      "Epoch 19/200 | Loss: 0.0100 | PPL: 1.0100\n",
      "Epoch 20/200 | Loss: 0.0099 | PPL: 1.0100\n",
      "Epoch 21/200 | Loss: 0.0099 | PPL: 1.0100\n",
      "Epoch 22/200 | Loss: 0.0099 | PPL: 1.0099\n",
      "Epoch 23/200 | Loss: 0.0098 | PPL: 1.0099\n",
      "Epoch 24/200 | Loss: 0.0097 | PPL: 1.0098\n",
      "Epoch 25/200 | Loss: 0.0097 | PPL: 1.0097\n",
      "Epoch 26/200 | Loss: 0.0096 | PPL: 1.0097\n",
      "Epoch 27/200 | Loss: 0.0096 | PPL: 1.0096\n",
      "Epoch 28/200 | Loss: 0.0096 | PPL: 1.0096\n",
      "Epoch 29/200 | Loss: 0.0095 | PPL: 1.0095\n",
      "Epoch 30/200 | Loss: 0.0094 | PPL: 1.0095\n",
      "Epoch 31/200 | Loss: 0.0094 | PPL: 1.0094\n",
      "Epoch 32/200 | Loss: 0.0093 | PPL: 1.0094\n",
      "Epoch 33/200 | Loss: 0.0093 | PPL: 1.0093\n",
      "Epoch 34/200 | Loss: 0.0092 | PPL: 1.0093\n",
      "Epoch 35/200 | Loss: 0.0092 | PPL: 1.0093\n",
      "Epoch 36/200 | Loss: 0.0092 | PPL: 1.0092\n",
      "Epoch 37/200 | Loss: 0.0091 | PPL: 1.0092\n",
      "Epoch 38/200 | Loss: 0.0091 | PPL: 1.0091\n",
      "Epoch 39/200 | Loss: 0.0091 | PPL: 1.0091\n",
      "Epoch 40/200 | Loss: 0.0090 | PPL: 1.0091\n",
      "Epoch 41/200 | Loss: 0.0090 | PPL: 1.0091\n",
      "Epoch 42/200 | Loss: 0.0090 | PPL: 1.0090\n",
      "Epoch 43/200 | Loss: 0.0090 | PPL: 1.0090\n",
      "Epoch 44/200 | Loss: 0.0090 | PPL: 1.0090\n",
      "Epoch 45/200 | Loss: 0.0090 | PPL: 1.0090\n",
      "Epoch 46/200 | Loss: 0.0089 | PPL: 1.0090\n",
      "Epoch 47/200 | Loss: 0.0089 | PPL: 1.0090\n",
      "Epoch 48/200 | Loss: 0.0089 | PPL: 1.0090\n",
      "Epoch 49/200 | Loss: 0.0089 | PPL: 1.0090\n",
      "Epoch 50/200 | Loss: 0.0089 | PPL: 1.0090\n",
      "Epoch 51/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 52/200 | Loss: 0.0089 | PPL: 1.0090\n",
      "Epoch 53/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 54/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 55/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 56/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 57/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 58/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 59/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 60/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 61/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 62/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 63/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 64/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 65/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 66/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 67/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 68/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 69/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 70/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 71/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 72/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 73/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 74/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 75/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 76/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 77/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 78/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 79/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 80/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 81/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 82/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 83/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 84/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 85/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 86/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 87/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 88/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 89/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 90/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 91/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 92/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 93/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 94/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 95/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 96/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 97/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 98/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 99/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 100/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 101/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 102/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 103/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 104/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 105/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 106/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 107/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 108/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 109/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 110/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 111/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 112/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 113/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 114/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 115/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 116/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 117/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 118/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 119/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 120/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 121/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 122/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 123/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 124/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 125/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 126/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 127/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 128/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 129/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 130/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 131/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 132/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 133/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 134/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 135/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 136/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 137/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 138/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 139/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 140/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 141/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 142/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 143/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 144/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 145/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 146/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 147/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 148/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 149/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 150/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 151/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 152/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 153/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 154/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 155/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 156/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 157/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 158/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 159/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 160/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 161/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 162/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 163/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 164/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 165/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 166/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 167/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 168/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 169/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 170/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 171/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 172/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 173/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 174/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 175/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 176/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 177/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 178/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 179/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 180/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 181/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 182/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 183/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 184/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 185/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 186/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 187/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 188/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 189/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 190/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 191/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 192/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 193/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 194/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 195/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 196/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 197/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 198/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 199/200 | Loss: 0.0089 | PPL: 1.0089\n",
      "Epoch 200/200 | Loss: 0.0089 | PPL: 1.0089\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_valid = 0\n",
    "    for batch in tr_loader:\n",
    "        x, lens = batch\n",
    "        x, lens = x.to(device), lens.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, lens)\n",
    "\n",
    "        loss_sum, valid = loss(logits, x, ignore_pad_id=0)\n",
    "        loss_val = loss_sum / valid\n",
    "        loss_val.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_val.item()\n",
    "        total_valid += valid.item()\n",
    "    ppl = exp(total_loss / max(total_valid, 1))\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Loss: {total_loss / total_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31b2a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (train): 725.9629\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity (train): {perplexity_corpus(model, tr_loader, pad_id=0, device=device):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58159a01",
   "metadata": {},
   "source": [
    "## Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (val): 346.0325\n"
     ]
    }
   ],
   "source": [
    "#se obtiene una perplejidad menor para el conjunto de validaci√≥n respe to a los otros dos modelos\n",
    "val_txt, val_y = get_texts_from_file(\"../../corpus/mex20_val.txt\", \"../../corpus/mex20_val_labels.txt\")\n",
    "val_data = TweetsDataset(val_txt, val_y, V)\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate(batch, pad_id=0)\n",
    ")\n",
    "print(f\"Perplexity (val): {perplexity_corpus(model, val_loader, pad_id=0, device=device):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009beae3",
   "metadata": {},
   "source": [
    "# Statistical Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "105c55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramData:\n",
    "\n",
    "    def __init__(self, vocab_max, tokenizer):\n",
    "        self.vocab_max = vocab_max\n",
    "        self.tokenizer = tokenizer\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.final_vocabulary = set()\n",
    "\n",
    "    def _tok(self, txt: str):\n",
    "        # si es funci√≥n, ll√°mala; si es objeto con .tokenize, √∫salo\n",
    "        return self.tokenizer(txt) if callable(self.tokenizer) else self.tokenizer.tokenize(txt)\n",
    "\n",
    "    def fit(self, raw_texts):\n",
    "\n",
    "        freq_dist = FreqDist()\n",
    "        tokenized_corpus = []\n",
    "\n",
    "        for txt in raw_texts:\n",
    "            tokens = self._tok(txt)\n",
    "            tokenized_corpus.append(tokens)\n",
    "            for w in tokens:\n",
    "                freq_dist[w] += 1\n",
    "\n",
    "        self.final_vocabulary = {tok for tok, _ in freq_dist.most_common(self.vocab_max)}\n",
    "        self.final_vocabulary.update([self.UNK, self.SOS, self.EOS])\n",
    "\n",
    "        transformed_corpus = []\n",
    "        for tokens in tokenized_corpus:\n",
    "            transformed_corpus.append(self.transform(tokens))\n",
    "        return transformed_corpus\n",
    "    \n",
    "    def mask_oov(self, w):\n",
    "        return self.UNK if w not in self.final_vocabulary else w\n",
    "    \n",
    "    def add_sos_eos(self, tokens):\n",
    "        return [self.SOS, self.SOS] + tokens + [self.EOS]\n",
    "\n",
    "    def transform(self, tokens):\n",
    "        transformed = []\n",
    "        for w in tokens:\n",
    "            transformed.append(self.mask_oov(w))\n",
    "        transformed = self.add_sos_eos(transformed)\n",
    "        return transformed\n",
    "    \n",
    "    def transform_text(self, txt: str):\n",
    "        return self.transform(self._tok(txt))\n",
    "    \n",
    "class TrigramLanguageModel:\n",
    "\n",
    "    def __init__(self, lambda1=0.4, lambda2=0.3, lambda3=0.3):\n",
    "        self.lambda1 = lambda1 # trigramas\n",
    "        self.lambda2 = lambda2 # bigramas\n",
    "        self.lambda3 = lambda3 # unigramas\n",
    "\n",
    "        # Contadores\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "\n",
    "        self.vocab = 0\n",
    "        self.total_tokens = 0\n",
    "        self.V = 0\n",
    "\n",
    "    def train(self, transformed_corpus, final_vocabulary):\n",
    "        self.vocab = final_vocabulary\n",
    "        self.V = len(final_vocabulary)\n",
    "\n",
    "        for tokens in transformed_corpus:\n",
    "            for i, w in enumerate(tokens):\n",
    "\n",
    "                # Unigramas\n",
    "                self.unigram_counts[w] = self.unigram_counts.get(w, 0) + 1\n",
    "\n",
    "                # Bigramas\n",
    "                if i > 0:\n",
    "                    w_prev = tokens[i-1]\n",
    "                    self.bigram_counts[(w_prev, w)] = self.bigram_counts.get((w_prev, w), 0) + 1\n",
    "                    \n",
    "                # Trigramas\n",
    "                if i > 1:\n",
    "                    w_prev2 = tokens[i-2]\n",
    "                    self.trigram_counts[(w_prev2, w_prev, w)] = \\\n",
    "                        self.trigram_counts.get((w_prev2, w_prev, w), 0) + 1\n",
    "        self.total_tokens = sum(self.unigram_counts.values())\n",
    "\n",
    "    def mask_oov(self, w):\n",
    "        return \"<unk>\" if w not in self.vocab else w\n",
    "\n",
    "    def unigram_probability(self, w):\n",
    "        return (self.unigram_counts.get(self.mask_oov(w), 0) + 1) / (self.total_tokens + self.V)\n",
    "    \n",
    "    def bigram_probability(self, w_prev, w):\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "        \n",
    "        numerator = self.bigram_counts.get((w_prev, w), 0) + 1\n",
    "        denominator = self.unigram_counts.get(w_prev, 0) + self.V\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def trigram_probability(self, w_prev2, w_prev, w):\n",
    "        w_prev2 = self.mask_oov(w_prev2)\n",
    "        w_prev = self.mask_oov(w_prev)\n",
    "        w = self.mask_oov(w)\n",
    "\n",
    "        numerator = self.trigram_counts.get((w_prev2, w_prev, w), 0) + 1\n",
    "        denominator = self.bigram_counts.get((w_prev2, w_prev), 0) + self.V\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def probability_of_word(self, w_prev2, w_prev, w):\n",
    "        return self.lambda1 * self.trigram_probability(w_prev2, w_prev, w) + \\\n",
    "                self.lambda2 * self.bigram_probability(w_prev, w) + \\\n",
    "                self.lambda3 * self.unigram_probability(w)\n",
    "    \n",
    "    def sequence_probability(self, sequence):\n",
    "        import math\n",
    "        log_prob = 0.0\n",
    "        for i in range(2, len(sequence)):\n",
    "            w_prev2 = sequence[i-2]\n",
    "            w_prev = sequence[i-1]\n",
    "            w = sequence[i]\n",
    "\n",
    "            p = self.probability_of_word(w_prev2, w_prev, w)\n",
    "            log_prob += math.log(p)\n",
    "        return math.exp(log_prob)\n",
    "    \n",
    "    def check_prob(self):\n",
    "        print(sum(self.unigram_probability(w) for w in self.vocab))\n",
    "\n",
    "        print(sum(self.bigram_probability(\"hola\", w) for w in self.vocab))\n",
    "\n",
    "        print(sum(self.trigram_probability(\"hola\", \"como\", w) for w in self.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b79b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_perplexity_trigram(model, transformed_corpus):\n",
    "    nll_sum = 0.0\n",
    "    tok_count = 0\n",
    "    for seq in transformed_corpus:\n",
    "        for i in range(2, len(seq)):\n",
    "            w2, w1, w = seq[i-2], seq[i-1], seq[i]\n",
    "            p = model.probability_of_word(w2, w1, w)\n",
    "            if p <= 0.0: p = 1e-12\n",
    "            nll_sum += -log(p)\n",
    "            tok_count += 1\n",
    "    return exp(nll_sum / max(tok_count, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97c7852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRIGRAM] train PPL=463.030 | val PPL=411.093\n"
     ]
    }
   ],
   "source": [
    "# 1) Fit en TRAIN: arma vocab y corpus transformado con <s>,</s>,<unk>\n",
    "tri_data = TrigramData(vocab_max=5000, tokenizer=tweet_tokenizer)\n",
    "train_transformed = tri_data.fit(tr_txt)   # lista de listas de tokens\n",
    "# 2) Instancia y ‚Äúentrena‚Äù (cuenta n-gramas)\n",
    "tri_lm = TrigramLanguageModel(lambda1=0.4, lambda2=0.3, lambda3=0.3)\n",
    "tri_lm.train(train_transformed, tri_data.final_vocabulary)\n",
    "\n",
    "# 3) TRANSFORM de VAL/TEST usando el MISMO vocab\n",
    "def transform_corpus(raw_texts, tri_data):\n",
    "    out = []\n",
    "    for txt in raw_texts:\n",
    "        toks = tri_data.tokenizer(txt)\n",
    "        out.append(tri_data.transform(toks))   # aplica UNK + <s>,<s>,...,</s>\n",
    "    return out\n",
    "\n",
    "val_transformed = transform_corpus(val_txt, tri_data)\n",
    "\n",
    "train_ppl = corpus_perplexity_trigram(tri_lm, train_transformed)\n",
    "val_ppl   = corpus_perplexity_trigram(tri_lm, val_transformed)\n",
    "print(f\"[TRIGRAM] train PPL={train_ppl:.3f} | val PPL={val_ppl:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9f7e2",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9134c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class NgramData():\n",
    "  \"\"\"\n",
    "  Esta clase toma un corpus y a trav√©s de los m√©todos fit y transform, se crea una lista de \n",
    "  n-gramas pensada para el entrenamiento de la red neuronal de Bengio pensando en una CBOW.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               N: int,\n",
    "               vocab_max: int = 5000,\n",
    "               tokenizer: callable = None,\n",
    "               embeddings: np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Constructor de la clase.\n",
    "\n",
    "    Args:\n",
    "        N (int): Tama√±o de los n-gramas.\n",
    "        vocab_max (int, optional): Tama√±o m√°ximo del vocabulario a considerar. Defaults to 5000.\n",
    "        tokenizier (callable, optional): Tokenizador. Defaults to None.\n",
    "        embeddings (np.ndarray, optional): Matriz de embeddings pre-entrenada. Debe entrar en el orden en el que entran las palabras. Defaults to None.\n",
    "    \"\"\"\n",
    "    self.N = N\n",
    "    self.vocab_max = vocab_max\n",
    "    self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "    self.embeddings = embeddings\n",
    "\n",
    "    # Tokens que no queremos en nuestro corpus.\n",
    "    self.punct = ['.', ',', ';', ':', '-', '^', '\"'\n",
    "                  '\"', '!', '¬°', '¬ø', '?', '<url>', '#', '@usuario']\n",
    "\n",
    "    # Tokens especiales\n",
    "    self.UNK = \"<unk>\"\n",
    "    self.SOS = \"<s>\"\n",
    "    self.EOS = \"</s>\"\n",
    "\n",
    "  def get_vocab_size(self) -> int:\n",
    "    \"\"\"\n",
    "    Devuelve el tama√±o del vocabulario.\n",
    "\n",
    "    Returns:\n",
    "        int: Tama√±o del vocabulario.\n",
    "    \"\"\"\n",
    "    return len(self.vocab)\n",
    "\n",
    "  def default_tokenizer(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizador por defecto. Simplemente separa cada oraci√≥n por espacios.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento a tokenizar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    return doc.split(\" \")\n",
    "\n",
    "  def remove_word(self, word: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica si la palabra en cuesti√≥n debe eliminarse seg√∫n los siguientes criterios:\n",
    "    - Es un signo de puntuaci√≥n\n",
    "    - Es un d√≠gito\n",
    "\n",
    "    Args:\n",
    "        word (str): Palabra a evaluar.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si se elimina.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    is_punct = True if word in self.punct else False\n",
    "    is_digit = word.isnumeric()\n",
    "    return is_punct or is_digit\n",
    "\n",
    "  def sortFreqDist(self, freq_dist: nltk.FreqDist) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con el top de palabras por frecuencia. El tama√±o de la lista es self.vocab_max.\n",
    "\n",
    "    Args:\n",
    "        freq_dist (nltk.FreqDist): Objeto de frecuencias (nltk) del corpus considerado.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tama√±o self.vocab_max.\n",
    "    \"\"\"\n",
    "    freq_dist = dict(freq_dist)\n",
    "    # Aqu√≠ key es una funci√≥n que se aplica a cada par√°metro\n",
    "    # antes de compararlo. En este caso se pasa\n",
    "    # freq_dist.get para asegurarse de que el ordenamiento\n",
    "    # se haga por las frecuencias y no por orden alfab√©tico.\n",
    "    return sorted(freq_dist,\n",
    "                  key=freq_dist.get,\n",
    "                  reverse=True)\n",
    "\n",
    "  def get_vocab(self, corpus: list[str]) -> set:\n",
    "    \"\"\"\n",
    "    Devuelve el vocabulario a partir de un corpus dado.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Corpus del cual se quiere obtener el vocabulario. Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        set: Vocabulario.\n",
    "    \"\"\"\n",
    "    freq_dist = FreqDist(\n",
    "      [w.lower()\n",
    "       for sentence in corpus\n",
    "       for w in self.tokenizer(sentence)\n",
    "       if not self.remove_word(w)]\n",
    "    )\n",
    "    sorted_words = self.sortFreqDist(freq_dist)[:self.vocab_max-3]\n",
    "    return set(sorted_words)\n",
    "\n",
    "  def fit(self, corpus: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Carga el vocabulario y crea diccionarios de √≠ndices <-> palabras. Adem√°s, si se aporta una matriz de embeddings pre-entrenados, tambi√©n construye la submatriz con los elementos del vocabulario.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "    \"\"\"\n",
    "    # Cargamos el vocabulario\n",
    "    self.vocab = self.get_vocab(corpus)\n",
    "    self.vocab.add(self.UNK)\n",
    "    self.vocab.add(self.SOS)\n",
    "    self.vocab.add(self.EOS)\n",
    "\n",
    "    # Diccionarios palabras <-> ids\n",
    "    self.w2id = dict()\n",
    "    self.id2w = dict()\n",
    "\n",
    "    if self.embeddings:\n",
    "      self.embeddings_matrix = np.empty([self.vocab_max,\n",
    "                                         self.embeddings.vector_size])\n",
    "\n",
    "    id = 0\n",
    "    for doc in corpus:\n",
    "      for word in self.tokenizer(doc):\n",
    "        word_ = word.lower()\n",
    "        if (word_ in self.vocab) and (not word_ in self.w2id):\n",
    "          self.w2id[word_] = id\n",
    "          self.id2w[id] = word_\n",
    "\n",
    "          # Si se aporta una matriz de embeddings,\n",
    "          # aqu√≠ se crea la submatriz.\n",
    "          if self.embeddings:\n",
    "            if word in self.embeddings:\n",
    "              self.embeddings_matrix[id] = self.embeddings[word_]\n",
    "            else:\n",
    "              self.embeddings_matrix[id] = np.random.rand(\n",
    "                self.embeddings.vector_size)\n",
    "\n",
    "          id += 1\n",
    "\n",
    "    # A√±adirmos los tokens especiales a los diccionarios.\n",
    "    self.w2id.update(\n",
    "      {self.UNK: id,\n",
    "       self.SOS: id + 1,\n",
    "       self.EOS: id + 2}\n",
    "    )\n",
    "    self.id2w.update(\n",
    "      {id: self.UNK,\n",
    "       id + 1: self.SOS,\n",
    "       id + 2: self.EOS}\n",
    "    )\n",
    "\n",
    "  def get_ngram_doc(self, doc: str) -> list:\n",
    "    \"\"\"\n",
    "    Devuelve una lista con n-gramas de un documento dado.\n",
    "\n",
    "    Args:\n",
    "        doc (str): Documento del que se quieren obtener los n-gramas.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de n-gramas.\n",
    "    \"\"\"\n",
    "    doc_tokens = self.tokenizer(doc)\n",
    "    doc_tokens = self.replace_unk(doc_tokens)\n",
    "    doc_tokens = [w.lower()\n",
    "                  for w in doc_tokens]\n",
    "    doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "\n",
    "    return list(nltk.ngrams(doc_tokens, self.N))\n",
    "\n",
    "  def replace_unk(self, doc_tokens: list[str]) -> list:\n",
    "    \"\"\"\n",
    "    Toma un lista de tokens e intercambia los tokens out-of-vocabulary por el token especial self.UNK.\n",
    "\n",
    "    Args:\n",
    "        doc_tokens (list[str]): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens procesada.\n",
    "    \"\"\"\n",
    "    for i, token in enumerate(doc_tokens):\n",
    "      if token.lower() not in self.vocab:\n",
    "        doc_tokens[i] = self.UNK\n",
    "    return doc_tokens\n",
    "\n",
    "  def transform(self, corpus: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Devuelve una tupla de arreglos de Numpy. El primero tendr√° los ids de las palabras en el contexto, mientras que la segunda el id de la palabra que se debe predecir.\n",
    "\n",
    "    Se piensa en un modelo de CBOW. Damos el contexto y queremos predecir la palabra que sigue.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): Lista de documentos.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Arreglos de numpy con ids de los contextos y id de la palabra objetivo.\n",
    "    \"\"\"\n",
    "    X_ngrams = list()\n",
    "    y = []\n",
    "\n",
    "    for doc in corpus:\n",
    "      doc_ngram = self.get_ngram_doc(doc)\n",
    "      for words_window in doc_ngram:\n",
    "        words_window_ids = [self.w2id[w]\n",
    "                            for w in words_window]\n",
    "        X_ngrams.append(list(words_window_ids[:-1]))\n",
    "        y.append(words_window_ids[-1])\n",
    "\n",
    "    return np.array(X_ngrams), np.array(y)\n",
    "  \n",
    "class NeuralLanguageModel(nn.Module):\n",
    "  \"\"\"\n",
    "  Red neuronal de Bengio :)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, args):\n",
    "    \"\"\"\n",
    "    Constructor  de la clase.\n",
    "\n",
    "    El modelo de red neuronal par lenguaje de Bengio tiene la siguiente estructura:\n",
    "    Para un modelo de n-gramas, se dan las primeras n-1 palabras como contexto y se intenta predecir la n-√©sima palabra.\n",
    "    (1) n-1 representaciones iniciales: suelen ser one-hot. Pero aqu√≠ se toman de NgramData.\n",
    "        x\n",
    "    (2) n-1 representaciones aprendidas de tama√±o m: se obtienen de manera individual (por palabra). \n",
    "        (x = Cx)\n",
    "        En esta implementaci√≥n C se inicia de manera aleatoria.\n",
    "    (3) Capa oculta de tama√±o h: se mezclan las n-1 representaciones del paso anterior y se aplica tanh. \n",
    "        (h = tanh(Hx + d))\n",
    "        Nosotros vamos a usar ReLu en vez de tanh.\n",
    "    (4) Capa de salida de tama√±o m: se aplica softmax a la salida de la capa anterior.\n",
    "        (y = softmax(Uh + b))\n",
    "        Nosotros no vamos a aplicar softmax aqu√≠, sino afuerita.\n",
    "\n",
    "    Args:\n",
    "        args (Any): Diccionario de variables.\n",
    "    \"\"\"\n",
    "    super(NeuralLanguageModel, self).__init__()\n",
    "\n",
    "    self.window_size = args.N - 1  # Las n-1 palabras que entran (el contexto).\n",
    "    self.embedding_size = args.m  # Tama√±o de las representaciones.\n",
    "\n",
    "    # Matriz C para convertir las representaciones. Pero est√° chido porque sus entradas son \"entrenables\".\n",
    "    self.emb = nn.Embedding(args.vocab_size, args.m)\n",
    "    # Primera capa oculta de las representaciones aprendidas a la oculta.\n",
    "    self.fc1 = nn.Linear(args.m * (args.N - 1), args.d_h)\n",
    "    # Un dropout para alocarnos\n",
    "    self.drop1 = nn.Dropout(p=args.dropout)\n",
    "    # Aqu√≠ solamente se va a hacer el producto por la matriz U.\n",
    "    # La softmax se va a aplicar por fuera de la red para obtener la siguiente palabra seg√∫n la red.\n",
    "    self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Aqu√≠ se cambia la representaci√≥n inicial por la aprendida.\n",
    "    # Es un producto matricial. Aqu√≠ las representaciones siguen siendo matrices.\n",
    "    x = self.emb(x)\n",
    "    # Se cambia el tama√±o para que se considere como una sola capa.\n",
    "    x = x.view(-1, self.window_size * self.embedding_size)\n",
    "    # Aqu√≠ se hace relu(Hx + d)\n",
    "    h = F.relu(self.fc1(x))  # relu(z) = max{0, z}\n",
    "    # El dropout para alocarnoooos wuuuuuuuu\n",
    "    h = self.drop1(h)\n",
    "\n",
    "    # Devolvemos solamente (Uh + b)\n",
    "    return self.fc2(h)\n",
    "  \n",
    "def get_preds(raw_logits: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  Aqu√≠ se toma la salida de la red neuronal (las neuronas de la √∫ltima capa oculta).\n",
    "  Uh + b\n",
    "  Se les aplica la softmax\n",
    "  softmax(Uh + b)\n",
    "  Y luego se devuelve el √≠ndice de la neurona de mayor valor.\n",
    "\n",
    "  Args:\n",
    "      raw_logits (torch.Tensor | float): La salida de la red (Uh + b)\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor | int: √çndice de la neurona con mayor valor despu√©s de softmax.\n",
    "  \"\"\"\n",
    "  # Se aplica softmax.\n",
    "  probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "  # Se obtiene el √≠ndice del valor m√°ximo.\n",
    "  y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fc3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ngram(logits, y):\n",
    "    loss_sum = F.cross_entropy(logits, y, reduction='sum')\n",
    "    valid = torch.tensor(y.numel(), device=y.device)\n",
    "    return loss_sum, valid\n",
    "\n",
    "@torch.no_grad()\n",
    "def perplexity_corpus_ngram(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    tot_sum, tot_valid = 0.0, 0\n",
    "    for X_win, y in loader:\n",
    "        X_win, y = X_win.to(device), y.to(device)\n",
    "        logits = model(X_win)\n",
    "        ls, v = loss_ngram(logits, y)\n",
    "        tot_sum += ls.item()\n",
    "        tot_valid += int(v.item())\n",
    "    return exp(tot_sum / max(tot_valid, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "ng = NgramData(N=N, vocab_max=5000, tokenizer=tweet_tokenizer)\n",
    "ng.fit(tr_txt)                                 # arma vocab y mapeos\n",
    "X_np, y_np = ng.transform(tr_txt)              # X: (num_samples, N-1) ; y: (num_samples,)\n",
    "\n",
    "# 3.2 Dataset/Loader\n",
    "X_t = torch.from_numpy(X_np).long()\n",
    "y_t = torch.from_numpy(y_np).long()\n",
    "ds_ng = TensorDataset(X_t, y_t)\n",
    "loader_ng = DataLoader(ds_ng, batch_size=256, shuffle=True)\n",
    "\n",
    "# 3.3 Modelo de Bengio\n",
    "args = SimpleNamespace(\n",
    "    N=N,\n",
    "    m=128,                # tama√±o de embedding\n",
    "    d_h=256,              # tama√±o de capa oculta\n",
    "    dropout=0.2,\n",
    "    vocab_size=ng.get_vocab_size()\n",
    ")\n",
    "model_ng = NeuralLanguageModel(args).to(device)\n",
    "opt_ng = torch.optim.AdamW(model_ng.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45cde5",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "588a0bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NGRAM] Epoch 1/10 | train PPL: 332.248\n",
      "[NGRAM] Epoch 2/10 | train PPL: 175.245\n",
      "[NGRAM] Epoch 3/10 | train PPL: 125.279\n",
      "[NGRAM] Epoch 4/10 | train PPL: 85.923\n",
      "[NGRAM] Epoch 5/10 | train PPL: 54.140\n",
      "[NGRAM] Epoch 6/10 | train PPL: 33.652\n",
      "[NGRAM] Epoch 7/10 | train PPL: 23.067\n",
      "[NGRAM] Epoch 8/10 | train PPL: 17.570\n",
      "[NGRAM] Epoch 9/10 | train PPL: 14.266\n",
      "[NGRAM] Epoch 10/10 | train PPL: 12.053\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    model_ng.train()\n",
    "    tot_sum, tot_valid = 0.0, 0\n",
    "\n",
    "    for X_win, y in loader_ng:\n",
    "        X_win, y = X_win.to(device), y.to(device)\n",
    "\n",
    "        opt_ng.zero_grad()\n",
    "        logits = model_ng(X_win)            # (B,V)\n",
    "        ls, v = loss_ngram(logits, y)\n",
    "        loss_val = ls / v\n",
    "        loss_val.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_ng.parameters(), 1.0)\n",
    "        opt_ng.step()\n",
    "\n",
    "        tot_sum += ls.item()\n",
    "        tot_valid += v.item()\n",
    "\n",
    "    ppl_train = exp(tot_sum / max(tot_valid, 1))\n",
    "    print(f\"[NGRAM] Epoch {epoch}/{epochs} | train PPL: {ppl_train:.3f}\")\n",
    "\n",
    "# (opcional) ppl de validaci√≥n: perplexity_corpus_ngram(model_ng, loader_ng_val, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d09d5fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NGRAM] train PPL: 7.555\n",
      "[NGRAM] val PPL: 564.345\n"
     ]
    }
   ],
   "source": [
    "X_np, y_np = ng.transform(val_txt)              # X: (num_samples, N-1) ; y: (num_samples,)\n",
    "\n",
    "# 3.2 Dataset/Loader\n",
    "X_t = torch.from_numpy(X_np).long()\n",
    "y_t = torch.from_numpy(y_np).long()\n",
    "ds_ng = TensorDataset(X_t, y_t)\n",
    "loader_val_ng = DataLoader(ds_ng, batch_size=256, shuffle=True)\n",
    "\n",
    "ppl_train_ngram = perplexity_corpus_ngram(model_ng, loader_ng, device=device)\n",
    "print(f\"[NGRAM] train PPL: {ppl_train_ngram:.3f}\")\n",
    "ppl_val_ngram = perplexity_corpus_ngram(model_ng, loader_val_ng, device=device)\n",
    "print(f\"[NGRAM] val PPL: {ppl_val_ngram:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434d850",
   "metadata": {},
   "source": [
    "|Modelo|PPL train|PP val|\n",
    "|:----:|:-------:|:----:|\n",
    "|LSTM + self att|725.9629|346.0325|\n",
    "|Statistical|463.030|411.093|\n",
    "|Neural|7.555|564.345|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4ae84",
   "metadata": {},
   "source": [
    "# Generacion de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b414eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# construye vocabulario inverso (id: token)\n",
    "def build_inv_vocab(V):\n",
    "    # V: dict token->id\n",
    "    inv = {i:t for t,i in V.items()}\n",
    "    return inv\n",
    "\n",
    "# muestrea un token a partir de los logits\n",
    "def sample_from_logits(logits, temperature=1.0, top_p=None):\n",
    "    \"\"\"\n",
    "    logits: 1D tensor (V,)\n",
    "    Devuelve: √≠ndice amuestrado (int)\n",
    "    \"\"\"\n",
    "    if temperature <= 0:\n",
    "        return int(torch.argmax(logits).item())\n",
    "\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # muestreo de probabilidad acumulada (top-p)\n",
    "    if top_p is not None and 0 < top_p < 1.0:\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        cutoff = (cum > top_p).nonzero(as_tuple=True)[0]\n",
    "        if len(cutoff) > 0:\n",
    "            k = int(cutoff[0].item() + 1)\n",
    "            keep = sorted_idx[:k]\n",
    "            mask = torch.ones_like(logits, dtype=torch.bool)\n",
    "            mask[keep] = False\n",
    "            logits = logits.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    idx = torch.multinomial(probs, num_samples=1)\n",
    "    return int(idx.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d10e6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_lstm_lm(model, V, prompt_tokens=None, max_new_tokens=30,\n",
    "                     temperature=1.0, top_k=None, top_p=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    model: LSTMSelfAttention entrenado (autoregresivo)\n",
    "    V: dict token->id\n",
    "    prompt_tokens: lista de tokens (si None, empieza con <s>)\n",
    "    Devuelve: lista de tokens (incluye el prompt y lo generado)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    invV = build_inv_vocab(V)\n",
    "\n",
    "    # tokens iniciales\n",
    "    if prompt_tokens is None or len(prompt_tokens) == 0:\n",
    "        toks = [\"<s>\"]\n",
    "    else:\n",
    "        toks = prompt_tokens\n",
    "\n",
    "    # ids iniciales\n",
    "    ids = [V.get(t, V[\"<unk>\"]) for t in toks]\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        lens = torch.tensor([x.size(1)], dtype=torch.long, device=device)\n",
    "        logits = model(x, lens)\n",
    "        next_logits = logits[0, -1, :]\n",
    "\n",
    "        idx = sample_from_logits(next_logits, temperature, top_p)\n",
    "        next_tok = invV.get(idx, \"<unk>\")\n",
    "        ids.append(idx)\n",
    "        toks.append(next_tok)\n",
    "        if next_tok == \"</s>\":\n",
    "            break\n",
    "\n",
    "    return toks\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_bengio_ngram(model, ng, prompt_tokens=None, max_new_tokens=20,\n",
    "                          temperature=1.0, top_p=None, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    Nm1 = getattr(model, \"window_size\")  # usa el tama√±o real del modelo\n",
    "\n",
    "    # contexto inicial de longitud EXACTA Nm1\n",
    "    prompt_tokens = prompt_tokens or []\n",
    "    ctx = ([\"<s>\"]*(Nm1-1) + prompt_tokens)[-Nm1:]\n",
    "\n",
    "    def id_of(tok): return ng.w2id.get(tok, ng.w2id[\"<unk>\"])\n",
    "\n",
    "    out = list(prompt_tokens)  # lo que imprimimos\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_win = torch.tensor([[id_of(t) for t in ctx]], dtype=torch.long, device=device)  # (1, Nm1)\n",
    "        logits = model(x_win)[0]  # (V,)\n",
    "        idx = sample_from_logits(logits, temperature=temperature, top_p=top_p)\n",
    "        tok = ng.id2w.get(idx, \"<unk>\")\n",
    "\n",
    "        out.append(tok)\n",
    "        ctx = (ctx + [tok])[-Nm1:]\n",
    "        if tok == \"</s>\":\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_trigram(tri_lm, tri_data, prompt_tokens=None, max_new_tokens=20,\n",
    "                     temperature=1.0, top_p=None, greedy=False):\n",
    "    \"\"\"\n",
    "    tri_lm: TrigramLanguageModel entrenado\n",
    "    tri_data: TrigramData (para vocab y mask_oov)\n",
    "    \"\"\"\n",
    "    # contexto inicial: dos BOS\n",
    "    ctx = [\"<s>\", \"<s>\"]\n",
    "    if prompt_tokens:\n",
    "        ctx += prompt_tokens\n",
    "    ctx = ctx[-2:]\n",
    "\n",
    "    toks = list(ctx)\n",
    "\n",
    "    # lista estable del vocabulario (para indexar)\n",
    "    vocab = sorted(list(tri_lm.vocab))  # set -> lista\n",
    "    # opcional: evita escoger <s> como siguiente palabra\n",
    "    if \"<s>\" in vocab:\n",
    "        vocab_wo_bos = [w for w in vocab if w != \"<s>\"]\n",
    "    else:\n",
    "        vocab_wo_bos = vocab\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        w2, w1 = ctx[-2], ctx[-1]\n",
    "\n",
    "        # construye la distribuci√≥n sobre todo el vocabulario\n",
    "        probs = np.array([tri_lm.probability_of_word(w2, w1, w) for w in vocab_wo_bos], dtype=np.float64)\n",
    "        probs = np.clip(probs, 1e-12, 1.0)\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "        # temperatura/top-p opcional (en numpy)\n",
    "        if temperature > 0 and temperature != 1.0:\n",
    "            # aplicamos a logits ficticios: log(p)^1/T -> p^(1/T) (aprox estabilizada)\n",
    "            logits = np.log(probs + 1e-12) / temperature\n",
    "            probs = np.exp(logits - logits.max())\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        if top_p is not None and 0 < top_p < 1.0:\n",
    "            sidx = np.argsort(-probs)\n",
    "            sorted_probs = probs[sidx]\n",
    "            csum = np.cumsum(sorted_probs)\n",
    "            k = np.searchsorted(csum, top_p) + 1\n",
    "            keep = sidx[:k]\n",
    "            mask = np.ones_like(probs, dtype=bool)\n",
    "            mask[keep] = False\n",
    "            probs = np.where(mask, 0.0, probs)\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        if greedy or temperature <= 0:\n",
    "            next_w = vocab_wo_bos[int(np.argmax(probs))]\n",
    "        else:\n",
    "            next_w = np.random.choice(vocab_wo_bos, p=probs)\n",
    "\n",
    "        toks.append(next_w)\n",
    "        ctx = (ctx + [next_w])[-2:]\n",
    "        if next_w == \"</s>\":\n",
    "            break\n",
    "\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de24823",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tu eres un #gay asco ! </s>\n"
     ]
    }
   ],
   "source": [
    "gen = generate_lstm_lm(\n",
    "    model, V,\n",
    "    prompt_tokens=[\"<s>\", \"tu\", \"eres\", \"un\"],\n",
    "    max_new_tokens=30, temperature=0.9, top_p=0.9, device=device\n",
    ")\n",
    "print(\" \".join(gen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "038b7074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> hora dos as√≠ che pasada coca üòç </s>\n"
     ]
    }
   ],
   "source": [
    "gen = generate_lstm_lm(\n",
    "    model, V,\n",
    "    prompt_tokens=[],\n",
    "    max_new_tokens=30, temperature=0.9, top_p=0.9, device=device\n",
    ")\n",
    "print(\" \".join(gen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb455f",
   "metadata": {},
   "source": [
    "# Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6dfc4eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> tu eres un pinche calor inmamable mosquitos <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "gen_b = generate_bengio_ngram(\n",
    "    model_ng, ng,\n",
    "    prompt_tokens=[\"<s>\", \"tu\", \"eres\", \"un\"],\n",
    "    max_new_tokens=20, temperature=0.9, top_p=0.9, device=device\n",
    ")\n",
    "print(\" \".join(gen_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e70d060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> <s> dice s√∫per <unk> viendo <unk> <unk> puede ser c√≠nico tipo <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "gen_b = generate_bengio_ngram(\n",
    "    model_ng, ng,\n",
    "    prompt_tokens=[\"<s>\", \"<s>\", \"<s>\", \"<s>\"],\n",
    "    max_new_tokens=20, temperature=0.9, top_p=0.9, device=device\n",
    ")\n",
    "print(\" \".join(gen_b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d8ef2",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3786b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> pinche #vivam√©xico ü§ó ay <unk> ‚úäüèº estr√©s contando sueldo temprano bueno d'alessio sacarle haha . </s>\n"
     ]
    }
   ],
   "source": [
    "gen_tri = generate_trigram(\n",
    "    tri_lm, tri_data,\n",
    "    prompt_tokens=tweet_tokenizer(\"tu eres un\"),\n",
    "    max_new_tokens=25, top_p=0.9, temperature=0.9\n",
    ")\n",
    "print(\" \".join(gen_tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20506140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> aniversario bonita ctm ‚òπ </s>\n"
     ]
    }
   ],
   "source": [
    "gen_tri = generate_trigram(\n",
    "    tri_lm, tri_data,\n",
    "    prompt_tokens=[],\n",
    "    max_new_tokens=25, top_p=0.9, temperature=0.9\n",
    ")\n",
    "print(\" \".join(gen_tri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164866dc",
   "metadata": {},
   "source": [
    "Todos los modelos generan textos con cierto nivel de coherencia, sin embargo muchos de los textos generados son agresivos debido a la naturaleza del corpus.\n",
    "\n",
    "Fue complicado decidir que arquitectura de RNN proponer desde cero, particularmente la parte de decidir hiperparametros para poder vencer a los otros modelos de lenguaje; si bien se logro una perplejidad menor a la de los otros dos, la generacion de texto aun es complicada debido a que no es el objetivo principal de los modelos de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595cebe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
