# -*- coding: utf-8 -*-
"""llamaCorrupcion_master.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nTUS0IL9e_BRQ59bRTyGhCrtZvRXiIbj
"""

import re
import formulario
import csv

#!pip install trl
#!pip uninstall bitsandbytes
#!pip install bitsandbytes>=0.41.1

import torch

if torch.cuda.get_device_capability()[0] >= 8:
    attn_implementation = "flash_attention_2"
    torch_dtype = torch.bfloat16
else:
    attn_implementation = "sdpa"#"eager"
    torch_dtype = torch.float16

import gc
import os
import pickle
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
)
from trl import ORPOConfig, ORPOTrainer

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb=128"

base_model = "/home/est_posgrado_isaac.barron/_hf_models/Llama-3.1-8B-Instruct"

# QLoRA configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch_dtype,
    bnb_4bit_use_double_quant=True,
)

# Load model
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    local_files_only=True,
    quantization_config=bnb_config,
    device_map="auto",
    attn_implementation=attn_implementation,
torch_dtype=torch_dtype,
)
tokenizer = AutoTokenizer.from_pretrained(base_model, local_files_only=True, pad_side = "left")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token
#model, tokenizer = setup_chat_format(model, tokenizer)

#pipe = pipeline("text-generation", model=base_model)
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
)
pipe.model.config.use_cache = False  # True = más rápido pero más memoria

#if memory staurates, garbage collector can be programmed periodically
def collect_garbage(input_message, output):
  del input_message
  del output
  gc.collect()
  gc.collect()
  torch.cuda.empty_cache()


import os
print("CWD: ", os.getcwd())

PROCESSED_LOG = "processed.txt"
FAILED_LOG = "failed.txt"

directorio = "./all_pages_clean/"
archivos = os.listdir(directorio)
try:
  archivos.remove('.ipynb_checkpoints')
except:
  pass
#print(archivos)

def load_set(log_path: str) -> set[str]:
    """Lee el archivo de log (si existe) y regresa un set con los nombres ya procesados."""
    if not os.path.exists(log_path):
        return set()
    ids = set()
    with open(log_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            file_id = line.split("\t", 1)[0].strip()
            if file_id:
                ids.add(file_id)
    return ids

def append_log(log_path: str, file_id: str, extra: str | None = None) -> None:
    """
    Agrega 1 línea al log y fuerza escritura a disco.
    Si extra existe, escribe: file_id \\t extra
    """
    with open(log_path, "a", encoding="utf-8") as f:
        if extra is None:
            f.write(file_id + "\n")
        else:
            f.write(f"{file_id}\t{extra}\n")
        f.flush()
        os.fsync(f.fileno())

processed = load_set(PROCESSED_LOG)
failed = load_set(FAILED_LOG)

def cleanup_gpu():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

'''with open(file_path, "r", encoding="utf-8") as fh:
  input_text = fh.read()

print("\n\nTEXT:\n\n")
print(input_text[:1000])
print("\n\n\n\n")'''

formulaire = [formulario.q1_tipo_entidad, formulario.q2_origen_entidad, formulario.q3_proposito, formulario.q4_informacion_presentada, formulario.q5_vision, formulario.q6_consecuencias, formulario.q7_que_muestra, formulario.q8_intencion, formulario.q9_localidad_problema, formulario.q10_involucrados, formulario.q11_formalidad, formulario.q12_nivel_educacion, formulario.q13_tono, formulario.q14_emociones]
qs = ['pagina', 'tipo_entidad', 'origen_entidad', 'proposito',
       'informacion_presentada', 'vision', 'consecuencias', 'que_muestra',
       'intencion', 'localizacion_problema', 'involucrados', 'formalidad',
       'nivel_educacion', 'tono', 'emociones']

tokenizer.padding_side="left"
batch_size = 64

#pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto")

with open("responses.csv", "a", encoding="utf-8") as f:
  writer = csv.writer(f, delimiter=';')

  if not (os.path.exists("responses.csv") and os.path.getsize("responses.csv") > 0):
    writer.writerow(qs)
    f.flush()
    os.fsync(f.fileno())

  for file in archivos:
      if (file not in processed) and (file not in failed):
        print(file)
        try:
            with open(directorio + "/" + file, "r", encoding="utf-8") as fh:
                input_text = fh.read()
            file_id = file[:4]
            for j in range(1,4):
              print("\t", j)
              #with open(directorio + "/" + file, "r", encoding="utf-8") as fh:
              #  input_text = fh.read()
              respuestas = [file_id]
              for i, pregunta in enumerate(formulaire, start=1):
                incisos = []
                print(f"\t\t{i}\n")
                message = pregunta(input_text)

                output = pipe(
                    message,
                    max_new_tokens=200,
                    do_sample=True,
                    top_k=50,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id,
                    return_full_text=False,
                )

                response = output[0]['generated_text'].strip()
                #start_index = response.rfind("<|start_header_id|>assistant<|end_header_id|>") + len("<|start_header_id|>assistant<|end_header_id|>")
                #respuesta = response[start_index:].strip()
                m = re.findall(r'(^|[\s])([a-i]\))', response)
                #print("m: ", m)
                incisos = ' '.join(t[1] for t in m)
                #print("incisos: ", incisos)
                respuestas.append(incisos)
                collect_garbage(message, output)
              #print("respuestas: ", respuestas)
              writer.writerow(respuestas)
              f.flush()
              os.fsync(f.fileno())

            append_log(PROCESSED_LOG, file_id)
            processed.add(file_id)
        except torch.OutOfMemoryError as e:
            if file_id not in failed:
            # Caso especial: OOM
                failed.add(file_id)
                append_log(FAILED_LOG, file_id, extra="OOM")
            print("OOM:", file_id)
            cleanup_gpu()

        except Exception as e:
            # Otro error
            if file_id not in failed:
                failed.add(file_id)
                append_log(FAILED_LOG, file_id, extra=repr(e))
                print("ERR:", file_id, repr(e))
            cleanup_gpu()
