{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTI8kpGqUaHv"
      },
      "source": [
        "**0. Instalar las librerias necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo-HXq_FP9GV",
        "outputId": "bba00627-1f82-4343-c02d-db21a4a4e998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: PyPDF2 in /home/juancho/.local/lib/python3.10/site-packages (3.0.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pdfminer.six in /home/juancho/.local/lib/python3.10/site-packages (20250506)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/juancho/.local/lib/python3.10/site-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /home/juancho/.local/lib/python3.10/site-packages (from pdfminer.six) (46.0.3)\n",
            "Requirement already satisfied: cffi>=2.0.0 in /home/juancho/.local/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.13.2 in /home/juancho/.local/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /home/juancho/.local/lib/python3.10/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six) (2.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "#!pip install gensim\n",
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb03JR2SP3ZY"
      },
      "source": [
        "**1. Importar las librerias necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "R4uM5hslUlno"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from  os import path,listdir,mkdir,rename,sep #estos modulos les permiten obtener el listado de los archivos de una carpeta, crear una carpeta y renombrar archivos\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "#from gensim import keywords\n",
        "\n",
        "#import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.cluster.hierarchy as shc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import Crypto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsNi-WgJ5P9o",
        "outputId": "ff0e220e-2ed7-4dd2-ea70-bc7fc101c946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/juancho/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgZ7E6pVOtgB",
        "outputId": "0197ea75-4a6d-44a6-b51c-e25ee6a30c2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/juancho/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/juancho/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#drive.mount('/content/drive/')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopws = set(stopwords.words('spanish'))\n",
        "\n",
        "#files = listdir(direccion)\n",
        "#print(files)\n",
        "\n",
        "#Para quitar el \"Copia de \"\n",
        "#for name in archivos:\n",
        "#  rename(direccion+name,direccion+name[9:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVrrHXcNUkzT"
      },
      "source": [
        "**2. Definir funciones**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GG0HxIJuUorF"
      },
      "outputs": [],
      "source": [
        "def converter_PyPDF2(namefiledir):\n",
        "  file = open(namefiledir, \"rb\")\n",
        "  pdf = PyPDF2.PdfFileReader(file)\n",
        "\n",
        "  num_pages = pdf.numPages\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  text = \"\"\n",
        "\n",
        "  while count < num_pages:\n",
        "    pageObj = pdf.getPage(count)\n",
        "    count +=1\n",
        "    try:\n",
        "      text += pageObj.extractText()\n",
        "    except:\n",
        "      text = text\n",
        "\n",
        "  return(text)\n",
        "\n",
        "def limpiartexto(texto):\n",
        "  referencesOUT = True\n",
        "  refined = True#False\n",
        "  longitud_minima_esperado_de_lines = 35\n",
        "  numero_de_stopword_esperados_de_lines = 1\n",
        "  numero_de_stopword_esperados_de_3_lines = 2\n",
        "\n",
        "  texto_limpio = \"\"\n",
        "\n",
        "  listarenglones = texto.split(\"\\n\")\n",
        "\n",
        "  for indice_reglon in range(len(listarenglones)-1):\n",
        "    if len(listarenglones[indice_reglon]) and listarenglones[indice_reglon][-1]==\"-\":\n",
        "      #print(\"#\"+listarenglones[indice_reglon]+\"#\", listarenglones[indice_reglon][-1])\n",
        "      #print(\"#continua#\"+listarenglones[indice_reglon+1]+\"#\")\n",
        "      continuacion_palabra = listarenglones[indice_reglon+1][0:listarenglones[indice_reglon+1].find(\" \")]\n",
        "\n",
        "      if listarenglones[indice_reglon][-2:]==\" -\":\n",
        "        listarenglones[indice_reglon] = listarenglones[indice_reglon][:-2]+continuacion_palabra\n",
        "        listarenglones[indice_reglon+1] = listarenglones[indice_reglon+1][len(continuacion_palabra):]\n",
        "      if listarenglones[indice_reglon][-1]==\"-\":\n",
        "        listarenglones[indice_reglon] = listarenglones[indice_reglon][:-1]+continuacion_palabra\n",
        "        listarenglones[indice_reglon+1] = listarenglones[indice_reglon+1][len(continuacion_palabra):]\n",
        "      #print(\"#2\"+listarenglones[indice_reglon]+\"#2\")\n",
        "\n",
        "  texto_limpio = \"\\n\".join(listarenglones)\n",
        "  text = texto_limpio\n",
        "\n",
        "\n",
        "  text_refined = str()\n",
        "\n",
        "  if referencesOUT:\n",
        "    startreferences = -1\n",
        "\n",
        "    if startreferences < 0: startreferences = text.rfind(\"REFERENCES\")-len(\"REFERENCES\")\n",
        "    if startreferences < 0: startreferences = text.rfind(\"References\")-len(\"References\")\n",
        "    if startreferences < 0: startreferences = text.rfind(\"R E F E R E N C E S\")-len(\"R E F E R E N C E S\")\n",
        "    if startreferences < 0: startreferences = text.rfind(\"R e f e r e n c e s\")-len(\"R e f e r e n c e s\")\n",
        "    if startreferences < 0: startreferences = text.rfind(\"Literature Cited\")-len(\"Literature Cited\")\n",
        "\n",
        "    #if startreferences < 0: print(\"How no!!\")\n",
        "\n",
        "    if startreferences > 0: text = text[:startreferences]\n",
        "\n",
        "\n",
        "  if not refined:\n",
        "    text_refined = text\n",
        "  else:\n",
        "    lines = text.split(\"\\n\")\n",
        "    for nline in range(len(lines)):\n",
        "\n",
        "      line = lines[nline]\n",
        "      validline = 0\n",
        "\n",
        "      preline = lines[nline-1:nline]\n",
        "      if len(preline)>0 : preline = preline[0]\n",
        "      else: preline = \"\"\n",
        "      postline = lines[nline+1:nline+2]\n",
        "      if len(postline)>0 : postline = postline[0]\n",
        "      else: postline = \"\"\n",
        "\n",
        "\n",
        "      if len(line) > longitud_minima_esperado_de_lines:\n",
        "        validline += 1\n",
        "      elif len(preline) > longitud_minima_esperado_de_lines: #and line.find(\".\")>0:\n",
        "        validline += 1\n",
        "\n",
        "      #print(\"#\", centroid_plusExterior_listline)\n",
        "      if len(set(line.split(\" \")) & stopws)  > numero_de_stopword_esperados_de_lines:\n",
        "        validline += 1\n",
        "      else:\n",
        "        triline = preline + \" \" + line + \" \" + postline\n",
        "        if len(set(triline.split(\" \")) & stopws)  > numero_de_stopword_esperados_de_3_lines:\n",
        "          validline += 1\n",
        "\n",
        "      if validline > 1:\n",
        "        text_refined += line+\" \" #+\"\\n\"\n",
        "\n",
        "    texto=text_refined\n",
        "\n",
        "    texto = texto.lower()\n",
        "\n",
        "    texto = word_tokenize(texto)\n",
        "\n",
        "    texto = list(filter(lambda a: not a in (stopws and set([\"et\",\"al\"])), texto))\n",
        "    texto = list(filter(lambda a: a.isalpha(), texto))\n",
        "    texto = \" \".join(texto)\n",
        "\n",
        "  return(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gGSqT2I4r2W0"
      },
      "outputs": [],
      "source": [
        "def read_pypdf2(namefile):\n",
        "  reader = PdfReader(namefile)\n",
        "  full_text = \"\"\n",
        "\n",
        "  if reader.is_encrypted:\n",
        "    try:\n",
        "      reader.decrypt('')\n",
        "    except:\n",
        "      print(\"Could not decrypt the PDF file.\")\n",
        "      return(full_text)\n",
        "\n",
        "  for page in reader.pages:\n",
        "    full_text += page.extract_text()\n",
        "\n",
        "  return(full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "yMnxaLpryiKC"
      },
      "outputs": [],
      "source": [
        "def extract_cont_word(text):\n",
        "  voc_count=Counter(text.split())\n",
        "  return(voc_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0nq8WYb-Ysi"
      },
      "source": [
        "**3. Variables glabales o de entrada**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hbedba6P-t9F"
      },
      "outputs": [],
      "source": [
        "direccion=\"./\"\n",
        "#direccion_pdf=\"./all_pages/\"\n",
        "direccion_pdf = \"/home/juancho/aerrors/\"\n",
        "direccion_txt=\"./all_pages_clean/txt/\"\n",
        "direccion_txtpypdf=\"./all_pages_clean/pypdf/\"\n",
        "lenguaje='spanish'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0484.html', '0041.html', '0425.html', '0487.html', '0087.pdf', '0560.pdf', '0249.pdf', '0308.pdf', '0375.html', '0313.html', '0069.html', '0292.html', '0473.pdf', '0485.html', '0402.html', '0044.mp4', '0500.pdf', '0498.pdf', '0228.html', '0336.html', '0315.pdf', '0254.pdf', '0505.html', '0475.pdf', '0172.html', '0175.pdf', '0342.mp4', '0345.mp4', '0225.html', '0508.pdf']\n"
          ]
        }
      ],
      "source": [
        "files = listdir(direccion_pdf)\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1k5Ny7Ooh3"
      },
      "source": [
        "**4. Abrir las carpetas con los archivos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUJIv43Y035H",
        "outputId": "391c946f-9be3-4d1f-cf1c-b126d527d604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sobre', 'tuya', 'estada', 'haya', 'sea', 'fuéramos', 'fuese', 'estuvisteis', 'estando', 'estábamos', 'soy', 'no', 'tú', 'tendría', 'hubieran', 'yo', 'ese', 'hubo', 'mucho', 'fuera', 'e', 'suyo', 'habíais', 'estaremos', 'habíamos', 'eran', 'ni', 'hubimos', 'eso', 'muy', 'estaban', 'algunos', 'estad', 'una', 'son', 'fuerais', 'hubiéramos', 'esos', 'somos', 'fue', 'hubieses', 'todo', 'mías', 'fuimos', 'sentid', 'sintiendo', 'estuve', 'tenga', 'tendré', 'habidos', 'habrás', 'habiendo', 'tengan', 'sería', 'para', 'seríamos', 'será', 'les', 'esa', 'hubiésemos', 'suya', 'tuviese', 'te', 'habido', 'del', 'tanto', 'habían', 'tuvieseis', 'estáis', 'fueran', 'habré', 'estamos', 'estarás', 'fuésemos', 'su', 'estuviésemos', 'tuyos', 'había', 'sean', 'qué', 'hubierais', 'y', 'habida', 'le', 'estarías', 'tenidas', 'tendríais', 'nuestras', 'estuviera', 'míos', 'estadas', 'estaríamos', 'hubisteis', 'hubieseis', 'tuvieras', 'tenida', 'esas', 'uno', 'habríais', 'estén', 'un', 'habrán', 'los', 'teníais', 'habrías', 'tened', 'tiene', 'mis', 'teniendo', 'el', 'tendrás', 'nuestros', 'estaré', 'hubiera', 'estuvo', 'fuisteis', 'habrá', 'estuvimos', 'nada', 'han', 'es', 'tuvieses', 'habías', 'unos', 'nosotros', 'poco', 'algunas', 'tengamos', 'que', 'esté', 'habidas', 'tuviésemos', 'porque', 'estado', 'estarían', 'serías', 'vuestras', 'ti', 'seré', 'era', 'contra', 'otra', 'seremos', 'durante', 'tengáis', 'tienes', 'tuvieron', 'tenía', 'quien', 'tu', 'fueseis', 'hayan', 'sentido', 'tenido', 'estuviese', 'ha', 'fueron', 'teníamos', 'fueras', 'estaba', 'otros', 'hay', 'de', 'a', 'tenéis', 'ellos', 'estuvieron', 'os', 'seáis', 'fueses', 'mi', 'estará', 'hubieron', 'ella', 'nosotras', 'cual', 'habréis', 'él', 'vosotras', 'ellas', 'hubiste', 'fuesen', 'nos', 'estés', 'estuviste', 'esta', 'éramos', 'estoy', 'estaríais', 'o', 'estuvieses', 'tuyas', 'hemos', 'habéis', 'suyas', 'están', 'seréis', 'estaría', 'las', 'tenías', 'pero', 'seríais', 'sentidas', 'tendrían', 'eras', 'mía', 'estás', 'todos', 'estuviéramos', 'estarán', 'tuviéramos', 'tendrán', 'tengo', 'hubiesen', 'sois', 'ante', 'sentidos', 'estar', 'serás', 'lo', 'ya', 'vuestro', 'tuvo', 'habrían', 'sentida', 'estos', 'serán', 'sin', 'mío', 'tus', 'fui', 'hube', 'suyos', 'tuve', 'al', 'estas', 'tuvieran', 'tendréis', 'tendrías', 'hasta', 'tendremos', 'estados', 'estuviesen', 'algo', 'tuyo', 'cuando', 'muchos', 'nuestra', 'nuestro', 'seas', 'siente', 'tuvimos', 'tendríamos', 'por', 'erais', 'tuvierais', 'desde', 'fuiste', 'otro', 'he', 'en', 'antes', 'este', 'estemos', 'tuviera', 'con', 'vuestros', 'donde', 'serían', 'tenemos', 'seamos', 'estuvieran', 'habría', 'tuvisteis', 'hayamos', 'eres', 'vosotros', 'has', 'sus', 'se', 'está', 'estuvieras', 'estabas', 'otras', 'estuvierais', 'más', 'tendrá', 'como', 'mí', 'hayas', 'me', 'esto', 'tuviesen', 'estaréis', 'tenidos', 'hubieras', 'estuvieseis', 'tuviste', 'vuestra', 'la', 'hubiese', 'quienes', 'estabais', 'también', 'entre', 'estéis', 'tenían', 'tengas', 'hayáis', 'tienen', 'habríamos', 'habremos', 'sí'}\n"
          ]
        }
      ],
      "source": [
        "print(stopws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PjXqHvZmMaA"
      },
      "source": [
        "**5. Extraer el texto del pdf**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EivGRHpZOm5i",
        "outputId": "6715daf2-3eaa-4211-fb96-252a026c13e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "Processing file: 0087.pdf\n",
            "0087.pdf\n",
            "Processing file: 0560.pdf\n",
            "0560.pdf\n",
            "Processing file: 0249.pdf\n",
            "0249.pdf\n",
            "Processing file: 0308.pdf\n",
            "0308.pdf\n",
            "Processing file: 0473.pdf\n",
            "0473.pdf\n",
            "Processing file: 0500.pdf\n",
            "0500.pdf\n",
            "Processing file: 0498.pdf\n",
            "0498.pdf\n",
            "Processing file: 0315.pdf\n",
            "0315.pdf\n",
            "Processing file: 0254.pdf\n",
            "0254.pdf\n",
            "Processing file: 0475.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0475.pdf\n",
            "Processing file: 0175.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n",
            "Cannot set gray stroke color because /'P0' is an invalid float value\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0175.pdf\n",
            "Processing file: 0508.pdf\n",
            "0508.pdf\n"
          ]
        }
      ],
      "source": [
        "files = listdir(direccion_pdf)\n",
        "if not path.exists(direccion_txt):\n",
        "  mkdir(direccion_txt)\n",
        "print(len(files))\n",
        "#print(files)\n",
        "for file in files:\n",
        "  #print(direccion_pdf+file)\n",
        "  if path.isfile(direccion_pdf+file) and file[-4:] == \".pdf\":\n",
        "    #print(direccion_pdf+file)\n",
        "    print(\"Processing file:\", file)\n",
        "    text = extract_text(direccion_pdf+file)\n",
        "    file_txt = open(direccion_txt+file[:-4]+\".txt\", \"w\")\n",
        "    file_txt.write(text)\n",
        "    file_txt.close()\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBBKXeQgsesO",
        "outputId": "955b29e7-bc7b-4929-a8da-46081590e20e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "['0484.html', '0041.html', '0425.html', '0487.html', '0087.pdf', '0560.pdf', '0249.pdf', '0308.pdf', '0375.html', '0313.html', '0069.html', '0292.html', '0473.pdf', '0485.html', '0402.html', '0044.mp4', '0500.pdf', '0498.pdf', '0228.html', '0336.html', '0315.pdf', '0254.pdf', '0505.html', '0475.pdf', '0172.html', '0175.pdf', '0342.mp4', '0345.mp4', '0225.html', '0508.pdf']\n",
            "0087.pdf\n",
            "0560.pdf\n",
            "0249.pdf\n",
            "0308.pdf\n",
            "0473.pdf\n",
            "0500.pdf\n",
            "0498.pdf\n",
            "0315.pdf\n",
            "0254.pdf\n",
            "0475.pdf\n",
            "0175.pdf\n",
            "0508.pdf\n"
          ]
        }
      ],
      "source": [
        "files = listdir(direccion_pdf)\n",
        "print(len(files))\n",
        "print(files)\n",
        "if not path.exists(direccion_txtpypdf):\n",
        "  mkdir(direccion_txtpypdf)\n",
        "\n",
        "for file in files:\n",
        " if path.isfile(direccion_pdf+file) and file[-4:] == \".pdf\":\n",
        "    print(file)\n",
        "    try:\n",
        "      text = read_pypdf2(direccion_pdf+file)\n",
        "      file_txt = open(direccion_txtpypdf+file[:-4]+\".txt\", \"w\", encoding='utf-8', errors='ignore')\n",
        "      file_txt.write(text)\n",
        "      file_txt.close()\n",
        "    except Exception as e:\n",
        "      print(\"Error processing file:\", file)\n",
        "      print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qT4E6evOkH"
      },
      "source": [
        "**6.Seleccionar txt de extracción**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "CAnhlU-pF3eP"
      },
      "outputs": [],
      "source": [
        "def count_long_words(counter, max_length=10):\n",
        "    \"\"\"\n",
        "    Cuenta cuántas palabras en un Counter tienen más de `min_length` letras.\n",
        "\n",
        "    Args:\n",
        "        counter (Counter): frecuencias de palabras.\n",
        "        min_length (int): longitud mínima de palabra (default=10).\n",
        "\n",
        "    Returns:\n",
        "        int: número de palabras que cumplen la condición.\n",
        "        list: lista de esas palabras (opcional para revisión).\n",
        "    \"\"\"\n",
        "    excluded_patterns = (\"http\", \"www\", \".com\", \"@\", \"doi\")\n",
        "\n",
        "\n",
        "    #long_words = [word for word in counter if len(word) > max_length]\n",
        "    #long_words = [word for word in counter if len(word) > max_length and not any(pat in word.lower() for pat in excluded_patterns)]\n",
        "    long_words = []\n",
        "\n",
        "    for word in counter:\n",
        "      if len(word) > max_length and not any(pat in word.lower() for pat in excluded_patterns):\n",
        "        # contar cuántos caracteres son letras\n",
        "        letters = sum(1 for ch in word if ch.isalpha())\n",
        "        ratio = letters / len(word)\n",
        "\n",
        "        if ratio >= 0.5:  # al menos 50% letras\n",
        "            long_words.append(word)\n",
        "\n",
        "    return len(long_words), long_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMTX1Td6vY0e",
        "outputId": "a28d7d3d-db1a-4dad-f166-43d4f6a1a019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "['0249.txt', '0498.txt', '0473.txt', '0087.txt', '0508.txt', '0475.txt', '0254.txt', '0315.txt', '0308.txt', '0500.txt', '0560.txt', '0175.txt']\n",
            "valor anomalo 0249.txt\n",
            "0.0\n",
            "Counter()\n",
            "nuevas 0 0\n",
            "nuevas 2194 21\n",
            "valor anomalo 0473.txt\n",
            "0.0\n",
            "Counter()\n",
            "nuevas 0 0\n",
            "nuevas 4737 20\n",
            "nuevas 2681 24\n",
            "nuevas 3700 72\n",
            "nuevas 1987 42\n",
            "nuevas 1812 21\n",
            "nuevas 2101 74\n",
            "nuevas 1703 65\n",
            "nuevas 3035 34\n",
            "nuevas 2435 409\n"
          ]
        }
      ],
      "source": [
        "files = listdir(direccion_txt)\n",
        "print(len(files))\n",
        "print(files)\n",
        "\n",
        "\n",
        "for file in files:\n",
        " if path.isfile(direccion_txt+file) and file[-4:] == \".txt\":\n",
        "    file_txt = open(direccion_txt+file, \"r\")\n",
        "    text = file_txt.read()\n",
        "    voc=extract_cont_word(text)\n",
        "    sizedoc=sum(voc.values())\n",
        "    file_txt.close()\n",
        "    #print(sizedoc)\n",
        "    frecuenciapromedio=sizedoc/(len(voc) + 1)\n",
        "    #print(frecuenciapromedio)\n",
        "    bajoumbral = {word: count for word, count in voc.items() if count > frecuenciapromedio}\n",
        "    #print(\"Palabras más frencuentes que el promedio:\", len(bajoumbral)/len(voc))\n",
        "    razonpalabrasfrecuentes=len(bajoumbral)/(len(voc) + 1)\n",
        "\n",
        "    if 0.1 > razonpalabrasfrecuentes or razonpalabrasfrecuentes > 0.2:\n",
        "      print(\"valor anomalo\",file)\n",
        "      print(razonpalabrasfrecuentes)\n",
        "      print(voc)\n",
        "\n",
        "    #print(valida_zipf(voc, top_n=400, plot=False))\n",
        "    max_length=20\n",
        "    if count_long_words(voc, max_length)[0] > 20:\n",
        "      print(\"\\n\\nNumero de palabras largas:\", count_long_words(voc, max_length))\n",
        "      print(voc,\"\\n\\n\\n\\n\")\n",
        "    max=0\n",
        "    for w in voc:\n",
        "      if len(w) > max:\n",
        "        max=len(w)\n",
        "\n",
        "    print(\"nuevas\",len(voc),max)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "xn2K4kcwajSE"
      },
      "outputs": [],
      "source": [
        "files = listdir(direccion_txt)\n",
        "\n",
        "for file in files[:10]:\n",
        " if path.isfile(direccion_txt+file) and path.isfile(direccion_txtpypdf+file) and file[-4:] == \".txt\":\n",
        "    file_txt = open(direccion_txt+file, \"r\")\n",
        "    file_txtpypdf = open(direccion_txtpypdf+file, \"r\")\n",
        "    text = file_txt.read()\n",
        "    textpypdf = file_txtpypdf.read()\n",
        "    voc=extract_cont_word(text)\n",
        "    vocpypdf=extract_cont_word(textpypdf)\n",
        "    sizedoc=sum(voc.values())\n",
        "    sizedocpypdf=sum(vocpypdf.values())\n",
        "    file_txt.close()\n",
        "    file_txtpypdf.close()\n",
        "    #print(sizedoc)\n",
        "    frecuenciapromedio=sizedoc/(len(voc) + 1)\n",
        "    frecuenciapromediopypdf=sizedocpypdf/(len(vocpypdf) + 1)\n",
        "    #print(frecuenciapromedio)\n",
        "    bajoumbral = {word: count for word, count in voc.items() if count > frecuenciapromedio}\n",
        "    bajoumbralpypdf = {word: count for word, count in vocpypdf.items() if count > frecuenciapromediopypdf}\n",
        "    #print(\"Palabras más frencuentes que el promedio:\", len(bajoumbral)/len(voc))\n",
        "    razonpalabrasfrecuentes=len(bajoumbral)/(len(voc) + 1)\n",
        "    razonpalabrasfrecuentespypdf=len(bajoumbralpypdf)/(len(vocpypdf) + 1)\n",
        "\n",
        "\n",
        "    #if 0.1 > razonpalabrasfrecuentes or razonpalabrasfrecuentes > 0.2:\n",
        "    #  print(\"valor anomalo\",file)\n",
        "    #  print(razonpalabrasfrecuentes)\n",
        "    #  print(voc)\n",
        "\n",
        "    #print(valida_zipf(voc, top_n=400, plot=False))\n",
        "    max_length=20\n",
        "    #if count_long_words(voc, max_length)[0] > 20:\n",
        "    #  print(\"\\n\\nNumero de palabras largas:\", count_long_words(voc, max_length))\n",
        "    #  print(voc,\"\\n\\n\\n\\n\")\n",
        "\n",
        "    num_long_words = count_long_words(voc, max_length)[0]\n",
        "    num_long_wordspypdf = count_long_words(vocpypdf, max_length)[0]\n",
        "\n",
        "    #if num_long_words > 20 or num_long_wordspypdf > 20 or 0.1 > razonpalabrasfrecuentes or razonpalabrasfrecuentes > 0.2 or 0.1 > razonpalabrasfrecuentespypdf or razonpalabrasfrecuentespypdf > 0.2:\n",
        "    if False:\n",
        "      print(\"\\n\\nNumero de palabras largas:\", num_long_words, num_long_wordspypdf)\n",
        "      print(\"valor anomalo\",file)\n",
        "      print(razonpalabrasfrecuentes, razonpalabrasfrecuentespypdf)\n",
        "      print(voc)\n",
        "      print(vocpypdf)\n",
        "      print(\"Zipf\",valida_zipf(voc, top_n=400, plot=False),valida_zipf(vocpypdf, top_n=400, plot=Fakse))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "9WZYtrh05UQu"
      },
      "outputs": [],
      "source": [
        "def valida_zipf(counter, top_n=50, plot=True):\n",
        "    \"\"\"\n",
        "    Valida ley de Zipf\n",
        "\n",
        "    \"\"\"\n",
        "    # palabras\n",
        "    freqs = np.array([c for _, c in counter.most_common(top_n)])\n",
        "    ranks = np.arange(1, len(freqs) + 1)\n",
        "\n",
        "    # Log transform\n",
        "    log_ranks = np.log(ranks)\n",
        "    log_freqs = np.log(freqs)\n",
        "\n",
        "    # Correlation (should be close to -1 if Zipf’s law holds)\n",
        "    correlation = np.corrcoef(log_ranks, log_freqs)[0,1]\n",
        "\n",
        "    if plot:\n",
        "        plt.scatter(log_ranks, log_freqs)\n",
        "        plt.plot(log_ranks, np.poly1d(np.polyfit(log_ranks, log_freqs, 1))(log_ranks), color=\"red\")\n",
        "        plt.xlabel(\"log(Rank)\")\n",
        "        plt.ylabel(\"log(Frequency)\")\n",
        "        plt.title(f\"Zipf's Law Validation (corr={correlation:.2f})\")\n",
        "        plt.show()\n",
        "\n",
        "    return {\"correlation\": correlation, \"top_n\": top_n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weXBosPQ9b6I",
        "outputId": "5d016dc5-a3d3-401d-a1bb-6249e3212549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0249.txt Zipf {'correlation': nan, 'top_n': 400} {'correlation': nan, 'top_n': 400}\n",
            "0498.txt Zipf {'correlation': -0.9940212145598855, 'top_n': 400} {'correlation': -0.9946448844729764, 'top_n': 400}\n",
            "0473.txt Zipf {'correlation': nan, 'top_n': 400} {'correlation': nan, 'top_n': 400}\n",
            "0087.txt Zipf {'correlation': -0.9955977091060156, 'top_n': 400} {'correlation': -0.995305403335225, 'top_n': 400}\n",
            "0508.txt Zipf {'correlation': -0.9917101343839825, 'top_n': 400} {'correlation': -0.9911748510549057, 'top_n': 400}\n",
            "0475.txt Zipf {'correlation': -0.9893456358191915, 'top_n': 400} {'correlation': -0.9893692045738376, 'top_n': 400}\n",
            "0254.txt Zipf {'correlation': -0.9863998298964739, 'top_n': 400} {'correlation': -0.9873569614576224, 'top_n': 400}\n",
            "0315.txt Zipf {'correlation': -0.9902824240071233, 'top_n': 400} {'correlation': -0.9898034723023286, 'top_n': 400}\n",
            "0308.txt Zipf {'correlation': -0.9906826244754292, 'top_n': 400} {'correlation': -0.9909841423257528, 'top_n': 400}\n",
            "0500.txt Zipf {'correlation': -0.9883501747487282, 'top_n': 400} {'correlation': -0.9903052696565147, 'top_n': 400}\n",
            "0560.txt Zipf {'correlation': -0.9975282476401157, 'top_n': 400} {'correlation': -0.9970133531009476, 'top_n': 400}\n",
            "0175.txt Zipf {'correlation': -0.9918279019633843, 'top_n': 400} {'correlation': -0.9923552231921461, 'top_n': 400}\n",
            "12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/juancho/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/home/juancho/.local/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n",
            "/home/juancho/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2889: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  c = cov(x, y, rowvar, dtype=dtype)\n",
            "/home/juancho/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "/home/juancho/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n"
          ]
        }
      ],
      "source": [
        "files = listdir(direccion_txt)\n",
        "\n",
        "direccion_txt_best=\"./all_pages_clean/best/\"\n",
        "\n",
        "if not path.exists(direccion_txt_best):\n",
        "  mkdir(direccion_txt_best)\n",
        "\n",
        "for file in files:\n",
        " if path.isfile(direccion_txt+file) and path.isfile(direccion_txtpypdf+file) and file[-4:] == \".txt\":\n",
        "    file_txt = open(direccion_txt+file, \"r\")\n",
        "    file_txtpypdf = open(direccion_txtpypdf+file, \"r\")\n",
        "    text = file_txt.read()\n",
        "    textpypdf = file_txtpypdf.read()\n",
        "    voc=extract_cont_word(text)\n",
        "    vocpypdf=extract_cont_word(textpypdf)\n",
        "    print(file,\"Zipf\",valida_zipf(voc, top_n=400, plot=False),valida_zipf(vocpypdf, top_n=400, plot=False))\n",
        "    Zipf=valida_zipf(voc, top_n=400, plot=False)\n",
        "    Zipfpypdf=valida_zipf(vocpypdf, top_n=400, plot=False)\n",
        "    if Zipf[\"correlation\"] < Zipfpypdf[\"correlation\"]:\n",
        "      file_txt_best = open(direccion_txt_best+file, \"w\")\n",
        "      file_txt_best.write(text)\n",
        "      file_txt_best.close()\n",
        "    else:\n",
        "      file_txt_best = open(direccion_txt_best+file, \"w\")\n",
        "      file_txt_best.write(textpypdf)\n",
        "      file_txt_best.close()\n",
        "\n",
        "print(len(listdir(direccion_txt_best)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCjOwGq7W5VC"
      },
      "source": [
        "**7. Limpieza y preprocesamiento de textos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFd-ffhtVk-n",
        "outputId": "4af084f5-8782-4605-d59c-4f11d2c5ca07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### 0249.txt\n",
            "#### 0498.txt\n",
            "#### 0473.txt\n",
            "#### 0087.txt\n",
            "#### 0508.txt\n",
            "#### 0475.txt\n",
            "#### 0254.txt\n",
            "#### 0315.txt\n",
            "#### 0308.txt\n",
            "#### 0500.txt\n",
            "#### 0560.txt\n",
            "#### 0175.txt\n"
          ]
        }
      ],
      "source": [
        "direccion_txt_best=\"./all_pages_clean/best/\"\n",
        "\n",
        "files_txt_best = listdir(direccion_txt_best)\n",
        "documentos=[]\n",
        "name_documentos=[]\n",
        "documentos_all=[]\n",
        "for file in files_txt_best:\n",
        " if path.isfile(direccion_txt_best+file) and file[-4:] == \".txt\":\n",
        "    print(\"####\",file)\n",
        "    file_content = open(direccion_txt_best+file, \"r\")\n",
        "    texto = file_content.read()\n",
        "    #voc=extract_cont_word(texto)\n",
        "    #print(voc)\n",
        "    file_content.close()\n",
        "    #print(\"&1\",texto[:500])\n",
        "    #print(\"$1\",len(texto))\n",
        "    texto = limpiartexto(texto)\n",
        "    #print(texto)\n",
        "    name_documentos.append(file)\n",
        "    documentos.append(texto)\n",
        "    documentos_all.append([file,texto])\n",
        "    #print(\"$2\",len(texto))\n",
        "    #print(\"&2\",texto[:500])\n",
        "\n",
        "#print(documentos_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
